% Options for packages loaded elsewhere
% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrreprt}
\usepackage{xcolor}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother


\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother





\setlength{\emergencystretch}{3em} % prevent overfull lines

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}



 


\KOMAoption{captions}{tableheading}
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={The Little Book of Linear Algebra},
  pdfauthor={Duc-Tam Nguyen},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{The Little Book of Linear Algebra}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{Version 0.2.0}
\author{Duc-Tam Nguyen}
\date{2025-09-25}
\begin{document}
\maketitle

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}

\bookmarksetup{startatroot}

\chapter{Content}\label{content}

\subsubsection{Chapter 1. Vectors, Scalars, and
Geometry}\label{chapter-1.-vectors-scalars-and-geometry}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Scalars, vectors, and coordinate systems (what they are, why we care)
\item
  Vector notation, components, and arrows (reading and writing vectors)
\item
  Vector addition and scalar multiplication (the two basic moves)
\item
  Linear combinations and span (building new vectors from old ones)
\item
  Length (norm) and distance (how big and how far)
\item
  Dot product (algebraic and geometric views)
\item
  Angles between vectors and cosine (measuring alignment)
\item
  Projections and decompositions (splitting along a direction)
\item
  Cauchy--Schwarz and triangle inequalities (two fundamental bounds)
\item
  Orthonormal sets in ℝ²/ℝ³ (nice bases you already know)
\end{enumerate}

\subsubsection{Chapter 2. Matrices and Basic
Operations}\label{chapter-2.-matrices-and-basic-operations}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{10}
\tightlist
\item
  Matrices as tables and as machines (two mental models)
\item
  Matrix shapes, indexing, and block views (seeing structure)
\item
  Matrix addition and scalar multiplication (componentwise rules)
\item
  Matrix--vector product (linear combos of columns)
\item
  Matrix--matrix product (composition of linear steps)
\item
  Identity, inverse, and transpose (three special friends)
\item
  Symmetric, diagonal, triangular, and permutation matrices (special
  families)
\item
  Trace and basic matrix properties (quick invariants)
\item
  Affine transforms and homogeneous coordinates (translations included)
\item
  Computing with matrices (cost counts and simple speedups)
\end{enumerate}

\subsubsection{Chapter 3. Linear Systems and
Elimination}\label{chapter-3.-linear-systems-and-elimination}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{20}
\tightlist
\item
  From equations to matrices (augmenting and encoding)
\item
  Row operations (legal moves that keep solutions)
\item
  Row-echelon and reduced row-echelon forms (target shapes)
\item
  Pivots, free variables, and leading ones (reading solutions)
\item
  Solving consistent systems (unique vs.~infinite solutions)
\item
  Detecting inconsistency (when no solution exists)
\item
  Gaussian elimination by hand (a disciplined procedure)
\item
  Back substitution and solution sets (finishing cleanly)
\item
  Rank and its first meaning (pivots as information)
\item
  LU factorization (elimination captured as L and U)
\end{enumerate}

\subsubsection{Chapter 4. Vector Spaces and
Subspaces}\label{chapter-4.-vector-spaces-and-subspaces}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{30}
\tightlist
\item
  Axioms of vector spaces (what ``space'' really means)
\item
  Subspaces, column space, and null space (where solutions live)
\item
  Span and generating sets (coverage of a space)
\item
  Linear independence and dependence (no redundancy vs.~redundancy)
\item
  Basis and coordinates (naming every vector uniquely)
\item
  Dimension (how many directions)
\item
  Rank--nullity theorem (dimensions that add up)
\item
  Coordinates relative to a basis (changing the ``ruler'')
\item
  Change-of-basis matrices (moving between coordinate systems)
\item
  Affine subspaces (lines and planes not through the origin)
\end{enumerate}

\subsubsection{Chapter 5. Linear Transformations and
Structure}\label{chapter-5.-linear-transformations-and-structure}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{40}
\tightlist
\item
  Linear transformations (preserving lines and sums)
\item
  Matrix representation of a linear map (choosing a basis)
\item
  Kernel and image (inputs that vanish; outputs we can reach)
\item
  Invertibility and isomorphisms (perfectly reversible maps)
\item
  Composition, powers, and iteration (doing it again and again)
\item
  Similarity and conjugation (same action, different basis)
\item
  Projections and reflections (idempotent and involutive maps)
\item
  Rotations and shear (geometric intuition)
\item
  Rank and operator viewpoint (rank beyond elimination)
\item
  Block matrices and block maps (divide and conquer structure)
\end{enumerate}

\subsubsection{Chapter 6. Determinants and
Volume}\label{chapter-6.-determinants-and-volume}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{50}
\tightlist
\item
  Areas, volumes, and signed scale factors (geometric entry point)
\item
  Determinant via linear rules (multilinearity, sign, normalization)
\item
  Determinant and row operations (how each move changes det)
\item
  Triangular matrices and product of diagonals (fast wins)
\item
  det(AB) = det(A)det(B) (multiplicative magic)
\item
  Invertibility and zero determinant (flat vs.~full volume)
\item
  Cofactor expansion (Laplace's method)
\item
  Permutations and sign (the combinatorial core)
\item
  Cramer's rule (solving with determinants, and when not to use it)
\item
  Computing determinants in practice (use LU, mind stability)
\end{enumerate}

\subsubsection{Chapter 7. Eigenvalues, Eigenvectors, and
Dynamics}\label{chapter-7.-eigenvalues-eigenvectors-and-dynamics}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{60}
\tightlist
\item
  Eigenvalues and eigenvectors (directions that stay put)
\item
  Characteristic polynomial (where eigenvalues come from)
\item
  Algebraic vs.~geometric multiplicity (how many and how independent)
\item
  Diagonalization (when a matrix becomes simple)
\item
  Powers of a matrix (long-term behavior via eigenvalues)
\item
  Real vs.~complex spectra (rotations and oscillations)
\item
  Defective matrices and a peek at Jordan form (when diagonalization
  fails)
\item
  Stability and spectral radius (grow, decay, or oscillate)
\item
  Markov chains and steady states (probabilities as linear algebra)
\item
  Linear differential systems (solutions via eigen-decomposition)
\end{enumerate}

\subsubsection{Chapter 8. Orthogonality, Least Squares, and
QR}\label{chapter-8.-orthogonality-least-squares-and-qr}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{70}
\tightlist
\item
  Inner products beyond dot product (custom notions of angle)
\item
  Orthogonality and orthonormal bases (perpendicular power)
\item
  Gram--Schmidt process (constructing orthonormal bases)
\item
  Orthogonal projections onto subspaces (closest point principle)
\item
  Least-squares problems (fit when exact solve is impossible)
\item
  Normal equations and geometry of residuals (why it works)
\item
  QR factorization (stable least squares via orthogonality)
\item
  Orthogonal matrices (length-preserving transforms)
\item
  Fourier viewpoint (expanding in orthogonal waves)
\item
  Polynomial and multifeature least squares (fitting more flexibly)
\end{enumerate}

\subsubsection{Chapter 9. SVD, PCA, and
Conditioning}\label{chapter-9.-svd-pca-and-conditioning}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{80}
\tightlist
\item
  Singular values and SVD (universal factorization)
\item
  Geometry of SVD (rotations + stretching)
\item
  Relation to eigen-decompositions (ATA and AAT)
\item
  Low-rank approximation (best small models)
\item
  Principal component analysis (variance and directions)
\item
  Pseudoinverse (Moore--Penrose) and solving ill-posed systems
\item
  Conditioning and sensitivity (how errors amplify)
\item
  Matrix norms and singular values (measuring size properly)
\item
  Regularization (ridge/Tikhonov to tame instability)
\item
  Rank-revealing QR and practical diagnostics (what rank really is)
\end{enumerate}

\subsubsection{Chapter 10. Applications and
Computation}\label{chapter-10.-applications-and-computation}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{90}
\tightlist
\item
  2D/3D geometry pipelines (cameras, rotations, and transforms)
\item
  Computer graphics and robotics (homogeneous tricks in action)
\item
  Graphs, adjacency, and Laplacians (networks via matrices)
\item
  Data preprocessing as linear ops (centering, whitening, scaling)
\item
  Linear regression and classification (from model to matrix)
\item
  PCA in practice (dimensionality reduction workflow)
\item
  Recommender systems and low-rank models (fill the missing entries)
\item
  PageRank and random walks (ranking with eigenvectors)
\item
  Numerical linear algebra essentials (floating point, BLAS/LAPACK)
\item
  Capstone problem sets and next steps (a roadmap to mastery)
\end{enumerate}

\bookmarksetup{startatroot}

\chapter{The Book}\label{the-book}

\subsection{Overtune}\label{overtune}

\emph{A soft opening, an invitation into the world of vectors and
spaces, where each step begins a journey.}

\textbf{1. Geometry's Dawn}

\begin{verbatim}
Lines cross in silence,
planes awaken with order,
numbers sketch the world.
\end{verbatim}

\textbf{2. Invitation to Learn}

\begin{verbatim}
Steps begin with dots,
arrows stretch into new paths,
the journey unfolds.
\end{verbatim}

\textbf{3. Light and Shadow}

\begin{verbatim}
Shadows fall on grids,
hidden shapes emerge in form,
clarity takes root.
\end{verbatim}

\textbf{4. The Seed of Structure}

\begin{verbatim}
One point, then a line,
spaces blossom out from rules,
infinity grows.
\end{verbatim}

\textbf{5. Whisper of Algebra}

\begin{verbatim}
Silent rules of space,
woven threads of thought align,
order sings through time.
\end{verbatim}

\textbf{6. Beginner's Welcome}

\begin{verbatim}
Empty page awaits,
axes cross like guiding hands,
first steps find their place.
\end{verbatim}

\textbf{7. Eternal Path}

\begin{verbatim}
From vectors to stars,
equations trace destiny,
patterns guide our sight.
\end{verbatim}

\newpage

\section{Chapter 1. Vectors, scalars, and
geometry}\label{chapter-1.-vectors-scalars-and-geometry-1}

\subsubsection{Opening}\label{opening}

\begin{verbatim}
Arrows in the air,
directions whisper softly—
the plane comes alive.
\end{verbatim}

\subsection{1. Scalars, Vectors, and Coordinate
Systems}\label{scalars-vectors-and-coordinate-systems}

When we begin learning linear algebra, everything starts with the
simplest building blocks: scalars and vectors. A scalar is just a single
number, like 3, --7, or π. It carries only magnitude and no direction.
Scalars are what we use for counting, measuring length, or scaling other
objects up and down. A vector, by contrast, is an ordered collection of
numbers. You can picture it as an arrow pointing somewhere in space, or
simply as a list like (2, 5) in 2D or (1, --3, 4) in 3D. Where scalars
measure ``how much,'' vectors measure both ``how much'' and ``which
way.''

\subsubsection{Coordinate Systems}\label{coordinate-systems}

To talk about vectors, we need a coordinate system. Imagine laying down
two perpendicular axes on a sheet of paper: the x-axis (left to right)
and the y-axis (up and down). Every point on the sheet can be described
with two numbers: how far along the x-axis, and how far along the
y-axis. This pair of numbers is a vector in 2D. Add a z-axis pointing up
from the page, and you have 3D space. Each coordinate system gives us a
way to describe vectors numerically, even though the underlying
``space'' is the same.

\subsubsection{Visualizing Scalars
vs.~Vectors}\label{visualizing-scalars-vs.-vectors}

\begin{itemize}
\tightlist
\item
  A scalar is like a single tick mark on a ruler.
\item
  A vector is like an arrow that starts at the origin (0, 0, \ldots) and
  ends at the point defined by its components. For example, the vector
  (3, 4) in 2D points from the origin to the point 3 units along the
  x-axis and 4 units along the y-axis.
\end{itemize}

\subsubsection{Why Start Here?}\label{why-start-here}

Understanding the difference between scalars and vectors is the
foundation for everything else in linear algebra. Every
concept-matrices, linear transformations, eigenvalues-eventually reduces
to how we manipulate vectors and scale them with scalars. Without this
distinction, the rest of the subject would have no anchor.

\subsubsection{Why It Matters}\label{why-it-matters}

Nearly every field of science and engineering depends on this idea.
Physics uses vectors for velocity, acceleration, and force. Computer
graphics uses them to represent points, colors, and transformations.
Data science treats entire datasets as high-dimensional vectors. By
mastering scalars and vectors early, you unlock the language in which
modern science and technology are written.

\subsubsection{Try It Yourself}\label{try-it-yourself}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Draw an x- and y-axis on a piece of paper. Plot the vector (2, 3).
\item
  Now draw the vector (--1, 4). Compare their directions and lengths.
\item
  Think: which of these two vectors points ``more upward''? Which is
  ``longer''?
\end{enumerate}

These simple experiments already give you intuition for the operations
you'll perform again and again in linear algebra.

\subsection{2. Vector Notation, Components, and
Arrows}\label{vector-notation-components-and-arrows}

Linear algebra gives us powerful ways to describe and manipulate
vectors, but before we can do anything with them, we need a precise
notation system. Notation is not just cosmetic-it tells us how to read,
write, and think about vectors clearly and unambiguously. In this
section, we'll explore how vectors are written, how their components are
represented, and how we can interpret them visually as arrows.

\subsubsection{Writing Vectors}\label{writing-vectors}

Vectors are usually denoted by lowercase letters in bold (like
\(\mathbf{v}, \mathbf{w}, \mathbf{x}\))\\
or with an arrow overhead (like \(\vec{v}\)).

For instance, the vector \(\mathbf{v} = (2, 5)\) is the same as
\(\vec{v} = (2, 5)\).

The style depends on context: mathematicians often use bold, physicists
often use arrows.\\
In handwritten notes, people sometimes underline vectors (e.g.,
\(\underline{v}\)) to avoid confusion with scalars.

The important thing is to distinguish vectors from scalars at a glance.

\subsubsection{Components of a Vector}\label{components-of-a-vector}

A vector in two dimensions has two components, written as \((x, y)\).\\
In three dimensions, it has three components: \((x, y, z)\).\\
More generally, an \(n\)-dimensional vector has \(n\) components:
\((v_1, v_2, \ldots, v_n)\).\\
Each component tells us how far the vector extends along one axis of the
coordinate system.

For example:

\begin{itemize}
\tightlist
\item
  \(\mathbf{v} = (3, 4)\) means the vector extends 3 units along the
  \(x\)-axis and 4 units along the \(y\)-axis.
\item
  \(\mathbf{w} = (-2, 0, 5)\) means the vector extends \(-2\) units
  along the \(x\)-axis, \(0\) along the \(y\)-axis, and 5 along the
  \(z\)-axis.
\end{itemize}

We often refer to the \(i\)-th component of a vector \(\mathbf{v}\) as
\(v_i\).\\
So, for \(\mathbf{v} = (3, 4, 5)\), we have \(v_1 = 3\), \(v_2 = 4\),
\(v_3 = 5\).

\subsubsection{Column vs.~Row Vectors}\label{column-vs.-row-vectors}

Vectors can be written in two common ways:

\begin{itemize}
\item
  As a row vector: \((v_1, v_2, v_3)\)\\
\item
  As a column vector:

  \[
  \begin{bmatrix}
  v_1 \\
  v_2 \\
  v_3
  \end{bmatrix}
  \]
\end{itemize}

Both represent the same abstract object.\\
Row vectors are convenient for quick writing, while column vectors are
essential when we start multiplying by matrices, because the dimensions
must align.

\subsubsection{Vectors as Arrows}\label{vectors-as-arrows}

The most intuitive way to picture a vector is as an arrow:

\begin{itemize}
\tightlist
\item
  It starts at the origin (0, 0, \ldots).
\item
  It ends at the point given by its components.
\end{itemize}

For example, the vector (2, 3) in 2D is drawn as an arrow from (0, 0) to
(2, 3). The arrow has both direction (where it points) and magnitude
(its length). This geometric picture makes abstract algebraic
manipulations much easier to grasp.

\subsubsection{Position Vectors vs.~Free
Vectors}\label{position-vectors-vs.-free-vectors}

There are two common interpretations of vectors:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Position vector - a vector that points from the origin to a specific
  point in space. Example: (2, 3) is the position vector for the point
  (2, 3).
\item
  Free vector - an arrow with length and direction, but not tied to a
  specific starting point. For instance, an arrow of length 5 pointing
  northeast can be drawn anywhere, but it still represents the same
  vector.
\end{enumerate}

In linear algebra, we often treat vectors as free vectors, because their
meaning does not depend on where they are drawn.

\subsubsection{Example: Reading a
Vector}\label{example-reading-a-vector}

Suppose u = (--3, 2).

\begin{itemize}
\tightlist
\item
  The first component (--3) means move 3 units left along the x-axis.
\item
  The second component (2) means move 2 units up along the y-axis. So
  the arrow points to the point (--3, 2). Even without a diagram, the
  components tell us exactly what the arrow would look like.
\end{itemize}

\subsubsection{Why It Matters}\label{why-it-matters-1}

Clear notation is the backbone of linear algebra. Without it, equations
quickly become unreadable, and intuition about direction and size is
lost. The way we write vectors determines how easily we can connect the
algebra (numbers and symbols) to the geometry (arrows and spaces). This
dual perspective-symbolic and visual-is what makes linear algebra
powerful and practical.

\subsubsection{Try It Yourself}\label{try-it-yourself-1}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Write down the vector (4, --1). Draw it on graph paper.
\item
  Rewrite the same vector as a column vector.
\item
  Translate the vector (4, --1) by moving its starting point to (2, 3)
  instead of the origin. Notice that the arrow looks the same-it just
  starts elsewhere.
\item
  For a harder challenge: draw the 3D vector (2, --1, 3). Even if you
  can't draw perfectly in 3D, try to show each component along the x, y,
  and z axes.
\end{enumerate}

By practicing both the notation and the arrow picture, you'll develop
fluency in switching between abstract symbols and concrete
visualizations. This skill will make every later concept in linear
algebra far more intuitive.

\subsection{3. Vector Addition and Scalar
Multiplication}\label{vector-addition-and-scalar-multiplication}

Once we know how to describe vectors with components and arrows, the
next step is to learn how to combine them. Two fundamental operations
form the backbone of linear algebra: adding vectors together and scaling
vectors with numbers (scalars). These two moves, though simple, generate
everything else we'll build later. With them, we can describe motion,
forces, data transformations, and more.

\subsubsection{Vector Addition in
Coordinates}\label{vector-addition-in-coordinates}

Suppose we have two vectors in 2D:\\
\(\mathbf{u} = (u_1, u_2), \quad \mathbf{v} = (v_1, v_2)\).

Their sum is defined as:\\
\[
\mathbf{u} + \mathbf{v} = (u_1 + v_1, \; u_2 + v_2).
\]

In words, you add corresponding components.\\
This works in higher dimensions too:

\[
(u_1, u_2, \ldots, u_n) + (v_1, v_2, \ldots, v_n) = (u_1 + v_1, \; u_2 + v_2, \; \ldots, \; u_n + v_n).
\]

Example: \[
(2, 3) + (-1, 4) = (2 - 1, \; 3 + 4) = (1, 7).
\]

\subsubsection{Vector Addition as
Geometry}\label{vector-addition-as-geometry}

The geometric picture is even more illuminating. If you draw vector u as
an arrow, then place the tail of v at the head of u, the arrow from the
start of u to the head of v is u + v. This is called the tip-to-tail
rule. The parallelogram rule is another visualization: place u and v
tail-to-tail, form a parallelogram, and the diagonal is their sum.

Example: u = (3, 1), v = (2, 2). Draw both from the origin. Their sum
(5, 3) is exactly the diagonal of the parallelogram they span.

\subsubsection{Scalar Multiplication in
Coordinates}\label{scalar-multiplication-in-coordinates}

Scalars stretch or shrink vectors.\\
If \(\mathbf{u} = (u_1, u_2, \ldots, u_n)\) and \(c\) is a scalar, then:

\[
c \cdot \mathbf{u} = (c \cdot u_1, \; c \cdot u_2, \; \ldots, \; c \cdot u_n).
\]

Example:

\[
2 \cdot (3, 4) = (6, 8).
\]

\[
(-1) \cdot (3, 4) = (-3, -4).
\]

Multiplying by a positive scalar stretches or compresses the arrow while
keeping the direction the same. Multiplying by a negative scalar flips
the arrow to point the opposite way.

\subsubsection{Scalar Multiplication as
Geometry}\label{scalar-multiplication-as-geometry}

Imagine the vector (1, 2). Draw it on graph paper: it goes right 1, up
2. Now double it: (2, 4). The arrow points in the same direction but is
twice as long. Halve it: (0.5, 1). It's the same direction but shorter.
Negate it: (--1, --2). Now the arrow points backward.

This geometric picture explains why we call these numbers ``scalars'':
they scale the vector.

\subsubsection{Combining Both: Linear
Combinations}\label{combining-both-linear-combinations}

Vector addition and scalar multiplication are not just separate
tricks-they combine to form the heart of linear algebra: linear
combinations.

A linear combination of vectors \(u\) and \(v\) is any vector of the
form\\
\(a \cdot u + b \cdot v\), where \(a\) and \(b\) are scalars.

Example:\\
If \(u = (1, 0)\) and \(v = (0, 1)\), then\\
\(3 \cdot u + 2 \cdot v = (3, 2)\).

This shows how any point on the grid can be reached by scaling and
adding these two basic vectors.\\
That's the essence of constructing spaces.

\subsubsection{Algebraic Properties}\label{algebraic-properties}

Vector addition and scalar multiplication obey rules that mirror
arithmetic with numbers:

\begin{itemize}
\tightlist
\item
  Commutativity: \(u + v = v + u\)\\
\item
  Associativity: \((u + v) + w = u + (v + w)\)\\
\item
  Distributivity over scalars:
  \(c \cdot (u + v) = c \cdot u + c \cdot v\)\\
\item
  Distributivity over numbers:
  \((a + b) \cdot u = a \cdot u + b \cdot u\)
\end{itemize}

These rules are not trivial bookkeeping - they guarantee that linear
algebra behaves predictably,\\
which is why it works as the language of science.

\subsubsection{Everyday Analogies}\label{everyday-analogies}

\begin{itemize}
\tightlist
\item
  Walking directions: If you walk 3 steps north and then 4 steps east,
  that's like adding (0, 3) + (4, 0) = (4, 3).
\item
  Forces in physics: If two people push on a box in different
  directions, the total force is the vector sum of their pushes.
\item
  Budget planning: Think of income and expenses as vectors of numbers.
  Combining them is just vector addition.
\end{itemize}

\subsubsection{Why It Matters}\label{why-it-matters-2}

With only these two operations-addition and scaling-you can already
describe lines, planes, and entire spaces. Any system that grows by
combining influences, like physics, economics, or machine learning, is
built on these simple rules. Later, when we define matrix
multiplication, dot products, and eigenvalues, they all reduce to
repeated patterns of adding and scaling vectors.

\subsubsection{Try It Yourself}\label{try-it-yourself-2}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Add (2, 3) and (--1, 4). Draw the result on graph paper.
\item
  Multiply (1, --2) by 3, and then add (0, 5). What is the final vector?
\item
  For a deeper challenge: Let u = (1, 2) and v = (2, --1). Sketch all
  vectors of the form a·u + b·v for integer values of a, b between --2
  and 2. Notice the grid of points you create-that's the span of these
  two vectors.
\end{enumerate}

This simple practice shows you how combining two basic vectors through
addition and scaling generates a whole structured space, the first
glimpse of linear algebra's real power.

\subsection{4. Linear Combinations and
Span}\label{linear-combinations-and-span}

After learning to add vectors and scale them, the natural next question
is: \emph{what can we build from these two operations?} The answer is
the concept of linear combinations, which leads directly to one of the
most fundamental ideas in linear algebra: the span of a set of vectors.
These ideas tell us not only what individual vectors can do, but how
groups of vectors can shape entire spaces.

\subsubsection{What Is a Linear
Combination?}\label{what-is-a-linear-combination}

A linear combination is any vector formed by multiplying vectors with
scalars and then adding the results together.

Formally, given vectors \(v_1, v_2, \ldots, v_k\) and scalars
\(a_1, a_2, \ldots, a_k\), a linear combination looks like:

\[
a_1 \cdot v_1 + a_2 \cdot v_2 + \cdots + a_k \cdot v_k.
\]

This is nothing more than repeated addition and scaling, but the idea is
powerful because it describes how vectors combine to generate new ones.

Example:\\
Let \(u = (1, 0)\) and \(v = (0, 1)\). Then any linear combination
\(a \cdot u + b \cdot v = (a, b)\).\\
This shows that every point in the 2D plane can be expressed as a linear
combination of these two simple vectors.

\subsubsection{Geometric Meaning}\label{geometric-meaning}

Linear combinations are about mixing directions and magnitudes. Each
vector acts like a ``directional ingredient,'' and the scalars control
how much of each ingredient you use.

\begin{itemize}
\tightlist
\item
  With one vector: You can only reach points on a single line through
  the origin.
\item
  With two non-parallel vectors in 2D: You can reach every point in the
  plane.
\item
  With three non-coplanar vectors in 3D: You can reach all of 3D space.
\end{itemize}

This progression shows that the power of linear combinations depends not
just on the vectors themselves but on how they relate to each other.

\subsubsection{The Span of a Set of
Vectors}\label{the-span-of-a-set-of-vectors}

The span of a set of vectors is the collection of all possible linear
combinations of them.\\
It answers the question: \emph{``What space do these vectors
generate?''}

Notation:

\[
\text{Span}\{v_1, v_2, \ldots, v_k\} =
\{a_1 v_1 + a_2 v_2 + \cdots + a_k v_k \;|\; a_i \in \mathbb{R}\}.
\]

Examples:

\begin{itemize}
\tightlist
\item
  \(\text{Span}\{(1, 0)\}\) = all multiples of \((1, 0)\), which is the
  \(x\)-axis.\\
\item
  \(\text{Span}\{(1, 0), (0, 1)\}\) = all of \(\mathbb{R}^2\), the
  entire plane.\\
\item
  \(\text{Span}\{(1, 2), (2, 4)\}\) = just the line through \((1, 2)\),
  because the second vector is a multiple of the first.
\end{itemize}

So the span depends heavily on whether the vectors add new directions or
just duplicate what's already there.

\subsubsection{Parallel and Independent
Vectors}\label{parallel-and-independent-vectors}

If vectors point in the same or opposite directions (one is a scalar
multiple of another), then their span is just a line. They don't add any
new coverage of space. But if they point in different directions, they
open up new dimensions. This leads to the critical idea of linear
independence, which we'll explore later: vectors are independent if none
of them is a linear combination of the others.

\subsubsection{Visualizing Span in Different
Dimensions}\label{visualizing-span-in-different-dimensions}

\begin{itemize}
\item
  In 2D:

  \begin{itemize}
  \tightlist
  \item
    One vector spans a line.
  \item
    Two independent vectors span the whole plane.
  \end{itemize}
\item
  In 3D:

  \begin{itemize}
  \tightlist
  \item
    One vector spans a line.
  \item
    Two independent vectors span a plane.
  \item
    Three independent vectors span all of 3D space.
  \end{itemize}
\item
  In higher dimensions: The same pattern continues. A set of k
  independent vectors spans a k-dimensional subspace inside the larger
  space.
\end{itemize}

\subsubsection{Everyday Analogies}\label{everyday-analogies-1}

\begin{itemize}
\tightlist
\item
  Color mixing: Red and blue paints can mix to make purple shades. Add
  yellow, and you can cover a broader spectrum. Similarly, vectors
  combine to produce new ones.
\item
  Cooking recipes: Ingredients (vectors) can be scaled up or down and
  combined in different amounts. The span is the full menu of dishes you
  can create with those ingredients.
\item
  Directions in navigation: If you can only walk north and south, your
  span is a line. Add east-west walking, and suddenly you can reach any
  location in the city grid.
\end{itemize}

\subsubsection{Algebraic Properties}\label{algebraic-properties-1}

\begin{itemize}
\tightlist
\item
  The span of vectors always includes the zero vector, because you can
  choose all scalars = 0.
\item
  The span is always a subspace, meaning it's closed under addition and
  scalar multiplication. If you add two vectors in the span, the result
  stays in the span.
\item
  The span grows when you add new independent vectors, but not if the
  new vector is just a combination of the old ones.
\end{itemize}

\subsubsection{Why It Matters}\label{why-it-matters-3}

Linear combinations and span are the foundation for almost everything
else in linear algebra:

\begin{itemize}
\tightlist
\item
  They define what it means for vectors to be independent or dependent.
\item
  They form the basis for solving linear systems (solutions are often
  described as spans).
\item
  They explain how dimensions arise in vector spaces.
\item
  They underpin practical methods like principal component analysis,
  where data is projected onto the span of a few important vectors.
\end{itemize}

In short, the span tells us the ``reach'' of a set of vectors, and
linear combinations are the mechanism to explore that reach.

\subsubsection{Try It Yourself}\label{try-it-yourself-3}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Take vectors (1, 0) and (0, 1). Write down three different linear
  combinations and plot them. What shape do you notice?
\item
  Try vectors (1, 2) and (2, 4). Write down three different linear
  combinations. Plot them. What's different from the previous case?
\item
  In 3D, consider (1, 0, 0) and (0, 1, 0). Describe their span. Add (0,
  0, 1). How does the span change?
\item
  Challenge: Pick vectors (1, 2, 3) and (4, 5, 6). Do they span a plane
  or all of 3D space? How can you tell?
\end{enumerate}

By experimenting with simple examples, you'll see clearly how the idea
of span captures the richness or limitations of combining vectors.

\subsection{5. Length (Norm) and
Distance}\label{length-norm-and-distance}

So far, vectors have been arrows with direction and components. To
compare them more meaningfully, we need ways to talk about how long they
are and how far apart they are. These notions are formalized through the
norm of a vector (its length) and the distance between vectors. These
concepts tie together the algebra of components and the geometry of
space.

\subsubsection{The Length (Norm) of a
Vector}\label{the-length-norm-of-a-vector}

The norm of a vector measures its magnitude, or how long the arrow is.\\
For a vector \(v = (v_1, v_2, \ldots, v_n)\) in \(n\)-dimensional space,
its norm is defined as:

\[
\|v\| = \sqrt{v_1^2 + v_2^2 + \cdots + v_n^2}.
\]

This formula comes directly from the Pythagorean theorem: the length of
the hypotenuse equals the square root of the sum of squares of the
legs.\\
In 2D, this is the familiar distance formula between the origin and a
point.

Examples:

\begin{itemize}
\item
  For \(v = (3, 4)\):\\
  \[
  \|v\| = \sqrt{3^2 + 4^2} = \sqrt{9 + 16} = 5.
  \]
\item
  For \(w = (1, -2, 2)\):\\
  \[
  \|w\| = \sqrt{1^2 + (-2)^2 + 2^2} = \sqrt{1 + 4 + 4} = \sqrt{9} = 3.
  \]
\end{itemize}

\subsubsection{Unit Vectors}\label{unit-vectors}

A unit vector is a vector whose length is exactly 1.\\
These are important because they capture direction without scaling.\\
To create a unit vector from any nonzero vector, divide by its norm:

\[
u = \frac{v}{\|v\|}.
\]

Example:\\
For \(v = (3, 4)\), the unit vector is

\[
u = \left(\tfrac{3}{5}, \tfrac{4}{5}\right).
\]

This points in the same direction as \((3, 4)\) but has length 1.

Unit vectors are like pure directions.\\
They're especially useful for projections, defining coordinate systems,
and normalizing data.

\subsubsection{Distance Between Vectors}\label{distance-between-vectors}

The distance between two vectors \(u\) and \(v\) is defined as the
length of their difference:

\[
\text{dist}(u, v) = \|u - v\|.
\]

Example:\\
Let \(u = (2, 1)\) and \(v = (5, 5)\). Then

\[
u - v = (-3, -4).
\]

Its norm is

\[
\sqrt{(-3)^2 + (-4)^2} = \sqrt{9 + 16} = 5.
\]

So the distance is 5. This matches our intuition: the straight-line
distance between points \((2, 1)\) and \((5, 5)\).

\subsubsection{Geometric Interpretation}\label{geometric-interpretation}

\begin{itemize}
\tightlist
\item
  The norm tells you how far a point is from the origin.
\item
  The distance tells you how far two points are from each other.
\end{itemize}

Both are computed with the same formula-the square root of sums of
squares-but applied in slightly different contexts.

\subsubsection{Different Kinds of Norms}\label{different-kinds-of-norms}

The formula above defines the Euclidean norm (or \(\ell_2\) norm), the
most common one.\\
But in linear algebra, other norms are also useful:

\begin{itemize}
\item
  \(\ell_1\) norm:\\
  \[
  \|v\|_1 = |v_1| + |v_2| + \cdots + |v_n|
  \]\\
  (sum of absolute values).
\item
  \(\ell_\infty\) norm:\\
  \[
  \|v\|_\infty = \max(|v_1|, |v_2|, \ldots, |v_n|)
  \]\\
  (largest component).
\end{itemize}

These norms change the geometry of ``length'' and ``distance.'' For
example, in the ℓ₁ norm, the unit circle is shaped like a diamond; in
the ℓ∞ norm, it looks like a square.

\subsubsection{Everyday Analogies}\label{everyday-analogies-2}

\begin{itemize}
\tightlist
\item
  Walking distance in a city: If streets are on a grid, the ℓ₁ norm (sum
  of absolute differences) is more natural than the Euclidean norm.
\item
  Climbing stairs: The Euclidean norm is like measuring the diagonal
  distance, but step by step you may actually walk an ℓ₁ distance.
\item
  Measuring error in data: Different norms capture different notions of
  ``closeness'' between predictions and reality. Euclidean distance
  punishes large errors heavily; ℓ₁ treats all errors equally.
\end{itemize}

\subsubsection{Algebraic Properties}\label{algebraic-properties-2}

Norms and distances satisfy critical properties that make them
consistent measures:

\begin{itemize}
\tightlist
\item
  Non-negativity: \(\|v\| \geq 0\), and \(\|v\| = 0\) only if
  \(v = 0\).\\
\item
  Homogeneity: \(\|c \cdot v\| = |c| \, \|v\|\) (scaling affects length
  predictably).\\
\item
  Triangle inequality: \(\|u + v\| \leq \|u\| + \|v\|\) (the direct path
  is shortest).\\
\item
  Symmetry (for distance): \(\text{dist}(u, v) = \text{dist}(v, u)\).
\end{itemize}

These properties are why norms and distances are robust tools across
mathematics.

\subsubsection{Why It Matters}\label{why-it-matters-4}

Understanding length and distance is the first step toward geometry in
higher dimensions. These notions:

\begin{itemize}
\tightlist
\item
  Allow us to compare vectors quantitatively.
\item
  Form the basis of concepts like angles, orthogonality, and
  projections.
\item
  Underpin optimization problems (e.g., ``find the closest vector'' is
  central to machine learning).
\item
  Define the geometry of spaces, which changes dramatically depending on
  which norm you use.
\end{itemize}

\subsubsection{Try It Yourself}\label{try-it-yourself-4}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute the norm of (6, 8). Then divide by the norm to find its unit
  vector.
\item
  Find the distance between (1, 1, 1) and (4, 5, 6).
\item
  Compare the Euclidean and Manhattan (ℓ₁) distances between (0, 0) and
  (3, 4). Which one matches your intuition if you were walking along a
  city grid?
\item
  Challenge: For vectors u = (2, --1, 3) and v = (--2, 0, 1), compute ‖u
  -- v‖. Then explain what this distance means geometrically.
\end{enumerate}

By working through these examples, you'll see how norms and distances
make abstract vectors feel as real as points and arrows you can measure
in everyday life.

\subsection{6. Dot Product (Algebraic and Geometric
Views)}\label{dot-product-algebraic-and-geometric-views}

The dot product is one of the most fundamental operations in linear
algebra. It looks like a simple formula, but it unlocks the ability to
measure angles, detect orthogonality, project one vector onto another,
and compute energy or work in physics. Understanding it requires seeing
both the algebraic view (a formula on components) and the geometric view
(a way to compare directions).

\subsubsection{Algebraic Definition}\label{algebraic-definition}

For two vectors of the same dimension, \(u = (u_1, u_2, \ldots, u_n)\)
and \(v = (v_1, v_2, \ldots, v_n)\), the dot product is defined as:

\[
u \cdot v = u_1 v_1 + u_2 v_2 + \cdots + u_n v_n.
\]

This is simply multiplying corresponding components and summing the
results.

Examples:

\begin{itemize}
\tightlist
\item
  \((2, 3) \cdot (4, 5) = (2 \times 4) + (3 \times 5) = 8 + 15 = 23\)\\
\item
  \((1, -2, 3) \cdot (0, 4, -1) = (1 \times 0) + (-2 \times 4) + (3 \times -1) = 0 - 8 - 3 = -11\)
\end{itemize}

Notice that the dot product is always a scalar, not a vector.

\subsubsection{Geometric Definition}\label{geometric-definition}

The dot product can also be defined in terms of vector length and angle:

\[
u \cdot v = \|u\| \, \|v\| \cos(\theta),
\]

where \(\theta\) is the angle between \(u\) and \(v\)
(\(0^\circ \leq \theta \leq 180^\circ\)).

This formula tells us:

\begin{itemize}
\tightlist
\item
  If the angle is acute (less than \(90^\circ\)), \(\cos(\theta) > 0\),
  so the dot product is positive.\\
\item
  If the angle is right (exactly \(90^\circ\)), \(\cos(\theta) = 0\), so
  the dot product is 0.\\
\item
  If the angle is obtuse (greater than \(90^\circ\)),
  \(\cos(\theta) < 0\), so the dot product is negative.
\end{itemize}

Thus, the sign of the dot product encodes directional alignment.

\subsubsection{Connecting the Two
Definitions}\label{connecting-the-two-definitions}

At first glance, the algebraic sum of products and the geometric
length--angle formula seem unrelated. But they are equivalent. To see
why, consider the law of cosines applied to a triangle formed by u, v,
and u -- v. Expanding both sides leads directly to the equivalence
between the two formulas. This dual interpretation is what makes the dot
product so powerful: it is both a computation rule and a geometric
measurement.

\subsubsection{Orthogonality}\label{orthogonality}

Two vectors are orthogonal (perpendicular) if and only if their dot
product is zero:

\[
u \cdot v = 0 \;\;\Longleftrightarrow\;\; \theta = 90^\circ.
\]

This gives us an algebraic way to check for perpendicularity without
drawing diagrams.

Example:\\
\((2, 1) \cdot (-1, 2) = (2 \times -1) + (1 \times 2) = -2 + 2 = 0\),\\
so the vectors are orthogonal.

\subsubsection{Projections}\label{projections}

The dot product also provides a way to project one vector onto
another.\\
The scalar projection of \(u\) onto \(v\) is:

\[
\text{proj}_{\text{scalar}}(u \text{ onto } v) = \frac{u \cdot v}{\|v\|}.
\]

The vector projection is then:

\[
\text{proj}_{\text{vector}}(u \text{ onto } v) = \frac{u \cdot v}{\|v\|^2} \, v.
\]

This allows us to decompose vectors into ``parallel'' and
``perpendicular'' components, which is central in geometry, physics, and
data analysis.

\subsubsection{Examples}\label{examples}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Compute \(u = (3, 4)\) and \(v = (4, 3)\).

  \begin{itemize}
  \tightlist
  \item
    Dot product: \((3 \times 4) + (4 \times 3) = 12 + 12 = 24\).\\
  \item
    Norms: \(\|u\| = 5\), \(\|v\| = 5\).\\
  \item
    \(\cos(\theta) = \tfrac{24}{5 \times 5} = \tfrac{24}{25} \approx 0.96\),
    so \(\theta \approx 16^\circ\).\\
    These vectors are nearly parallel.
  \end{itemize}
\item
  Compute \(u = (1, 2, -1)\) and \(v = (2, -1, 1)\).

  \begin{itemize}
  \tightlist
  \item
    Dot product:
    \((1 \times 2) + (2 \times -1) + (-1 \times 1) = 2 - 2 - 1 = -1\).\\
  \item
    Norms: \(\|u\| = \sqrt{6}\), \(\|v\| = \sqrt{6}\).\\
  \item
    \(\cos(\theta) = \tfrac{-1}{\sqrt{6} \times \sqrt{6}} = -\tfrac{1}{6}\),
    so \(\theta \approx 99.6^\circ\).\\
    Slightly obtuse.
  \end{itemize}
\end{enumerate}

\subsubsection{Physical Interpretation}\label{physical-interpretation}

In physics, the dot product computes work:

\[
\text{Work} = \text{Force} \cdot \text{Displacement}
           = \|\text{Force}\| \, \|\text{Displacement}\| \cos(\theta).
\]

Only the component of the force in the direction of motion contributes.
If you push straight down on a box while trying to move it horizontally,
the dot product is zero: no work is done in the direction of motion.

\subsubsection{Everyday Analogies}\label{everyday-analogies-3}

\begin{itemize}
\tightlist
\item
  Teamwork analogy: Two people pushing a car. If they push in nearly the
  same direction, the dot product is large and positive (strong
  cooperation). If they push at right angles, the dot product is zero
  (they don't help each other). If they push in opposite directions, the
  dot product is negative (they work against each other).
\item
  Conversation analogy: Think of two ideas. If they're aligned, the
  ``dot product'' of their meanings is high. If they're unrelated, it's
  zero. If they contradict, it's negative.
\end{itemize}

\subsubsection{Algebraic Properties}\label{algebraic-properties-3}

\begin{itemize}
\tightlist
\item
  Commutative: \(u \cdot v = v \cdot u\)\\
\item
  Distributive: \(u \cdot (v + w) = u \cdot v + u \cdot w\)\\
\item
  Scalar compatibility: \((c \cdot u) \cdot v = c \,(u \cdot v)\)\\
\item
  Non-negativity: \(v \cdot v = \|v\|^2 \geq 0\)
\end{itemize}

These guarantee that the dot product behaves consistently and meshes
with the structure of vector spaces.

\subsubsection{Why It Matters}\label{why-it-matters-5}

The dot product is the first bridge between algebra and geometry. It:

\begin{itemize}
\tightlist
\item
  Defines angles and orthogonality in higher dimensions.
\item
  Powers projections and decompositions, which underlie least squares,
  regression, and data fitting.
\item
  Appears in physics as energy, power, and work.
\item
  Serves as the kernel of many machine learning methods (e.g.,
  similarity measures in high-dimensional spaces).
\end{itemize}

Without the dot product, linear algebra would lack a way to connect
numbers with geometry and meaning.

\subsubsection{Try It Yourself}\label{try-it-yourself-5}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute (2, --1) · (--3, 4). Then find the angle between them.
\item
  Check if (1, 2, 3) and (2, 4, 6) are orthogonal. What does the dot
  product tell you?
\item
  Find the projection of (3, 1) onto (1, 2). Draw the original vector,
  the projection, and the perpendicular component.
\item
  In physics terms: Suppose a 10 N force is applied at 60° to the
  direction of motion, and the displacement is 5 m. How much work is
  done?
\end{enumerate}

These exercises reveal the dual power of the dot product: as a formula
to compute and as a geometric tool to interpret.

\subsection{7. Angles Between Vectors and
Cosine}\label{angles-between-vectors-and-cosine}

Having defined the dot product, we are now ready to measure angles
between vectors. In everyday life, angles tell us how two lines or
directions relate-whether they point the same way, are perpendicular, or
are opposed. In linear algebra, the dot product and cosine function give
us a precise, generalizable way to define angles in any dimension, not
just in 2D or 3D. This section explores how we compute, interpret, and
apply vector angles.

\subsubsection{The Definition of an Angle Between
Vectors}\label{the-definition-of-an-angle-between-vectors}

For two nonzero vectors \(u\) and \(v\), the angle \(\theta\) between
them is defined by:

\[
\cos(\theta) = \frac{u \cdot v}{\|u\| \, \|v\|}.
\]

This formula comes directly from the geometric definition of the dot
product.\\
Rearranging gives:

\[
\theta = \arccos\!\left(\frac{u \cdot v}{\|u\| \, \|v\|}\right).
\]

Key points:

\begin{itemize}
\tightlist
\item
  \(\theta\) is always between \(0^\circ\) and \(180^\circ\) (or \(0\)
  and \(\pi\) radians).\\
\item
  The denominator normalizes the dot product by dividing by the product
  of lengths, so the result is dimensionless and always between \(-1\)
  and \(1\).\\
\item
  The cosine value directly encodes alignment: positive, zero, or
  negative.
\end{itemize}

\subsubsection{Interpretation of Cosine
Values}\label{interpretation-of-cosine-values}

The cosine tells us about the directional relationship:

\begin{itemize}
\tightlist
\item
  \(\cos(\theta) = 1 \;\;\Rightarrow\;\; \theta = 0^\circ\) → vectors
  point in exactly the same direction.\\
\item
  \(\cos(\theta) = 0 \;\;\Rightarrow\;\; \theta = 90^\circ\) → vectors
  are orthogonal (perpendicular).\\
\item
  \(\cos(\theta) = -1 \;\;\Rightarrow\;\; \theta = 180^\circ\) → vectors
  point in exactly opposite directions.\\
\item
  \(\cos(\theta) > 0\) → acute angle → vectors point more ``together''
  than apart.\\
\item
  \(\cos(\theta) < 0\) → obtuse angle → vectors point more ``against''
  each other.
\end{itemize}

Thus, the cosine compresses geometric alignment into a single number.

\subsubsection{Examples}\label{examples-1}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \(u = (1, 0), \; v = (0, 1)\)

  \begin{itemize}
  \tightlist
  \item
    Dot product: \(1 \times 0 + 0 \times 1 = 0\)\\
  \item
    Norms: \(1\) and \(1\)\\
  \item
    \(\cos(\theta) = 0 \;\Rightarrow\; \theta = 90^\circ\)\\
    The vectors are perpendicular, as expected.
  \end{itemize}
\item
  \(u = (2, 3), \; v = (4, 6)\)

  \begin{itemize}
  \tightlist
  \item
    Dot product: \((2 \times 4) + (3 \times 6) = 8 + 18 = 26\)\\
  \item
    Norms: \(\sqrt{2^2 + 3^2} = \sqrt{13}\), and
    \(\sqrt{4^2 + 6^2} = \sqrt{52} = 2\sqrt{13}\)\\
  \item
    \(\cos(\theta) = \tfrac{26}{\sqrt{13} \cdot 2\sqrt{13}} = \tfrac{26}{26} = 1\)\\
  \item
    \(\theta = 0^\circ\)\\
    These vectors are multiples, so they align perfectly.
  \end{itemize}
\item
  \(u = (1, 1), \; v = (-1, 1)\)

  \begin{itemize}
  \tightlist
  \item
    Dot product: \((1 \times -1) + (1 \times 1) = -1 + 1 = 0\)\\
  \item
    \(\cos(\theta) = 0 \;\Rightarrow\; \theta = 90^\circ\)\\
    The vectors are perpendicular, forming diagonals of a square.
  \end{itemize}
\end{enumerate}

\subsubsection{Angles in Higher
Dimensions}\label{angles-in-higher-dimensions}

The beauty of the formula is that it works in any dimension.\\
Even in \(\mathbb{R}^{100}\) or higher, we can define the angle between
two vectors using only their dot product and norms.

While we cannot visualize the geometry directly in high dimensions, the
cosine formula still captures how aligned two directions are:

\[
\cos(\theta) = \frac{u \cdot v}{\|u\| \, \|v\|}.
\]

This is critical in machine learning, where data often lives in very
high-dimensional spaces.

\subsubsection{Cosine Similarity}\label{cosine-similarity}

The cosine of the angle between two vectors is often called cosine
similarity. It is widely used in data analysis and machine learning to
measure how similar two data vectors are, independent of their
magnitude.

\begin{itemize}
\tightlist
\item
  In text mining, documents are turned into word-frequency vectors.
  Cosine similarity measures how ``close in topic'' two documents are,
  regardless of length.
\item
  In recommendation systems, cosine similarity compares user preference
  vectors to suggest similar users or items.
\end{itemize}

This demonstrates how a geometric concept extends far beyond pure math.

\subsubsection{Orthogonality Revisited}\label{orthogonality-revisited}

The angle formula reinforces the special role of orthogonality.\\
If \(\cos(\theta) = 0\), then \(u \cdot v = 0\).

This means the dot product not only computes length but also serves as a
direct test for perpendicularity.\\
This algebraic shortcut is far easier than manually checking geometric
right angles.

\subsubsection{Angles and Projections}\label{angles-and-projections}

Angles are closely tied to projections.\\
The length of the projection of \(u\) onto \(v\) is
\(\|u\|\cos(\theta)\).

If the angle is small, the projection is large --- most of \(u\) lies in
the direction of \(v\).\\
If the angle is close to \(90^\circ\), the projection shrinks toward
zero.

Thus, the cosine acts as a scaling factor between directions.

\subsubsection{Everyday Analogies}\label{everyday-analogies-4}

\begin{itemize}
\item
  Teamwork analogy: If two people push a heavy object, the effectiveness
  of their combined effort depends on the angle.\\
  If they push in nearly the same direction (small \(\theta\)), they
  cooperate efficiently.\\
  If they push at right angles (\(\theta = 90^\circ\)), they waste
  effort.\\
  If they push opposite each other (\(\theta \approx 180^\circ\)), they
  cancel out.
\item
  Conversation analogy: If two people's opinions align
  (\(\cos \theta \approx 1\)), they agree strongly.\\
  If they are orthogonal (\(\cos \theta = 0\)), they are unrelated.\\
  If they oppose each other (\(\cos \theta \approx -1\)), they
  fundamentally disagree.
\item
  Navigation analogy: If two roads meet at a small angle, their
  directions are similar.\\
  If they cross perpendicularly, their directions are independent.
\end{itemize}

\subsubsection{Why It Matters}\label{why-it-matters-6}

Angles between vectors provide:

\begin{itemize}
\tightlist
\item
  A way to generalize geometry beyond 2D/3D.
\item
  A measure of similarity in high-dimensional data.
\item
  The foundation for orthogonality, projections, and decomposition of
  spaces.
\item
  A tool for optimization: in gradient descent, for example, the angle
  between the gradient and step direction determines how effectively we
  reduce error.
\end{itemize}

Without the ability to measure angles, we could not connect algebraic
manipulations with geometric intuition or practical applications.

\subsubsection{Try It Yourself}\label{try-it-yourself-6}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute the angle between (2, 1) and (1, --1). Interpret the result.
\item
  Find two vectors in 3D that form a 60° angle. Verify using the cosine
  formula.
\item
  Consider word vectors for ``cat'' and ``dog'' in a machine learning
  model. Why might cosine similarity be a better measure of similarity
  than Euclidean distance?
\item
  Challenge: In \(\mathbb{R}^3\), find a vector orthogonal to both (1,
  2, 3) and (3, 2, 1). What angle does it make with each of them?
\end{enumerate}

By experimenting with these problems, you will see how angles provide
the missing link between algebraic formulas and geometric meaning in
linear algebra.

\subsection{8. Projections and
Decompositions}\label{projections-and-decompositions}

In earlier sections, we saw how the dot product measures alignment and
how the cosine formula gives us angles between vectors. The next natural
step is to use these tools to project one vector onto another.
Projection is a way to ``shadow'' one vector onto the direction of
another, splitting vectors into meaningful parts: one along a given
direction and one perpendicular to it. This is the essence of
decomposition, and it is everywhere in linear algebra, geometry,
physics, and data science.

\subsubsection{Scalar Projection}\label{scalar-projection}

The scalar projection of a vector \(u\) onto a vector \(v\) measures how
much of \(u\) lies in the direction of \(v\). It is given by:

\[
\text{proj}_{\text{scalar}}(u \text{ onto } v) = \frac{u \cdot v}{\|v\|}.
\]

\begin{itemize}
\tightlist
\item
  If this value is positive, \(u\) has a component pointing in the same
  direction as \(v\).\\
\item
  If it is negative, \(u\) points partly in the opposite direction.\\
\item
  If it is zero, \(u\) is completely perpendicular to \(v\).
\end{itemize}

Example:\\
\(u = (3, 4)\), \(v = (1, 0)\).\\
Dot product: \((3 \times 1 + 4 \times 0) = 3\).\\
\(\|v\| = 1\).\\
So the scalar projection is \(3\). This tells us \(u\) has a ``shadow''
of length \(3\) on the \(x\)-axis.

\subsubsection{Vector Projection}\label{vector-projection}

The vector projection gives the actual arrow in the direction of \(v\)
that corresponds to this scalar amount:

\[
\text{proj}_{\text{vector}}(u \text{ onto } v) = \frac{u \cdot v}{\|v\|^2} \, v.
\]

This formula normalizes \(v\) into a unit vector, then scales it by the
scalar projection.\\
The result is a new vector lying along \(v\), capturing exactly the
``parallel'' part of \(u\).

Example:\\
\(u = (3, 4)\), \(v = (1, 2)\)

\begin{itemize}
\tightlist
\item
  Dot product: \(3 \times 1 + 4 \times 2 = 3 + 8 = 11\)\\
\item
  Norm squared of \(v\): \((1^2 + 2^2) = 5\)\\
\item
  Coefficient: \(11 / 5 = 2.2\)\\
\item
  Projection vector: \(2.2 \cdot (1, 2) = (2.2, 4.4)\)
\end{itemize}

So the part of \((3, 4)\) in the direction of \((1, 2)\) is
\((2.2, 4.4)\).

\subsubsection{Perpendicular Component}\label{perpendicular-component}

Once we have the projection, we can find the perpendicular component
(often called the rejection) simply by subtracting:

\[
u_{\perp} = u - \text{proj}_{\text{vector}}(u \text{ onto } v).
\]

This gives the part of \(u\) that is entirely orthogonal to \(v\).

Example continued:\\
\(u_{\perp} = (3, 4) - (2.2, 4.4) = (0.8, -0.4)\)

Check:\\
\((0.8, -0.4) \cdot (1, 2) = 0.8 \times 1 + (-0.4) \times 2 = 0.8 - 0.8 = 0\).\\
Indeed, orthogonal.

\subsubsection{Geometric Picture}\label{geometric-picture}

Projection is like dropping a perpendicular from one vector onto
another. Imagine shining a light perpendicular to v: the shadow of u on
the line spanned by v is the projection. This visualization explains why
projections split vectors naturally into two pieces:

\begin{itemize}
\tightlist
\item
  Parallel part: Along the line of v.
\item
  Perpendicular part: Orthogonal to v, forming a right angle.
\end{itemize}

Together, these two parts reconstruct the original vector exactly.

\subsubsection{Decomposition of Vectors}\label{decomposition-of-vectors}

Every vector \(u\) can be decomposed relative to another vector \(v\)
into two parts:

\[
u = \text{proj}_{\text{vector}}(u \text{ onto } v) + \big(u - \text{proj}_{\text{vector}}(u \text{ onto } v)\big).
\]

This decomposition is unique and geometrically meaningful.\\
It generalizes to subspaces: we can project onto entire planes or
higher-dimensional spans, splitting a vector into a ``within-subspace''
part and a ``perpendicular-to-subspace'' part.

\subsubsection{Applications}\label{applications}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Physics (Work and Forces): Work is the projection of force onto
  displacement. Only the part of the force in the direction of motion
  contributes. Example: Pushing on a sled partly sideways wastes
  effort-the sideways component projects to zero.
\item
  Geometry and Engineering: Projections are used in CAD (computer-aided
  design) to flatten 3D objects onto 2D surfaces, like blueprints or
  shadows.
\item
  Computer Graphics: Rendering 3D scenes onto a 2D screen is
  fundamentally a projection process.
\item
  Data Science: Projecting high-dimensional data onto a
  lower-dimensional subspace (like the first two principal components in
  PCA) makes patterns visible while preserving as much information as
  possible.
\item
  Signal Processing: Decomposition into projections onto sine and cosine
  waves forms the basis of Fourier analysis, which powers audio, image,
  and video compression.
\end{enumerate}

\subsubsection{Everyday Analogies}\label{everyday-analogies-5}

\begin{itemize}
\tightlist
\item
  Flashlight shadow: Imagine shining a flashlight so that a stick casts
  a shadow on the floor. The shadow is the projection of the stick onto
  the floor plane.
\item
  Team effort: If two people pull a box, the effective progress in one
  person's direction is the projection of the other's pull onto that
  direction.
\item
  Grades analogy: If your performance is a mix of effort and
  distraction, the projection onto ``effort'' shows how much of your
  work aligns with real progress.
\end{itemize}

\subsubsection{Algebraic Properties}\label{algebraic-properties-4}

\begin{itemize}
\tightlist
\item
  Projections are linear: proj(u + w) = proj(u) + proj(w).
\item
  The perpendicular part is always orthogonal to the direction of
  projection.
\item
  The decomposition is unique: no other pair of parallel and
  perpendicular vectors will reconstruct u.
\item
  The projection operator onto a unit vector v̂ satisfies: proj(u) = (v̂
  v̂ᵀ)u, showing how projection can be expressed in matrix form.
\end{itemize}

\subsubsection{Why It Matters}\label{why-it-matters-7}

Projection is not just a geometric trick; it is the core of many
advanced topics:

\begin{itemize}
\tightlist
\item
  Least squares regression is finding the projection of a data vector
  onto the span of predictor vectors.
\item
  Orthogonal decompositions like Gram--Schmidt and QR factorization rely
  on projections to build orthogonal bases.
\item
  Optimization methods often involve projecting guesses back onto
  feasible sets.
\item
  Machine learning uses projections constantly to reduce dimensions,
  compare vectors, and align features.
\end{itemize}

Without projection, we could not cleanly separate influence along
directions or reduce complexity in structured ways.

\subsubsection{Try It Yourself}\label{try-it-yourself-7}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Project (2, 3) onto (1, 0). What does the perpendicular component look
  like?
\item
  Project (3, 1) onto (2, 2). Verify the perpendicular part is
  orthogonal.
\item
  Decompose (5, 5, 0) into parallel and perpendicular parts relative to
  (1, 0, 0).
\item
  Challenge: Write the projection matrix for projecting onto (1, 2).
  Apply it to (3, 4). Does it match the formula?
\end{enumerate}

Through these exercises, you will see that projection is more than an
operation-it is a lens through which we decompose, interpret, and
simplify vectors and spaces.

\subsection{9. Cauchy--Schwarz and Triangle
Inequalities}\label{cauchyschwarz-and-triangle-inequalities}

Linear algebra is not only about operations with vectors-it also
involves understanding the fundamental relationships between them. Two
of the most important results in this regard are the Cauchy--Schwarz
inequality and the triangle inequality. These are cornerstones of vector
spaces because they establish precise boundaries for lengths, angles,
and inner products. Without them, the geometry of linear algebra would
fall apart.

\subsubsection{The Cauchy--Schwarz
Inequality}\label{the-cauchyschwarz-inequality}

For any two vectors \(u\) and \(v\) in \(\mathbb{R}^n\), the
Cauchy--Schwarz inequality states:

\[
|u \cdot v| \leq \|u\| \, \|v\|.
\]

This means that the absolute value of the dot product of two vectors is
always less than or equal to the product of their lengths.

Equality holds if and only if u and v are linearly dependent (i.e., one
is a scalar multiple of the other).

\paragraph{Why It Is True}\label{why-it-is-true}

Recall the geometric formula for the dot product:

\[
u \cdot v = \|u\| \, \|v\| \cos(\theta).
\]

Since \(-1 \leq \cos(\theta) \leq 1\), the magnitude of the dot product
cannot exceed \(\|u\| \, \|v\|\).\\
This is exactly the inequality.

\paragraph{Example}\label{example}

Let \(u = (3, 4)\) and \(v = (-4, 3)\).

\begin{itemize}
\tightlist
\item
  Dot product: \((3 \times -4) + (4 \times 3) = -12 + 12 = 0\)\\
\item
  Norms: \(\|u\| = 5\), \(\|v\| = 5\)\\
\item
  Product of norms: \(25\)\\
\item
  \(|u \cdot v| = 0 \leq 25\), which satisfies the inequality
\end{itemize}

Equality does not hold since they are not multiples - they are
perpendicular.

\paragraph{Intuition}\label{intuition}

The inequality tells us that two vectors can never ``overlap'' more
strongly than the product of their magnitudes. If they align perfectly,
the overlap is maximum (equality). If they're perpendicular, the overlap
is zero.

Think of it as: ``the shadow of one vector on another can never be
longer than the vector itself.''

\subsubsection{The Triangle Inequality}\label{the-triangle-inequality}

For any vectors \(u\) and \(v\), the triangle inequality states:

\[
\|u + v\| \leq \|u\| + \|v\|.
\]

This mirrors the geometric fact that in a triangle, any side is at most
as long as the sum of the other two sides.

\paragraph{Example}\label{example-1}

Let \(u = (1, 2)\) and \(v = (3, 4)\).

\begin{itemize}
\tightlist
\item
  \(\|u + v\| = \|(4, 6)\| = \sqrt{16 + 36} = \sqrt{52} \approx 7.21\)\\
\item
  \(\|u\| + \|v\| = \sqrt{5} + 5 \approx 2.24 + 5 = 7.24\)
\end{itemize}

Indeed, \(7.21 \leq 7.24\), very close in this case.

\paragraph{Equality Case}\label{equality-case}

The triangle inequality becomes equality when the vectors point in
exactly the same direction (or are scalar multiples with nonnegative
coefficients). For example, (1, 1) and (2, 2) produce equality because
adding them gives a vector whose length equals the sum of their lengths.

\subsubsection{Everyday Analogies}\label{everyday-analogies-6}

\begin{itemize}
\tightlist
\item
  Cauchy--Schwarz: Imagine comparing two people's study habits across
  multiple subjects. Each vector represents hours spent in each subject.
  The dot product represents how much their habits ``align.''
  Cauchy--Schwarz says the alignment can never exceed the product of
  their individual efforts.
\item
  Triangle inequality: Think of walking in a city. If you want to get
  from home to the store and then from the store to the park, the total
  distance is at least as long as going straight from home to the park.
  There is no shortcut longer than the direct route.
\end{itemize}

\subsubsection{Extensions}\label{extensions}

\begin{itemize}
\tightlist
\item
  These inequalities hold in all inner product spaces, not just ℝⁿ. This
  means they apply to functions, sequences, and more abstract
  mathematical objects.
\item
  In Hilbert spaces (infinite-dimensional generalizations), they remain
  just as essential.
\end{itemize}

\subsubsection{Why They Matter}\label{why-they-matter}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  They guarantee that the dot product and norm are well-behaved and
  geometrically meaningful.
\item
  They ensure that the norm satisfies the requirements of a distance
  measure: nonnegativity, symmetry, and triangle inequality.
\item
  They underpin the validity of projections, orthogonality, and least
  squares methods.
\item
  They are essential in proving convergence of algorithms, error bounds,
  and stability in numerical linear algebra.
\end{enumerate}

Without these inequalities, we could not trust that the geometry of
vector spaces behaves consistently.

\subsubsection{Try It Yourself}\label{try-it-yourself-8}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Verify Cauchy--Schwarz for (2, --1, 3) and (--1, 4, 0). Compute both
  sides.
\item
  Try the triangle inequality for (--3, 4) and (5, --12). Does equality
  hold?
\item
  Find two vectors where Cauchy--Schwarz is an equality. Explain why.
\item
  Challenge: Prove the triangle inequality in \(\mathbb{R}^2\) using
  only the Pythagorean theorem and algebra, without relying on dot
  products.
\end{enumerate}

Working through these problems will show you why these inequalities are
not abstract curiosities but the structural glue of linear algebra's
geometry.

\subsection{\texorpdfstring{10. Orthonormal sets in \(\mathbb{R}^2\) and
\(\mathbb{R}^3\)}{10. Orthonormal sets in \textbackslash mathbb\{R\}\^{}2 and \textbackslash mathbb\{R\}\^{}3}}\label{orthonormal-sets-in-mathbbr2-and-mathbbr3}

Up to now, we've discussed vectors, their lengths, angles, and how to
project one onto another. A natural culmination of these ideas is the
concept of orthonormal sets. These are collections of vectors that are
not only orthogonal (mutually perpendicular) but also normalized (each
of length 1). Orthonormal sets form the cleanest, most efficient
coordinate systems in linear algebra. They are the mathematical
equivalent of having rulers at right angles, perfectly calibrated to
unit length.

\subsubsection{Orthogonal and
Normalized}\label{orthogonal-and-normalized}

Let's break the term ``orthonormal'' into two parts:

\begin{itemize}
\item
  Orthogonal: Two vectors \(u\) and \(v\) are orthogonal if
  \(u \cdot v = 0\).\\
  In \(\mathbb{R}^2\), this means the vectors meet at a right angle.\\
  In \(\mathbb{R}^3\), it means they form perpendicular directions.
\item
  Normalized: A vector \(v\) is normalized if its length is \(1\), i.e.,
  \(\|v\| = 1\).\\
  Such vectors are called unit vectors.
\end{itemize}

When we combine both conditions, we get orthonormal vectors: vectors
that are both perpendicular to each other and have unit length.

\subsubsection{\texorpdfstring{Orthonormal Sets in
\(\mathbb{R}^2\)}{Orthonormal Sets in \textbackslash mathbb\{R\}\^{}2}}\label{orthonormal-sets-in-mathbbr2}

In two dimensions, an orthonormal set typically consists of two
vectors.\\
A classic example is:

\(e_1 = (1, 0), \quad e_2 = (0, 1)\)

\begin{itemize}
\tightlist
\item
  Dot product:
  \(e_1 \cdot e_2 = (1 \times 0 + 0 \times 1) = 0 \;\;\Rightarrow\;\;\)
  orthogonal\\
\item
  Lengths: \(\|e_1\| = 1\), \(\|e_2\| = 1 \;\;\Rightarrow\;\;\)
  normalized
\end{itemize}

Thus, \(\{e_1, e_2\}\) is an orthonormal set.\\
In fact, this is the standard basis for \(\mathbb{R}^2\).\\
Any vector \((x, y)\) can be written as \(x e_1 + y e_2\).\\
This is the simplest coordinate system.

\subsubsection{\texorpdfstring{Orthonormal Sets in
\(\mathbb{R}^3\)}{Orthonormal Sets in \textbackslash mathbb\{R\}\^{}3}}\label{orthonormal-sets-in-mathbbr3}

In three dimensions, an orthonormal set usually has three vectors.\\
The standard basis is:

\(e_1 = (1, 0, 0), \quad e_2 = (0, 1, 0), \quad e_3 = (0, 0, 1)\)

\begin{itemize}
\tightlist
\item
  Each pair has dot product zero, so they are orthogonal\\
\item
  Each has length \(1\), so they are normalized\\
\item
  Together, they span all of \(\mathbb{R}^3\)
\end{itemize}

Geometrically, they correspond to the \(x\)-, \(y\)-, and \(z\)-axes in
3D space.\\
Any vector \((x, y, z)\) can be written as a linear combination
\(x e_1 + y e_2 + z e_3\).

\subsubsection{Beyond the Standard
Basis}\label{beyond-the-standard-basis}

The standard basis is not the only orthonormal set. For example:

\(u = \left(\tfrac{1}{\sqrt{2}}, \tfrac{1}{\sqrt{2}}\right), \quad
 v = \left(-\tfrac{1}{\sqrt{2}}, \tfrac{1}{\sqrt{2}}\right)\)

\begin{itemize}
\tightlist
\item
  Dot product:
  \((\tfrac{1}{\sqrt{2}})(-\tfrac{1}{\sqrt{2}}) + (\tfrac{1}{\sqrt{2}})(\tfrac{1}{\sqrt{2}}) = -\tfrac{1}{2} + \tfrac{1}{2} = 0\)\\
\item
  Lengths:
  \(\sqrt{(\tfrac{1}{\sqrt{2}})^2 + (\tfrac{1}{\sqrt{2}})^2} = \sqrt{\tfrac{1}{2} + \tfrac{1}{2}} = 1\)
\end{itemize}

So \(\{u, v\}\) is also orthonormal in \(\mathbb{R}^2\).\\
These vectors are rotated \(45^\circ\) relative to the standard axes.

Similarly, in \(\mathbb{R}^3\), you can construct rotated orthonormal
sets (such as unit vectors along diagonals), as long as the conditions
of perpendicularity and unit length hold.

\subsubsection{Properties of Orthonormal
Sets}\label{properties-of-orthonormal-sets}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Simplified coordinates: If \(\{v_1, \ldots, v_k\}\) is an orthonormal
  set, then for any vector \(u\) in their span, the coefficients are
  easy to compute:\\
  \[
  c_i = u \cdot v_i
  \]\\
  This is much simpler than solving systems of equations.
\item
  Pythagorean theorem generalized: If vectors are orthonormal, the
  squared length of their sum is the sum of the squares of their
  coefficients.\\
  For example, if \(u = a v_1 + b v_2\), then\\
  \[
  \|u\|^2 = a^2 + b^2
  \]
\item
  Projection is easy: Projecting onto an orthonormal set is
  straightforward --- just take dot products.
\item
  Matrices become nice: When vectors form the columns of a matrix,
  orthonormality makes that matrix an orthogonal matrix, which has
  special properties: its transpose equals its inverse, and it preserves
  lengths and angles.
\end{enumerate}

\subsubsection{\texorpdfstring{Importance in \(\mathbb{R}^2\) and
\(\mathbb{R}^3\)}{Importance in \textbackslash mathbb\{R\}\^{}2 and \textbackslash mathbb\{R\}\^{}3}}\label{importance-in-mathbbr2-and-mathbbr3}

\begin{itemize}
\tightlist
\item
  In geometry, orthonormal bases correspond to coordinate axes.
\item
  In physics, they represent independent directions of motion or force.
\item
  In computer graphics, orthonormal sets define camera axes and object
  rotations.
\item
  In engineering, they simplify stress, strain, and rotation analysis.
\end{itemize}

Even though \(\mathbb{R}^2\) and \(\mathbb{R}^3\) are relatively simple,
the same ideas extend naturally to higher dimensions, where
visualization is impossible but the algebra is identical.

\subsubsection{Everyday Analogies}\label{everyday-analogies-7}

\begin{itemize}
\tightlist
\item
  Navigating a city: Imagine two perpendicular streets, each marked in
  meters. Walking 3 units east and 4 units north gives coordinates (3,
  4). That's an orthonormal system.
\item
  Furniture assembly: When you're told to align one part ``straight up''
  and another ``straight across,'' you're working with orthonormal
  directions.
\item
  Digital screens: Computer monitors use pixel grids aligned
  horizontally and vertically, a practical realization of
  orthonormality.
\end{itemize}

\subsubsection{Why Orthonormal Sets
Matter}\label{why-orthonormal-sets-matter}

Orthonormality is the gold standard for building bases in linear
algebra:

\begin{itemize}
\tightlist
\item
  It makes calculations fast and simple.
\item
  It ensures numerical stability in computations (important in
  algorithms and simulations).
\item
  It underpins key decompositions like QR factorization, singular value
  decomposition (SVD), and spectral theorems.
\item
  It provides the cleanest way to think about space: orthogonal,
  independent directions scaled to unit length.
\end{itemize}

Whenever possible, mathematicians and engineers prefer orthonormal bases
over arbitrary ones.

\subsubsection{Try It Yourself}\label{try-it-yourself-9}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Verify that (3/5, 4/5) and (--4/5, 3/5) form an orthonormal set in
  \(\mathbb{R}^2\).
\item
  Construct three orthonormal vectors in \(\mathbb{R}^3\) that are not
  the standard basis. Hint: start with (1/√2, 1/√2, 0) and build
  perpendiculars.
\item
  For u = (2, 1), compute its coordinates relative to the orthonormal
  set \{(1/√2, 1/√2), (--1/√2, 1/√2)\}.
\item
  Challenge: Prove that if \{v₁, \ldots, vₖ\} is orthonormal, then the
  matrix with these as columns is orthogonal, i.e., QᵀQ = I.
\end{enumerate}

Through these exercises, you will see how orthonormal sets make every
aspect of linear algebra-from projections to decompositions-simpler,
cleaner, and more powerful.

\subsubsection{Closing}\label{closing}

\begin{verbatim}
Lengths, angles revealed,
projections trace hidden lines,
clarity takes shape.
\end{verbatim}

\section{Chapter 2. Matrices and basic
operations}\label{chapter-2.-matrices-and-basic-operations-1}

\subsubsection{Opening}\label{opening-1}

\begin{verbatim}
Rows and columns meet,
woven grids of silent rules,
machines of order.
\end{verbatim}

\subsection{11. Matrices as Tables and as
Machines}\label{matrices-as-tables-and-as-machines}

The next stage in our journey is to move from vectors to matrices. A
matrix may look like just a rectangular array of numbers, but in linear
algebra it plays two distinct and equally important roles:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  As a table of numbers, storing data, coefficients, or geometric
  patterns in a compact form.
\item
  As a machine that transforms vectors into other vectors, capturing the
  essence of linear transformations.
\end{enumerate}

Both views are valid, and learning to switch between them is crucial to
building intuition.

\subsubsection{Matrices as Tables}\label{matrices-as-tables}

At the most basic level, a matrix is a grid of numbers arranged into
rows and columns.

\begin{itemize}
\item
  A \(2 \times 2\) matrix has 2 rows and 2 columns:

  \[
  A = \begin{bmatrix} 
  a_{11} & a_{12} \\ 
  a_{21} & a_{22} 
  \end{bmatrix}
  \]
\item
  A \(3 \times 2\) matrix has 3 rows and 2 columns:

  \[
  B = \begin{bmatrix} 
  b_{11} & b_{12} \\ 
  b_{21} & b_{22} \\ 
  b_{31} & b_{32} 
  \end{bmatrix}
  \]
\end{itemize}

Each entry \(a_{ij}\) or \(b_{ij}\) tells us the number in the i-th row
and j-th column. The rows of a matrix can represent constraints,
equations, or observations; the columns can represent features,
variables, or directions.

In this sense, matrices are data containers, organizing information
efficiently. That's why matrices show up in spreadsheets, statistics,
computer graphics, and scientific computing.

\subsubsection{Matrices as Machines}\label{matrices-as-machines}

The deeper view of a matrix is as a function from vectors to vectors. If
x is a column vector, then multiplying A·x produces a new vector.

For example:

\[
A = \begin{bmatrix} 
2 & 0 \\ 
1 & 3 
\end{bmatrix}, \quad
\mathbf{x} = \begin{bmatrix} 
4 \\ 
5 
\end{bmatrix}.
\]

Multiplying:

\[
A\mathbf{x} = \begin{bmatrix} 
2×4 + 0×5 \\ 
1×4 + 3×5 
\end{bmatrix} 
= \begin{bmatrix} 
8 \\ 
19 
\end{bmatrix}.
\]

Here, the matrix is acting as a machine that takes input (4, 5) and
outputs (8, 19). The ``machine rules'' are encoded in the rows of A.

\subsubsection{Column View of Matrix
Multiplication}\label{column-view-of-matrix-multiplication}

Another way to see it: multiplying A·x is the same as taking a linear
combination of A's columns.

If

\[
A = \begin{bmatrix} 
a_1 & a_2 
\end{bmatrix}, \quad \mathbf{x} = \begin{bmatrix} 
x_1 \\ 
x_2 
\end{bmatrix},
\]

then:

\[
A\mathbf{x} = x_1 a_1 + x_2 a_2.
\]

So the vector x tells the machine ``how much'' of each column to mix
together. This column view is critical-it connects matrices to span,
dimension, and basis ideas we saw earlier.

\subsubsection{The Duality of Tables and
Machines}\label{the-duality-of-tables-and-machines}

\begin{itemize}
\tightlist
\item
  As a table, a matrix is a static object: numbers written in rows and
  columns.
\item
  As a machine, the same numbers become instructions for transforming
  vectors.
\end{itemize}

This duality is not just conceptual-it's the key to understanding why
linear algebra is so powerful. A dataset, once stored as a table, can be
interpreted as a transformation. Likewise, a transformation, once
understood, can be encoded as a table.

\subsubsection{Examples in Practice}\label{examples-in-practice}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Physics: A stress--strain matrix is a table of coefficients. But it
  also acts as a machine that transforms applied forces into
  deformations.
\item
  Computer Graphics: A 2D rotation matrix is a machine that spins
  vectors, but it can be stored in a simple 2×2 table.
\item
  Economics: Input--output models use matrices as tables of production
  coefficients. Applying them to demand vectors transforms them into
  resource requirements.
\end{enumerate}

\subsubsection{Geometric Intuition}\label{geometric-intuition}

Every 2×2 or 3×3 matrix corresponds to some linear transformation in the
plane or space. Examples:

\begin{itemize}
\tightlist
\item
  Scaling: \(\begin{bmatrix} 2 & 0 \\ 0 & 2 \end{bmatrix}\) doubles
  lengths.
\item
  Reflection: \(\begin{bmatrix} 1 & 0 \\ 0 & -1 \end{bmatrix}\) flips
  across the x-axis.
\item
  Rotation:
  \(\begin{bmatrix} \cos θ & -\sin θ \\ \sin θ & \cos θ \end{bmatrix}\)
  rotates vectors by θ.
\end{itemize}

These are not just tables of numbers-they are precise, reusable
machines.

\subsubsection{Why This Matters}\label{why-this-matters}

This section sets the stage for all matrix theory:

\begin{itemize}
\tightlist
\item
  Thinking of matrices as tables helps in data interpretation and
  organization.
\item
  Thinking of matrices as machines helps in understanding linear
  transformations, eigenvalues, and decompositions.
\item
  Most importantly, learning to switch between the two perspectives
  makes linear algebra both concrete and abstract-bridging computation
  with geometry.
\end{itemize}

\subsubsection{Try It Yourself}\label{try-it-yourself-10}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Write a 2×3 matrix and identify its rows and columns. What might they
  represent in a real-world dataset?
\item
  Multiply \(\begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix}\) by
  \(\begin{bmatrix} 2 \\ –1 \end{bmatrix}\). Interpret the result using
  both the row and column views.
\item
  Construct a matrix that scales vectors by 2 along the x-axis and
  reflects them across the y-axis. Test it on (1, 1).
\item
  Challenge: Show how the same 3×3 rotation matrix can be viewed as a
  data table of cosines/sines and as a machine that turns input vectors.
\end{enumerate}

By mastering both perspectives, you'll see matrices not just as numbers
but as dynamic objects that encode and execute transformations.

\subsection{12. Matrix Shapes, Indexing, and Block
Views}\label{matrix-shapes-indexing-and-block-views}

Matrices come in many shapes and sizes, and the way we label their
entries matters. This section is about learning how to read and write
matrices carefully, how to work with rows and columns, and how to use
block structure to simplify problems. These seemingly simple ideas are
what allow us to manipulate large systems with precision and efficiency.

\subsubsection{Shapes of Matrices}\label{shapes-of-matrices}

The shape of a matrix is given by its number of rows and columns:

\begin{itemize}
\tightlist
\item
  A m×n matrix has m rows and n columns.
\item
  Rows run horizontally, columns run vertically.
\item
  Square matrices have m = n; rectangular matrices have m ≠ n.
\end{itemize}

Examples:

\begin{itemize}
\item
  A 2×3 matrix:

  \[
  \begin{bmatrix} 
  1 & 2 & 3 \\ 
  4 & 5 & 6 
  \end{bmatrix}
  \]
\item
  A 3×2 matrix:

  \[
  \begin{bmatrix} 
  7 & 8 \\ 
  9 & 10 \\ 
  11 & 12 
  \end{bmatrix}
  \]
\end{itemize}

Shape matters because it determines whether certain operations (like
multiplication) are possible.

\subsubsection{Indexing: The Language of
Entries}\label{indexing-the-language-of-entries}

Each entry in a matrix has two indices: one for its row, one for its
column.

\begin{itemize}
\tightlist
\item
  \(a_{ij}\) = entry in row i, column j.
\item
  The first index always refers to the row, the second to the column.
\end{itemize}

For example, in

\[
A = \begin{bmatrix} 
1 & 4 & 7 \\ 
2 & 5 & 8 \\ 
3 & 6 & 9 
\end{bmatrix},
\]

we have:

\begin{itemize}
\tightlist
\item
  \(a_{11} = 1\), \(a_{23} = 8\), \(a_{32} = 6\).
\end{itemize}

Indexing is the grammar of matrix language. Without it, we can't specify
positions or write formulas clearly.

\subsubsection{Rows and Columns as
Vectors}\label{rows-and-columns-as-vectors}

Every row and every column of a matrix is itself a vector.

\begin{itemize}
\tightlist
\item
  The i-th row is written as \(A_{i,*}\).
\item
  The j-th column is written as \(A_{*,j}\).
\end{itemize}

Example: From the matrix above,

\begin{itemize}
\tightlist
\item
  First row: (1, 4, 7).
\item
  Second column: (4, 5, 6).
\end{itemize}

This duality is powerful: rows often represent constraints or equations,
while columns represent directions or features. Later, when we interpret
matrix--vector products, we'll see that multiplying A·x means combining
columns, while multiplying yᵀ·A means combining rows.

\subsubsection{Submatrices}\label{submatrices}

Sometimes we want just part of a matrix. A submatrix is formed by
selecting certain rows and columns.

Example: From

\[
B = \begin{bmatrix} 
2 & 4 & 6 \\ 
1 & 3 & 5 \\ 
7 & 8 & 9 
\end{bmatrix},
\]

the submatrix of the first two rows and last two columns is:

\[
\begin{bmatrix} 
4 & 6 \\ 
3 & 5 
\end{bmatrix}.
\]

Submatrices allow us to zoom in and isolate parts of a problem.

\subsubsection{Block Matrices: Dividing to
Conquer}\label{block-matrices-dividing-to-conquer}

Large matrices can often be broken into blocks, which are smaller
submatrices arranged inside. This is like dividing a spreadsheet into
quadrants.

For example:

\[
C = \begin{bmatrix} 
A_{11} & A_{12} \\ 
A_{21} & A_{22} 
\end{bmatrix},
\]

where each \(A_{ij}\) is itself a smaller matrix.

This structure is useful in:

\begin{itemize}
\tightlist
\item
  Computation: Algorithms often process blocks instead of individual
  entries.
\item
  Theory: Many proofs and factorizations rely on viewing a matrix in
  blocks (e.g., LU, QR, Schur decomposition).
\item
  Applications: Partitioning data tables into logical sections.
\end{itemize}

Example: Splitting a 4×4 matrix into four 2×2 blocks helps us treat it
as a ``matrix of matrices.''

\subsubsection{Special Shapes}\label{special-shapes}

Some shapes of matrices are so common they deserve names:

\begin{itemize}
\tightlist
\item
  Row vector: 1×n matrix.
\item
  Column vector: n×1 matrix.
\item
  Diagonal matrix: Nonzero entries only on the diagonal.
\item
  Identity matrix: Square diagonal matrix with 1's on the diagonal.
\item
  Zero matrix: All entries are 0.
\end{itemize}

Recognizing these shapes saves time and clarifies reasoning.

\subsubsection{Everyday Analogies}\label{everyday-analogies-8}

\begin{itemize}
\tightlist
\item
  Spreadsheets: A matrix is like a grid of cells, each with a row and
  column label. Indexing lets you specify exactly which cell you mean.
\item
  Maps: Cities on a map can be located by coordinates (row, column). The
  same logic applies to entries in a matrix.
\item
  Lego blocks: Just as large Lego structures are built from smaller
  blocks, large matrices are often analyzed by splitting into
  submatrices.
\end{itemize}

\subsubsection{Why It Matters}\label{why-it-matters-8}

Careful attention to matrix shapes, indexing, and block views ensures:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Precision: We can describe positions unambiguously.
\item
  Structure awareness: Recognizing patterns (diagonal, triangular,
  block) leads to more efficient computations.
\item
  Scalability: Block partitioning is the foundation of modern numerical
  linear algebra libraries, where matrices are too large to handle entry
  by entry.
\item
  Geometry: Rows and columns as vectors connect matrix structure to
  span, basis, and dimension.
\end{enumerate}

These basic tools prepare us for multiplication, transformations, and
factorization.

\subsubsection{Try It Yourself}\label{try-it-yourself-11}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Write a 3×4 matrix and label the entry in row 2, column 3.
\item
  Extract a 2×2 submatrix from the corners of a 4×4 matrix of your
  choice.
\item
  Break a 6×6 matrix into four 3×3 blocks. How would you represent it
  compactly?
\item
  Challenge: Given

  \[
  D = \begin{bmatrix} 
  1 & 2 & 3 & 4 \\ 
  5 & 6 & 7 & 8 \\ 
  9 & 10 & 11 & 12 
  \end{bmatrix},
  \]

  write it as a block matrix with a 2×2 block in the top-left, a 2×2
  block in the top-right, and a 1×4 block in the bottom row.
\end{enumerate}

By practicing with shapes, indexing, and blocks, you'll develop the
ability to navigate matrices not just as raw grids of numbers but as
structured objects ready for deeper algebraic and geometric insights.

\subsection{13. Matrix Addition and Scalar
Multiplication}\label{matrix-addition-and-scalar-multiplication}

Before exploring matrix--vector and matrix--matrix multiplication, it is
essential to understand the simplest operations we can perform with
matrices: addition and scalar multiplication. These operations extend
the rules we learned for vectors, but now applied to entire grids of
numbers. Although straightforward, they are the foundation for more
complex algebraic manipulations and help establish the idea of matrices
as elements of a vector space.

\subsubsection{Matrix Addition: Entry by
Entry}\label{matrix-addition-entry-by-entry}

If two matrices \(A\) and \(B\) have the same shape (same number of rows
and columns), we can add them by adding corresponding entries.

Formally: If

\[
A = [a_{ij}], \quad B = [b_{ij}],
\]

then

\[
A + B = [a_{ij} + b_{ij}].
\]

Example:

\[
\begin{bmatrix} 
1 & 2 & 3 \\ 
4 & 5 & 6 
\end{bmatrix}
+
\begin{bmatrix} 
7 & 8 & 9 \\ 
10 & 11 & 12 
\end{bmatrix}
=
\begin{bmatrix} 
8 & 10 & 12 \\ 
14 & 16 & 18 
\end{bmatrix}.
\]

Key point: Addition is only defined if the matrices are the same shape.
A 2×3 matrix cannot be added to a 3×2 matrix.

\subsubsection{Scalar Multiplication: Scaling Every
Entry}\label{scalar-multiplication-scaling-every-entry}

A scalar multiplies every entry of a matrix.

Formally: For scalar \(c\) and matrix \(A = [a_{ij}]\),

\[
cA = [c \cdot a_{ij}].
\]

Example:

\[
3 \cdot 
\begin{bmatrix} 
2 & -1 \\ 
0 & 4 
\end{bmatrix}
=
\begin{bmatrix} 
6 & -3 \\ 
0 & 12 
\end{bmatrix}.
\]

This mirrors vector scaling: stretching or shrinking the whole matrix by
a constant factor.

\subsubsection{Properties of Addition and Scalar
Multiplication}\label{properties-of-addition-and-scalar-multiplication}

These two operations satisfy familiar algebraic properties that make the
set of all m×n matrices into a vector space:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Commutativity: \(A + B = B + A\).
\item
  Associativity: \((A + B) + C = A + (B + C)\).
\item
  Additive identity: \(A + 0 = A\), where 0 is the zero matrix.
\item
  Additive inverse: For every \(A\), there exists \(-A\) such that
  \(A + (-A) = 0\).
\item
  Distributivity: \(c(A + B) = cA + cB\).
\item
  Compatibility: \((c + d)A = cA + dA\).
\item
  Scalar associativity: \((cd)A = c(dA)\).
\item
  Unit scalar: \(1A = A\).
\end{enumerate}

These guarantee that working with matrices feels like working with
numbers and vectors, only in a higher-level setting.

\subsubsection{Matrix Arithmetic as Table
Operations}\label{matrix-arithmetic-as-table-operations}

From the table view, addition and scalar multiplication are just simple
bookkeeping: line up two tables of the same shape and add entry by
entry; multiply the whole table by a constant.

Example: Imagine two spreadsheets of monthly expenses. Adding them gives
combined totals. Multiplying by 12 converts a monthly table into a
yearly estimate.

\subsubsection{Matrix Arithmetic as Machine
Operations}\label{matrix-arithmetic-as-machine-operations}

From the machine view, these operations adjust the behavior of linear
transformations:

\begin{itemize}
\tightlist
\item
  Adding matrices corresponds to adding their effects when applied to
  vectors.
\item
  Scaling a matrix scales the effect of the transformation.
\end{itemize}

Example: Let \(A\) rotate vectors slightly, and \(B\) stretch vectors.
The matrix \(A + B\) represents a transformation that applies both
influences together. Scaling by 2 doubles the effect of the
transformation.

\subsubsection{Special Case: Zero and
Identity}\label{special-case-zero-and-identity}

\begin{itemize}
\tightlist
\item
  Zero matrix: All entries are 0. Adding it to any matrix changes
  nothing.
\item
  Scalar multiples of the identity: \(cI\) scales every vector by c when
  applied. For example, \(2I\) doubles every vector's length.
\end{itemize}

These act as neutral or scaling elements in matrix arithmetic.

\subsubsection{Geometric Intuition}\label{geometric-intuition-1}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  In \(\mathbb{R}^2\) or \(\mathbb{R}^3\), adding transformation
  matrices is like superimposing geometric effects: e.g., one matrix
  shears, another rotates, their sum mixes both.
\item
  Scaling a transformation makes its action stronger or weaker. Doubling
  a shear makes it twice as pronounced.
\end{enumerate}

This shows that even before multiplication, addition and scaling already
have geometric meaning.

\subsubsection{Everyday Analogies}\label{everyday-analogies-9}

\begin{itemize}
\tightlist
\item
  Recipes: Adding two recipes (matrices) ingredient by ingredient gives
  a combined shopping list. Multiplying a recipe by 3 scales it for 3
  times as many people.
\item
  Financial planning: Adding two budget tables gives a combined budget.
  Multiplying by 12 scales monthly costs to yearly totals.
\item
  Mixing effects: Think of audio signals represented by matrices. Adding
  them overlays sounds; scaling adjusts volume.
\end{itemize}

\subsubsection{Why It Matters}\label{why-it-matters-9}

Though simple, these operations:

\begin{itemize}
\tightlist
\item
  Define matrices as elements of vector spaces.
\item
  Lay the groundwork for linear combinations of matrices, critical in
  eigenvalue problems, optimization, and control theory.
\item
  Enable modular problem-solving: break big transformations into smaller
  ones and recombine them.
\item
  Appear everywhere in practice, from combining datasets to scaling
  transformations.
\end{itemize}

Without addition and scalar multiplication, we could not treat matrices
systematically as algebraic objects.

\subsubsection{Try It Yourself}\label{try-it-yourself-12}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Add
\end{enumerate}

\[
\begin{bmatrix} 
2 & 0 \\ 
1 & 3 
\end{bmatrix}
\quad \text{and} \quad
\begin{bmatrix} 
-2 & 5 \\ 
4 & -3 
\end{bmatrix}.
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Multiply
\end{enumerate}

\[
\begin{bmatrix} 
1 & -1 & 2 \\ 
0 & 3 & 4 
\end{bmatrix}
\]

by --2.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\item
  Show that (A + B) + C = A + (B + C) with explicit 2×2 matrices.
\item
  Challenge: Construct two 3×3 matrices A and B such that A + B = 0.
  What does that tell you about B?
\end{enumerate}

By practicing these fundamentals, you will see that even the most basic
operations on matrices already build the algebraic backbone for deeper
results like matrix multiplication, transformations, and factorization.

\subsection{14. Matrix--Vector Product (Linear Combinations of
Columns)}\label{matrixvector-product-linear-combinations-of-columns}

We now arrive at one of the most important operations in all of linear
algebra: the matrix--vector product. This operation takes a matrix \(A\)
and a vector x, and produces a new vector. While the computation is
straightforward, its interpretations are deep: it can be seen as
combining rows, as combining columns, or as applying a linear
transformation. This is the operation that connects matrices to the
geometry of vector spaces.

\subsubsection{The Algebraic Rule}\label{the-algebraic-rule}

Suppose \(A\) is an \(m \times n\) matrix, and x is a vector in
\(\mathbb{R}^n\). The product \(A\mathbf{x}\) is a vector in
\(\mathbb{R}^m\), defined as:

\[
A = 
\begin{bmatrix} 
a_{11} & a_{12} & \cdots & a_{1n} \\ 
a_{21} & a_{22} & \cdots & a_{2n} \\ 
\vdots & \vdots & \ddots & \vdots \\ 
a_{m1} & a_{m2} & \cdots & a_{mn} 
\end{bmatrix}, 
\quad
\mathbf{x} = 
\begin{bmatrix} 
x_1 \\ x_2 \\ \vdots \\ x_n
\end{bmatrix}.
\]

Then:

\[
A\mathbf{x} =
\begin{bmatrix} 
a_{11}x_1 + a_{12}x_2 + \cdots + a_{1n}x_n \\ 
a_{21}x_1 + a_{22}x_2 + \cdots + a_{2n}x_n \\ 
\vdots \\ 
a_{m1}x_1 + a_{m2}x_2 + \cdots + a_{mn}x_n
\end{bmatrix}.
\]

Each entry of the output is a dot product between one row of \(A\) and
the vector x.

\subsubsection{Row View: Dot Products}\label{row-view-dot-products}

From the row perspective, \(A\mathbf{x}\) is computed row by row:

\begin{itemize}
\tightlist
\item
  Take each row of \(A\).
\item
  Dot it with x.
\item
  That result becomes one entry of the output.
\end{itemize}

Example:

\[
A =
\begin{bmatrix} 
2 & 1 \\ 
3 & 4 \\ 
-1 & 2 
\end{bmatrix}, \quad
\mathbf{x} =
\begin{bmatrix} 
5 \\ 
-1
\end{bmatrix}.
\]

\begin{itemize}
\tightlist
\item
  First row dot x: \(2(5) + 1(-1) = 9\).
\item
  Second row dot x: \(3(5) + 4(-1) = 11\).
\item
  Third row dot x: \((-1)(5) + 2(-1) = -7\).
\end{itemize}

So:

\[
A\mathbf{x} = 
\begin{bmatrix} 
9 \\ 11 \\ -7
\end{bmatrix}.
\]

\subsubsection{Column View: Linear
Combinations}\label{column-view-linear-combinations}

From the column perspective, \(A\mathbf{x}\) is a linear combination of
the columns of A.

If

\[
A = 
\begin{bmatrix} 
| & | &  & | \\ 
a_1 & a_2 & \cdots & a_n \\ 
| & | &  & |
\end{bmatrix}, 
\quad
\mathbf{x} =
\begin{bmatrix} 
x_1 \\ x_2 \\ \vdots \\ x_n
\end{bmatrix},
\]

then:

\[
A\mathbf{x} = x_1 a_1 + x_2 a_2 + \cdots + x_n a_n.
\]

That is: multiply each column of \(A\) by the corresponding entry in x,
then add them up.

This interpretation connects directly to the idea of span: the set of
all vectors \(A\mathbf{x}\) as x varies is exactly the span of the
columns of \(A\).

\subsubsection{The Machine View: Linear
Transformations}\label{the-machine-view-linear-transformations}

The machine view ties everything together: multiplying a vector by a
matrix means applying the linear transformation represented by the
matrix.

\begin{itemize}
\tightlist
\item
  If \(A\) is a 2×2 rotation matrix, then \(A\mathbf{x}\) rotates the
  vector x.
\item
  If \(A\) is a scaling matrix, then \(A\mathbf{x}\) stretches or
  shrinks x.
\item
  If \(A\) is a projection matrix, then \(A\mathbf{x}\) projects x onto
  a line or plane.
\end{itemize}

Thus, the algebraic definition encodes geometric and functional meaning.

\subsubsection{Examples of Geometric
Action}\label{examples-of-geometric-action}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Scaling:
\end{enumerate}

\[
A = \begin{bmatrix} 2 & 0 \\ 0 & 2 \end{bmatrix}.
\]

Then \(A\mathbf{x}\) doubles the length of any vector x.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Reflection:
\end{enumerate}

\[
A = \begin{bmatrix} 1 & 0 \\ 0 & -1 \end{bmatrix}.
\]

This flips vectors across the x-axis.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Rotation by θ:
\end{enumerate}

\[
A = \begin{bmatrix} \cosθ & -\sinθ \\ \sinθ & \cosθ \end{bmatrix}.
\]

This rotates vectors counterclockwise by θ in the plane.

\subsubsection{Everyday Analogies}\label{everyday-analogies-10}

\begin{itemize}
\tightlist
\item
  Mixing ingredients: The vector x is a recipe, and the columns of \(A\)
  are the ingredients. The product \(A\mathbf{x}\) is the final mixture.
\item
  Weighted averages: A student's final grade is a matrix--vector
  product: weights (the vector) multiplied by scores (the matrix
  columns).
\item
  Signal processing: Combining input signals with weights produces a new
  output, just like multiplying a matrix of signals by a weight vector.
\end{itemize}

\subsubsection{Why It Matters}\label{why-it-matters-10}

The matrix--vector product is the building block of everything in linear
algebra:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  It defines the action of a matrix as a linear map.
\item
  It connects directly to span and dimension (columns generate all
  possible outputs).
\item
  It underpins solving linear systems, eigenvalue problems, and
  decompositions.
\item
  It is the engine of computation in applied mathematics, from computer
  graphics to machine learning (e.g., neural networks compute billions
  of matrix--vector products).
\end{enumerate}

\subsubsection{Try It Yourself}\label{try-it-yourself-13}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute
\end{enumerate}

\[
\begin{bmatrix} 
1 & 2 & 3 \\ 
4 & 5 & 6 
\end{bmatrix}
\begin{bmatrix} 
2 \\ 
0 \\ 
1 
\end{bmatrix}.
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\item
  Express the result of the above product as a linear combination of the
  columns of the matrix.
\item
  Construct a 2×2 matrix that reflects vectors across the line
  \(y = x\). Test it on (1, 0) and (0, 1).
\item
  Challenge: For a 3×3 matrix, show that the set of all possible
  \(A\mathbf{x}\) (as x varies) is exactly the column space of \(A\).
\end{enumerate}

By mastering both the computational rules and the interpretations of the
matrix--vector product, you will gain the most important insight in
linear algebra: matrices are not just tables-they are engines that
transform space.

\subsection{15. Matrix--Matrix Product (Composition of Linear
Steps)}\label{matrixmatrix-product-composition-of-linear-steps}

Having understood how a matrix acts on a vector, the next natural step
is to understand how one matrix can act on another. This leads us to the
matrix--matrix product, a rule for combining two matrices into a single
new matrix. Though the arithmetic looks complicated at first, the
underlying idea is elegant: multiplying two matrices represents
composing two linear transformations.

\subsubsection{The Algebraic Rule}\label{the-algebraic-rule-1}

Suppose \(A\) is an \(m \times n\) matrix and \(B\) is an \(n \times p\)
matrix. Their product \(C = AB\) is an \(m \times p\) matrix defined by:

\[
c_{ij} = \sum_{k=1}^n a_{ik} b_{kj}.
\]

That is: each entry of \(C\) is the dot product of the i-th row of \(A\)
with the j-th column of \(B\).

\subsubsection{Example: A 2×3 times a
3×2}\label{example-a-23-times-a-32}

\[
A = 
\begin{bmatrix} 
1 & 2 & 3 \\ 
4 & 5 & 6 
\end{bmatrix}, \quad 
B = 
\begin{bmatrix} 
7 & 8 \\ 
9 & 10 \\ 
11 & 12 
\end{bmatrix}.
\]

Product: \(C = AB\) will be 2×2.

\begin{itemize}
\tightlist
\item
  \(c_{11} = 1\cdot 7 + 2\cdot 9 + 3\cdot 11 = 58\).
\item
  \(c_{12} = 1\cdot 8 + 2\cdot 10 + 3\cdot 12 = 64\).
\item
  \(c_{21} = 4\cdot 7 + 5\cdot 9 + 6\cdot 11 = 139\).
\item
  \(c_{22} = 4\cdot 8 + 5\cdot 10 + 6\cdot 12 = 154\).
\end{itemize}

So:

\[
C = 
\begin{bmatrix} 
58 & 64 \\ 
139 & 154 
\end{bmatrix}.
\]

\subsubsection{Column View: Linear Combinations of
Columns}\label{column-view-linear-combinations-of-columns}

From the column perspective, \(AB\) is computed by applying \(A\) to
each column of \(B\).

If \(B = [b_1 \; b_2 \; \cdots \; b_p]\), then:

\[
AB = [A b_1 \; A b_2 \; \cdots \; A b_p].
\]

That is: multiply \(A\) by each column of \(B\). This is often the
simplest way to think of the product.

\subsubsection{Row View: Linear Combinations of
Rows}\label{row-view-linear-combinations-of-rows}

From the row perspective, each row of \(AB\) is formed by combining rows
of \(B\) using coefficients from a row of \(A\). This dual view is less
common but equally useful, especially in proofs and algorithms.

\subsubsection{The Machine View: Composition of
Transformations}\label{the-machine-view-composition-of-transformations}

The most important interpretation is the machine view: multiplying
matrices corresponds to composing transformations.

\begin{itemize}
\tightlist
\item
  If \(A\) maps \(\mathbb{R}^n \to \mathbb{R}^m\) and \(B\) maps
  \(\mathbb{R}^p \to \mathbb{R}^n\), then \(AB\) maps
  \(\mathbb{R}^p \to \mathbb{R}^m\).
\item
  In words: do \(B\) first, then \(A\).
\end{itemize}

Example:

\begin{itemize}
\tightlist
\item
  Let \(B\) rotate vectors by 90°.
\item
  Let \(A\) scale vectors by 2.
\item
  Then \(AB\) rotates and then scales-both steps combined into a single
  transformation.
\end{itemize}

\subsubsection{Geometric Examples}\label{geometric-examples}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Scaling then rotation:
\end{enumerate}

\[
A = \begin{bmatrix} 2 & 0 \\ 0 & 2 \end{bmatrix}, \quad 
B = \begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix}.
\]

Then \(AB\) scales vectors by 2 after rotating them 90°.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Projection then reflection: If \(B\) projects onto the x-axis and
  \(A\) reflects across the y-axis, then \(AB\) represents ``project
  then reflect.''
\end{enumerate}

\subsubsection{Properties of Matrix
Multiplication}\label{properties-of-matrix-multiplication}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Associative: \((AB)C = A(BC)\).
\item
  Distributive: \(A(B + C) = AB + AC\).
\item
  Not commutative: In general, \(AB \neq BA\). Order matters!
\item
  Identity: \(AI = IA = A\).
\end{enumerate}

These properties highlight that while multiplication is structured, it
is not symmetric. The order encodes the order of operations in
transformations.

\subsubsection{Everyday Analogies}\label{everyday-analogies-11}

\begin{itemize}
\tightlist
\item
  Cooking steps: If \(B\) is ``chop vegetables'' and \(A\) is ``cook
  vegetables,'' then \(AB\) is ``cook after chopping.'' Doing it the
  other way (BA) would make no sense!
\item
  Assembly line: Each machine (matrix) performs an operation on the
  input. Chaining them corresponds to multiplying the matrices.
\item
  Maps and routes: Going from home to the station (B), then from station
  to office (A) equals the combined route home→office (AB).
\end{itemize}

\subsubsection{Why It Matters}\label{why-it-matters-11}

Matrix multiplication is the core of linear algebra because:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  It encodes function composition in algebraic form.
\item
  It provides a way to capture multiple transformations in a single
  matrix.
\item
  It underpins algorithms in computer graphics, robotics, statistics,
  and machine learning.
\item
  It reveals deeper structure, like commutativity failing, which
  reflects real-world order of operations.
\end{enumerate}

Almost every application of linear algebra-solving equations, computing
eigenvalues, training neural networks-relies on efficient matrix
multiplication.

\subsubsection{Try It Yourself}\label{try-it-yourself-14}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute
\end{enumerate}

\[
\begin{bmatrix} 
1 & 0 \\ 
2 & 3 
\end{bmatrix}
\begin{bmatrix} 
4 & 5 \\ 
6 & 7 
\end{bmatrix}.
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Show that \(AB \neq BA\) for the matrices
\end{enumerate}

\[
A = \begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix}, 
\quad 
B = \begin{bmatrix} 0 & 0 \\ 1 & 0 \end{bmatrix}.
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\item
  Construct two 2×2 matrices where \(AB = BA\). Why does commutativity
  happen here?
\item
  Challenge: If \(A\) is a projection and \(B\) is a rotation, compute
  \(AB\) and \(BA\). Do they represent the same geometric operation?
\end{enumerate}

Through these perspectives, the matrix--matrix product shifts from being
a mechanical formula to being a language for combining linear steps-each
product telling the story of ``do this, then that.''

\subsection{16. Identity, Inverse, and
Transpose}\label{identity-inverse-and-transpose}

With addition, scalar multiplication, and matrix multiplication in
place, we now introduce three special operations and objects that form
the backbone of matrix algebra: the identity matrix, the inverse of a
matrix, and the transpose of a matrix. Each captures a fundamental
principle-neutrality, reversibility, and symmetry-and together they
provide the algebraic structure that makes linear algebra so powerful.

\subsubsection{The Identity Matrix}\label{the-identity-matrix}

The identity matrix is the matrix equivalent of the number 1 in
multiplication.

\begin{itemize}
\tightlist
\item
  Definition: The identity matrix \(I_n\) is the \(n \times n\) matrix
  with 1's on the diagonal and 0's everywhere else.
\end{itemize}

Example (3×3):

\[
I_3 = \begin{bmatrix} 
1 & 0 & 0 \\ 
0 & 1 & 0 \\ 
0 & 0 & 1 
\end{bmatrix}.
\]

\begin{itemize}
\item
  Property: For any \(n \times n\) matrix \(A\),

  \[
  AI_n = I_nA = A.
  \]
\item
  Machine view: \(I\) does nothing-it maps every vector to itself.
\end{itemize}

\subsubsection{The Inverse of a Matrix}\label{the-inverse-of-a-matrix}

The inverse is the matrix equivalent of the reciprocal of a number.

\begin{itemize}
\item
  Definition: For a square matrix \(A\), its inverse \(A^{-1}\) is the
  matrix such that

  \[
  AA^{-1} = A^{-1}A = I.
  \]
\item
  Not all matrices have inverses. A matrix is invertible if and only if
  it is square and its determinant is nonzero.
\end{itemize}

Example:

\[
A = \begin{bmatrix} 
2 & 1 \\ 
1 & 1 
\end{bmatrix}, 
\quad 
A^{-1} = \begin{bmatrix} 
1 & -1 \\ 
-1 & 2 
\end{bmatrix}.
\]

Check:

\[
AA^{-1} = \begin{bmatrix} 
2 & 1 \\ 
1 & 1 
\end{bmatrix}
\begin{bmatrix} 
1 & -1 \\ 
-1 & 2 
\end{bmatrix}
=
\begin{bmatrix} 
1 & 0 \\ 
0 & 1 
\end{bmatrix} = I.
\]

\begin{itemize}
\tightlist
\item
  Machine view: Applying \(A\) transforms a vector. Applying \(A^{-1}\)
  undoes that transformation, restoring the original input.
\end{itemize}

\subsubsection{Non-Invertible Matrices}\label{non-invertible-matrices}

Some matrices cannot be inverted. These are called singular.

\begin{itemize}
\item
  Example:

  \[
  B = \begin{bmatrix} 
  2 & 4 \\ 
  1 & 2 
  \end{bmatrix}.
  \]

  Here, the second column is a multiple of the first. The transformation
  squashes vectors into a line, losing information-so it cannot be
  reversed.
\end{itemize}

This ties invertibility to geometry: a transformation that collapses
dimensions cannot be undone.

\subsubsection{The Transpose of a
Matrix}\label{the-transpose-of-a-matrix}

The transpose reflects a matrix across its diagonal.

\begin{itemize}
\item
  Definition: For \(A = [a_{ij}]\),

  \[
  A^T = [a_{ji}].
  \]
\item
  In words: rows become columns, columns become rows.
\end{itemize}

Example:

\[
A = \begin{bmatrix} 
1 & 2 & 3 \\ 
4 & 5 & 6 
\end{bmatrix}, 
\quad 
A^T = \begin{bmatrix} 
1 & 4 \\ 
2 & 5 \\ 
3 & 6 
\end{bmatrix}.
\]

\begin{itemize}
\item
  Properties:

  \begin{itemize}
  \tightlist
  \item
    \((A^T)^T = A\).
  \item
    \((A + B)^T = A^T + B^T\).
  \item
    \((cA)^T = cA^T\).
  \item
    \((AB)^T = B^T A^T\) (note the reversed order!).
  \end{itemize}
\end{itemize}

\subsubsection{Symmetric and Orthogonal
Matrices}\label{symmetric-and-orthogonal-matrices}

Two important classes emerge from the transpose:

\begin{itemize}
\item
  Symmetric matrices: \(A = A^T\). Example:

  \[
  \begin{bmatrix} 
  2 & 3 \\ 
  3 & 5 
  \end{bmatrix}.
  \]

  These have beautiful properties: real eigenvalues and orthogonal
  eigenvectors.
\item
  Orthogonal matrices: \(Q^TQ = I\). Their columns form an orthonormal
  set, and they represent pure rotations/reflections.
\end{itemize}

\subsubsection{Everyday Analogies}\label{everyday-analogies-12}

\begin{itemize}
\tightlist
\item
  Identity: A mirror that doesn't distort your reflection-it leaves
  everything unchanged.
\item
  Inverse: A ``reverse recipe''-if one step mixes, the inverse unmixed
  (where possible).
\item
  Transpose: Reorganizing a spreadsheet by flipping rows and columns.
\end{itemize}

\subsubsection{Why It Matters}\label{why-it-matters-12}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The identity guarantees a neutral element for multiplication.
\item
  The inverse provides a way to solve equations
  \(A\mathbf{x} = \mathbf{b}\) via \(\mathbf{x} = A^{-1}\mathbf{b}\).
\item
  The transpose ties matrices to geometry, inner products, and symmetry.
\item
  Together, they form the algebraic foundation for deeper topics:
  determinants, eigenvalues, factorizations, and numerical methods.
\end{enumerate}

Without these tools, matrix algebra would lack structure and
reversibility.

\subsubsection{Try It Yourself}\label{try-it-yourself-15}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute the transpose of
\end{enumerate}

\[
\begin{bmatrix} 
1 & 0 & 2 \\ 
-3 & 4 & 5 
\end{bmatrix}.
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Verify that \((AB)^T = B^TA^T\) for
\end{enumerate}

\[
A = \begin{bmatrix} 1 & 2 \\ 0 & 3 \end{bmatrix}, \quad 
B = \begin{bmatrix} 4 & 0 \\ 5 & 6 \end{bmatrix}.
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Find the inverse of
\end{enumerate}

\[
\begin{bmatrix} 
3 & 2 \\ 
1 & 1 
\end{bmatrix}.
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Challenge: Show that if \(Q\) is orthogonal, then \(Q^{-1} = Q^T\).
  Interpret this geometrically as saying ``rotations can be undone by
  transposing.''
\end{enumerate}

Through these exercises, you'll see how identity, inverse, and transpose
anchor the structure of linear algebra, providing neutrality,
reversibility, and symmetry in every calculation.

\subsection{17. Symmetric, Diagonal, Triangular, and Permutation
Matrices}\label{symmetric-diagonal-triangular-and-permutation-matrices}

Not all matrices are created equal-some have special shapes or patterns
that give them unique properties. These structured matrices are the
workhorses of linear algebra: they simplify computation, reveal
geometry, and form the building blocks for algorithms. In this section,
we study four especially important classes: symmetric, diagonal,
triangular, and permutation matrices.

\subsubsection{Symmetric Matrices}\label{symmetric-matrices}

A matrix is symmetric if it equals its transpose:

\[
A = A^T.
\]

Example:

\[
\begin{bmatrix} 
2 & 3 & 4 \\ 
3 & 5 & 6 \\ 
4 & 6 & 9 
\end{bmatrix}.
\]

\begin{itemize}
\tightlist
\item
  Geometric meaning: Symmetric matrices represent linear transformations
  that have no ``handedness.'' They often arise in physics (energy,
  covariance, stiffness).
\item
  Algebraic fact: Symmetric matrices have real eigenvalues and an
  orthonormal basis of eigenvectors. This property underpins the
  spectral theorem, one of the pillars of linear algebra.
\end{itemize}

\subsubsection{Diagonal Matrices}\label{diagonal-matrices}

A matrix is diagonal if all non-diagonal entries are zero.

\[
D = \begin{bmatrix} 
d_1 & 0 & 0 \\ 
0 & d_2 & 0 \\ 
0 & 0 & d_3 
\end{bmatrix}.
\]

\begin{itemize}
\item
  Multiplying by \(D\) scales each coordinate separately.
\item
  Computations with diagonals are lightning fast:

  \begin{itemize}
  \tightlist
  \item
    Adding: add diagonal entries.
  \item
    Multiplying: multiply diagonal entries.
  \item
    Inverting: invert each diagonal entry (if nonzero).
  \end{itemize}
\end{itemize}

Example:

\[
\begin{bmatrix} 
2 & 0 \\ 
0 & 3 
\end{bmatrix}
\begin{bmatrix} 
x \\ 
y
\end{bmatrix}
=
\begin{bmatrix} 
2x \\ 
3y
\end{bmatrix}.
\]

This is why diagonalization is so valuable: turning a general matrix
into a diagonal one simplifies everything.

\subsubsection{Triangular Matrices}\label{triangular-matrices}

A matrix is upper triangular if all entries below the main diagonal are
zero, and lower triangular if all entries above the diagonal are zero.

\begin{itemize}
\item
  Upper triangular example:

  \[
  \begin{bmatrix} 
  1 & 2 & 3 \\ 
  0 & 4 & 5 \\ 
  0 & 0 & 6 
  \end{bmatrix}.
  \]
\item
  Lower triangular example:

  \[
  \begin{bmatrix} 
  7 & 0 & 0 \\ 
  8 & 9 & 0 \\ 
  10 & 11 & 12 
  \end{bmatrix}.
  \]
\end{itemize}

Why they matter:

\begin{itemize}
\tightlist
\item
  Determinant = product of diagonal entries.
\item
  Easy to solve systems by substitution (forward or backward).
\item
  Every square matrix can be factored into triangular matrices (LU
  decomposition).
\end{itemize}

\subsubsection{Permutation Matrices}\label{permutation-matrices}

A permutation matrix is obtained by permuting the rows (or columns) of
an identity matrix.

Example:

\[
P = \begin{bmatrix} 
0 & 1 & 0 \\ 
1 & 0 & 0 \\ 
0 & 0 & 1 
\end{bmatrix}.
\]

Multiplying by \(P\):

\begin{itemize}
\tightlist
\item
  On the left, permutes the rows of a matrix.
\item
  On the right, permutes the columns of a matrix.
\end{itemize}

Permutation matrices are used in pivoting strategies in elimination,
ensuring numerical stability in solving systems. They are also
orthogonal: \(P^{-1} = P^T\).

\subsubsection{Connections Between Them}\label{connections-between-them}

\begin{itemize}
\tightlist
\item
  A diagonal matrix is a special case of triangular (both upper and
  lower).
\item
  Symmetric matrices often become diagonal under orthogonal
  transformations.
\item
  Permutation matrices help reorder triangular or diagonal matrices
  without breaking their structure.
\end{itemize}

Together, these classes show that structure leads to simplicity-many
computational algorithms exploit these patterns for speed and stability.

\subsubsection{Everyday Analogies}\label{everyday-analogies-13}

\begin{itemize}
\tightlist
\item
  Symmetric: A perfectly balanced seesaw-forces are mirrored on both
  sides.
\item
  Diagonal: Independent volume knobs for each channel of a stereo
  system-no cross-interference.
\item
  Triangular: Step-by-step instructions where each step depends only on
  earlier steps, never later ones.
\item
  Permutation: Shuffling cards-same elements, different order.
\end{itemize}

\subsubsection{Why It Matters}\label{why-it-matters-13}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Symmetric matrices guarantee stable and interpretable
  eigen-decompositions.
\item
  Diagonal matrices make computation effortless.
\item
  Triangular matrices are the backbone of elimination and factorization
  methods.
\item
  Permutation matrices preserve structure while reordering, critical for
  algorithms.
\end{enumerate}

Almost every advanced method in numerical linear algebra relies on
reducing general matrices into one of these structured forms.

\subsubsection{Try It Yourself}\label{try-it-yourself-16}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Verify that
\end{enumerate}

\[
\begin{bmatrix} 
1 & 2 \\ 
2 & 5 
\end{bmatrix}
\]

is symmetric. Find its transpose.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Compute the determinant of
\end{enumerate}

\[
\begin{bmatrix} 
3 & 0 & 0 \\ 
0 & 4 & 0 \\ 
0 & 0 & 5 
\end{bmatrix}.
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Solve
\end{enumerate}

\[
\begin{bmatrix} 
2 & 3 & 1 \\ 
0 & 5 & 2 \\ 
0 & 0 & 4 
\end{bmatrix}
\mathbf{x} =
\begin{bmatrix} 
1 \\ 
2 \\ 
3 
\end{bmatrix}
\]

using back substitution.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Construct a 4×4 permutation matrix that swaps the first and last rows.
  Apply it to a 4×1 vector of your choice.
\end{enumerate}

By exploring these four structured families, you'll start to see that
not all matrices are messy-many have order hidden in their arrangement,
and exploiting that order is the key to both theoretical understanding
and efficient computation.

\subsection{18. Trace and Basic Matrix
Properties}\label{trace-and-basic-matrix-properties}

So far we have studied shapes, multiplication rules, and special classes
of matrices. In this section we introduce a simple but surprisingly
powerful quantity: the trace of a matrix. Along with it, we review a set
of basic matrix properties that provide shortcuts, invariants, and
insights into how matrices behave.

\subsubsection{Definition of the Trace}\label{definition-of-the-trace}

For a square matrix \(A = [a_{ij}]\) of size \(n \times n\), the trace
is the sum of the diagonal entries:

\[
\text{tr}(A) = a_{11} + a_{22} + \cdots + a_{nn}.
\]

Example:

\[
A = \begin{bmatrix} 
2 & 5 & 7 \\ 
0 & 3 & 1 \\ 
4 & 6 & 8 
\end{bmatrix}, 
\quad 
\text{tr}(A) = 2 + 3 + 8 = 13.
\]

The trace extracts a single number summarizing the ``diagonal content''
of a matrix.

\subsubsection{Properties of the Trace}\label{properties-of-the-trace}

The trace is linear and interacts nicely with multiplication and
transposition:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Linearity:

  \begin{itemize}
  \tightlist
  \item
    \(\text{tr}(A + B) = \text{tr}(A) + \text{tr}(B)\).
  \item
    \(\text{tr}(cA) = c \cdot \text{tr}(A)\).
  \end{itemize}
\item
  Cyclic Property:

  \begin{itemize}
  \tightlist
  \item
    \(\text{tr}(AB) = \text{tr}(BA)\), as long as the products are
    defined.
  \item
    More generally,
    \(\text{tr}(ABC) = \text{tr}(BCA) = \text{tr}(CAB)\).
  \item
    But in general, \(\text{tr}(AB) \neq \text{tr}(A)\text{tr}(B)\).
  \end{itemize}
\item
  Transpose Invariance:

  \begin{itemize}
  \tightlist
  \item
    \(\text{tr}(A^T) = \text{tr}(A)\).
  \end{itemize}
\item
  Similarity Invariance:

  \begin{itemize}
  \tightlist
  \item
    If \(B = P^{-1}AP\), then \(\text{tr}(B) = \text{tr}(A)\).
  \item
    This means the trace is a similarity invariant, depending only on
    the linear transformation, not the basis.
  \end{itemize}
\end{enumerate}

\subsubsection{Trace and Eigenvalues}\label{trace-and-eigenvalues}

One of the most important connections is between the trace and
eigenvalues:

\[
\text{tr}(A) = \lambda_1 + \lambda_2 + \cdots + \lambda_n,
\]

where \(\lambda_i\) are the eigenvalues of \(A\) (counting
multiplicity).

This links the simple diagonal sum to the deep spectral properties of
the matrix.

Example:

\[
A = \begin{bmatrix} 1 & 0 \\ 0 & 3 \end{bmatrix}, \quad 
\text{tr}(A) = 4, \quad 
\lambda_1 = 1, \; \lambda_2 = 3, \quad \lambda_1 + \lambda_2 = 4.
\]

\subsubsection{Other Basic Matrix
Properties}\label{other-basic-matrix-properties}

Alongside the trace, here are some important algebraic facts that every
student of linear algebra must know:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Determinant vs.~Trace:

  \begin{itemize}
  \tightlist
  \item
    For 2×2 matrices,
    \(A = \begin{bmatrix} a & b \\ c & d \end{bmatrix}\),
    \(\text{tr}(A) = a + d\), \(\det(A) = ad - bc\).
  \item
    Together, trace and determinant encode the eigenvalues: roots of
    \(x^2 - \text{tr}(A)x + \det(A) = 0\).
  \end{itemize}
\item
  Norms and Inner Products:

  \begin{itemize}
  \tightlist
  \item
    The Frobenius norm is defined using the trace:
    \(\|A\|_F = \sqrt{\text{tr}(A^TA)}\).
  \end{itemize}
\item
  Orthogonal Invariance:

  \begin{itemize}
  \tightlist
  \item
    For any orthogonal matrix \(Q\),
    \(\text{tr}(Q^TAQ) = \text{tr}(A)\).
  \end{itemize}
\end{enumerate}

\subsubsection{Geometric and Practical
Meaning}\label{geometric-and-practical-meaning}

\begin{itemize}
\tightlist
\item
  The trace of a transformation can be seen as the sum of its action
  along the coordinate axes.
\item
  In physics, the trace of the stress tensor measures pressure.
\item
  In probability, the trace of a covariance matrix is the total variance
  of a system.
\item
  In statistics and machine learning, the trace is often used as a
  measure of overall ``size'' or complexity of a model.
\end{itemize}

\subsubsection{Everyday Analogies}\label{everyday-analogies-14}

\begin{itemize}
\tightlist
\item
  Grades on report card: The trace is like summing the ``main subjects''
  (diagonal entries) without looking at electives (off-diagonal
  entries).
\item
  Company budget: The diagonal could represent department totals, and
  the trace is the grand total.
\item
  Lighting system: Imagine each diagonal entry is a light switch
  brightness for each room; the trace is the sum total brightness in the
  building.
\end{itemize}

\subsubsection{Why It Matters}\label{why-it-matters-14}

The trace is deceptively simple but incredibly powerful:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  It connects directly to eigenvalues, forming a bridge between raw
  matrix entries and spectral theory.
\item
  It is invariant under similarity, making it a reliable measure of a
  transformation independent of basis.
\item
  It shows up in optimization, physics, statistics, and quantum
  mechanics.
\item
  It simplifies computations: many proofs in linear algebra reduce to
  trace properties.
\end{enumerate}

\subsubsection{Try It Yourself}\label{try-it-yourself-17}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute the trace of
\end{enumerate}

\[
\begin{bmatrix} 
4 & 2 & 0 \\ 
-1 & 3 & 5 \\ 
7 & 6 & 1 
\end{bmatrix}.
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Verify that \(\text{tr}(AB) = \text{tr}(BA)\) for
\end{enumerate}

\[
A = \begin{bmatrix} 1 & 2 \\ 0 & 3 \end{bmatrix}, \quad 
B = \begin{bmatrix} 4 & 0 \\ 5 & 6 \end{bmatrix}.
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  For the 2×2 matrix
\end{enumerate}

\[
\begin{bmatrix} 2 & 1 \\ 1 & 2 \end{bmatrix},
\]

compute its eigenvalues and check that their sum equals the trace.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Challenge: Show that the total variance of a dataset with covariance
  matrix \(\Sigma\) is equal to \(\text{tr}(\Sigma)\).
\end{enumerate}

Mastering the trace and its properties will prepare you for the next
leap: understanding how matrices interact with volume, orientation, and
determinants.

\subsection{19. Affine Transforms and Homogeneous
Coordinates}\label{affine-transforms-and-homogeneous-coordinates}

Up to now, matrices have been used to describe linear transformations:
scaling, rotating, reflecting, projecting. But real-world geometry often
involves more than just linear effects-it includes translations (shifts)
as well. A pure linear map cannot move the origin, so to handle
translations (and combinations of them with rotations, scalings, and
shears), we extend our toolkit to affine transformations. The secret
weapon that makes this work is the idea of homogeneous coordinates.

\subsubsection{What is an Affine
Transformation?}\label{what-is-an-affine-transformation}

An affine transformation is any map of the form:

\[
f(\mathbf{x}) = A\mathbf{x} + \mathbf{b},
\]

where \(A\) is a matrix (linear part) and \(\mathbf{b}\) is a vector
(translation part).

\begin{itemize}
\tightlist
\item
  \(A\) handles scaling, rotation, reflection, shear, or projection.
\item
  \(\mathbf{b}\) shifts everything by a constant amount.
\end{itemize}

Examples in 2D:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Rotate by 90° and then shift right by 2.
\item
  Stretch vertically by 3 and shift upward by 1.
\end{enumerate}

Affine maps preserve parallel lines and ratios of distances along lines,
but not necessarily angles or lengths.

\subsubsection{Why Linear Maps Alone Aren't
Enough}\label{why-linear-maps-alone-arent-enough}

If we only use a 2×2 matrix in 2D or 3×3 in 3D, the origin always stays
fixed. That's a limitation: real-world movements (like moving a shape
from one place to another) require shifting the origin too. To capture
both linear and translational effects uniformly, we need a clever trick.

\subsubsection{Homogeneous Coordinates}\label{homogeneous-coordinates}

The trick is to add one extra coordinate.

\begin{itemize}
\tightlist
\item
  In 2D, a point \((x, y)\) becomes \((x, y, 1)\).
\item
  In 3D, a point \((x, y, z)\) becomes \((x, y, z, 1)\).
\end{itemize}

This new representation is called homogeneous coordinates. It allows us
to fold translations into matrix multiplication.

\subsubsection{Affine Transform as a Matrix in Homogeneous
Form}\label{affine-transform-as-a-matrix-in-homogeneous-form}

In 2D:

\[
\begin{bmatrix} 
a & b & t_x \\ 
c & d & t_y \\ 
0 & 0 & 1 
\end{bmatrix}
\begin{bmatrix} 
x \\ y \\ 1
\end{bmatrix}
=
\begin{bmatrix} 
ax + by + t_x \\ 
cx + dy + t_y \\ 
1
\end{bmatrix}.
\]

Here,

\begin{itemize}
\tightlist
\item
  The 2×2 block \(\begin{bmatrix} a & b \\ c & d \end{bmatrix}\) is the
  linear part.
\item
  The last column \(\begin{bmatrix} t_x \\ t_y \end{bmatrix}\) is the
  translation.
\end{itemize}

So with one unified matrix, we can handle both linear transformations
and shifts.

\subsubsection{Examples in 2D}\label{examples-in-2d}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Translation by (2, 3):
\end{enumerate}

\[
\begin{bmatrix} 
1 & 0 & 2 \\ 
0 & 1 & 3 \\ 
0 & 0 & 1 
\end{bmatrix}.
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Scaling by 2 in x and 3 in y, then shifting by (--1, 4):
\end{enumerate}

\[
\begin{bmatrix} 
2 & 0 & -1 \\ 
0 & 3 & 4 \\ 
0 & 0 & 1 
\end{bmatrix}.
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Rotation by 90° and shift right by 5:
\end{enumerate}

\[
\begin{bmatrix} 
0 & -1 & 5 \\ 
1 & 0 & 0 \\ 
0 & 0 & 1 
\end{bmatrix}.
\]

\subsubsection{Homogeneous Coordinates in
3D}\label{homogeneous-coordinates-in-3d}

In 3D, affine transformations use 4×4 matrices. The upper-left 3×3 block
handles rotation, scaling, or shear; the last column encodes
translation.

Example: translation by (2, --1, 4):

\[
\begin{bmatrix} 
1 & 0 & 0 & 2 \\ 
0 & 1 & 0 & -1 \\ 
0 & 0 & 1 & 4 \\ 
0 & 0 & 0 & 1 
\end{bmatrix}.
\]

This formulation is universal in computer graphics and robotics.

\subsubsection{Everyday Analogies}\label{everyday-analogies-15}

\begin{itemize}
\tightlist
\item
  Photography: Moving a camera involves not only rotating it (linear
  part) but also translating it in space.
\item
  Navigation: Walking north (linear step) and then shifting east
  (translation).
\item
  Graphics software: Scaling an image larger and then dragging it to a
  new position-exactly affine transformations in action.
\end{itemize}

\subsubsection{Why It Matters}\label{why-it-matters-15}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Unified representation: Using homogeneous coordinates, we can treat
  translations as matrices, enabling consistent matrix multiplication
  for all transformations.
\item
  Practicality: This approach underpins 3D graphics pipelines,
  animation, CAD, robotics, and computer vision.
\item
  Composability: Multiple affine transformations can be combined into a
  single homogeneous matrix by multiplying them.
\item
  Geometry preserved: Affine maps preserve straight lines and
  parallelism, essential in engineering and design.
\end{enumerate}

\subsubsection{Try It Yourself}\label{try-it-yourself-18}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Write the homogeneous matrix that reflects across the x-axis and then
  shifts up by 3. Apply it to \((2, 1)\).
\item
  Construct a 4×4 homogeneous matrix that rotates around the z-axis by
  90° and translates by (1, 2, 0).
\item
  Show that multiplying two 3×3 homogeneous matrices in 2D yields
  another valid affine transform.
\item
  Challenge: Prove that affine maps preserve parallel lines by applying
  a general affine matrix to two parallel lines and checking their
  slopes.
\end{enumerate}

Mastering affine transformations and homogeneous coordinates bridges the
gap between pure linear algebra and real-world geometry, giving you the
mathematical foundation behind computer graphics, robotics, and spatial
modeling.

\subsection{20. Computing with Matrices (Cost Counts and Simple
Speedups)}\label{computing-with-matrices-cost-counts-and-simple-speedups}

Thus far, we have studied what matrices are and what they represent. But
in practice, working with matrices also means thinking about
computation-how much work operations take, how algorithms can be sped
up, and why structure matters. This section introduces the basic ideas
of computational cost in matrix operations, simple strategies for
efficiency, and why these considerations are crucial in modern
applications.

\subsubsection{Counting Operations: The Cost
Model}\label{counting-operations-the-cost-model}

The simplest way to measure the cost of a matrix operation is to count
the basic arithmetic operations (additions and multiplications).

\begin{itemize}
\item
  Matrix--vector product: For an \(m \times n\) matrix and an
  \(n \times 1\) vector:

  \begin{itemize}
  \tightlist
  \item
    Each of the \(m\) output entries requires \(n\) multiplications and
    \(n-1\) additions.
  \item
    Total cost ≈ \(2mn\) operations.
  \end{itemize}
\item
  Matrix--matrix product: For an \(m \times n\) matrix times an
  \(n \times p\) matrix:

  \begin{itemize}
  \tightlist
  \item
    Each of the \(mp\) entries requires \(n\) multiplications and
    \(n-1\) additions.
  \item
    Total cost ≈ \(2mnp\) operations.
  \end{itemize}
\item
  Gaussian elimination (solving \(Ax=b\)): For an \(n \times n\) system:

  \begin{itemize}
  \tightlist
  \item
    Roughly \(\tfrac{2}{3}n^3\) operations.
  \end{itemize}
\end{itemize}

These counts show how quickly costs grow with dimension. Doubling \(n\)
makes the work 8 times larger for elimination.

\subsubsection{Why Cost Counts Matter}\label{why-cost-counts-matter}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Scalability: Small problems (2×2 or 3×3) are trivial, but modern
  datasets involve matrices with millions of rows. Knowing the cost is
  essential.
\item
  Feasibility: Some exact algorithms become impossible for very large
  matrices. Approximation methods are used instead.
\item
  Optimization: Engineers and scientists design specialized algorithms
  to reduce costs by exploiting structure (sparsity, symmetry,
  triangular form).
\end{enumerate}

\subsubsection{Simple Speedups with
Structure}\label{simple-speedups-with-structure}

\begin{itemize}
\tightlist
\item
  Diagonal Matrices: Multiplying by a diagonal matrix costs only \(n\)
  operations (scale each component).
\item
  Triangular Matrices: Solving triangular systems requires only
  \(\tfrac{1}{2}n^2\) operations (substitution), far cheaper than
  general elimination.
\item
  Sparse Matrices: If most entries are zero, we skip multiplications by
  zero. For large sparse systems, cost scales with the number of
  nonzeros, not \(n^2\).
\item
  Block Matrices: Breaking matrices into blocks allows algorithms to
  reuse optimized small-matrix routines (common in BLAS libraries).
\end{itemize}

\subsubsection{Memory Considerations}\label{memory-considerations}

Cost is not only arithmetic: storage also matters.

\begin{itemize}
\tightlist
\item
  A dense \(n \times n\) matrix requires \(n^2\) entries of memory.
\item
  Sparse storage formats (like CSR, COO) record only nonzero entries and
  their positions, saving massive space.
\item
  Memory access speed can dominate arithmetic cost in large
  computations.
\end{itemize}

\subsubsection{Parallelism and Hardware}\label{parallelism-and-hardware}

Modern computing leverages hardware for speed:

\begin{itemize}
\tightlist
\item
  Vectorization (SIMD): Perform many multiplications at once.
\item
  Parallelization: Split work across many CPU cores.
\item
  GPUs: Specialize in massive parallel matrix--vector and matrix--matrix
  operations (critical in deep learning).
\end{itemize}

This is why linear algebra libraries (BLAS, LAPACK, cuBLAS) are
indispensable: they squeeze performance from hardware.

\subsubsection{Everyday Analogies}\label{everyday-analogies-16}

\begin{itemize}
\tightlist
\item
  Cooking: Preparing one dish is easy. Preparing 100 dishes means
  thinking about batch size, oven space, and parallel steps.
\item
  Budgeting: Adding two small budgets by hand is fine; merging thousands
  of spreadsheets requires software and structure.
\item
  Travel: Driving across town is quick. Driving cross-country requires
  fuel planning, route optimization, and rest stops-scale changes
  everything.
\end{itemize}

\subsubsection{Why It Matters}\label{why-it-matters-16}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Efficiency: Understanding cost lets us choose the right algorithm for
  the job.
\item
  Algorithm design: Structured matrices (diagonal, sparse, orthogonal)
  make computations much faster and more stable.
\item
  Applications: Every field that uses matrices-graphics, optimization,
  statistics, AI-relies on efficient computation.
\item
  Foundations: Later topics like LU/QR/SVD factorization are motivated
  by balancing cost and stability.
\end{enumerate}

\subsubsection{Try It Yourself}\label{try-it-yourself-19}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute the number of operations required for multiplying a 1000×500
  matrix with a 500×200 matrix. Compare with multiplying a 1000×1000
  dense matrix by a vector.
\item
  Show how solving a 3×3 triangular system is faster than Gaussian
  elimination. Count the exact multiplications and additions.
\item
  Construct a sparse 5×5 matrix with only 7 nonzero entries. Estimate
  the cost of multiplying it by a vector versus a dense 5×5 matrix.
\item
  Challenge: Suppose you need to store a 1,000,000×1,000,000 dense
  matrix. Estimate how much memory (in bytes) it would take if each
  entry is 8 bytes. Could it fit on a laptop? Why do sparse formats save
  the day?
\end{enumerate}

By learning to count costs and exploit structure, you prepare yourself
not only to understand matrices abstractly but also to use them
effectively in real-world, large-scale problems. This balance between
theory and computation is at the heart of modern linear algebra.

\subsubsection{Closing}\label{closing-1}

\begin{verbatim}
Patterns intertwine,
transformations gently fold,
structure in the square.
\end{verbatim}

\section{Chapter 3. Linear Systems and
Elimination}\label{chapter-3.-linear-systems-and-elimination-1}

\subsection{21. From Equations to
Matrices}\label{from-equations-to-matrices}

Linear algebra often begins with systems of equations-collections of
unknowns linked by linear relationships. While these systems can be
solved directly using substitution or elimination, they quickly become
messy when there are many variables. The key insight of linear algebra
is that all systems of linear equations can be captured compactly by
matrices and vectors. This section explains how we move from equations
written out in words and symbols to the matrix form that powers
computation.

\subsubsection{A Simple Example}\label{a-simple-example}

Consider this system of two equations in two unknowns:

\[
\begin{cases}  
2x + y = 5 \\  
3x - y = 4  
\end{cases}
\]

At first glance, this is just algebra: two equations, two unknowns. But
notice the structure: each equation is a sum of multiples of the
variables, set equal to a constant. This pattern-linear combinations of
unknowns equal to a result-is exactly what matrices capture.

\subsubsection{Writing in Coefficient Table
Form}\label{writing-in-coefficient-table-form}

Extract the coefficients of each variable from the system:

\begin{itemize}
\tightlist
\item
  First equation: coefficients are \(2\) for \(x\), \(1\) for \(y\).
\item
  Second equation: coefficients are \(3\) for \(x\), \(-1\) for \(y\).
\end{itemize}

Arrange these coefficients in a rectangular array:

\[
A = \begin{bmatrix}  
2 & 1 \\  
3 & -1  
\end{bmatrix}.
\]

This matrix \(A\) is called the coefficient matrix.

Next, write the unknowns as a vector:

\[
\mathbf{x} = \begin{bmatrix} x \\ y \end{bmatrix}.
\]

Finally, write the right-hand side constants as another vector:

\[
\mathbf{b} = \begin{bmatrix} 5 \\ 4 \end{bmatrix}.
\]

Now the entire system can be written in a single line:

\[
A\mathbf{x} = \mathbf{b}.
\]

\subsubsection{Why This is Powerful}\label{why-this-is-powerful}

This compact form hides no information; it is equivalent to the original
equations. But it gives us enormous advantages:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Clarity: We see the structure clearly-the system is ``matrix times
  vector equals vector.''
\item
  Scalability: Whether we have 2 equations or 2000, the same notation
  applies.
\item
  Tools: All the machinery of matrix operations (elimination, inverses,
  decompositions) now becomes available.
\item
  Geometry: The matrix equation \(A\mathbf{x} = \mathbf{b}\) means:
  combine the columns of \(A\) (scaled by entries of x) to land on b.
\end{enumerate}

\subsubsection{A Larger Example}\label{a-larger-example}

System of three equations in three unknowns:

\[
\begin{cases}  
x + 2y - z = 2 \\  
2x - y + 3z = 1 \\  
3x + y + 2z = 4  
\end{cases}
\]

\begin{itemize}
\item
  Coefficient matrix:

  \[
  A = \begin{bmatrix}  
  1 & 2 & -1 \\  
  2 & -1 & 3 \\  
  3 & 1 & 2  
  \end{bmatrix}.
  \]
\item
  Unknown vector:

  \[
  \mathbf{x} = \begin{bmatrix} x \\ y \\ z \end{bmatrix}.
  \]
\item
  Constant vector:

  \[
  \mathbf{b} = \begin{bmatrix} 2 \\ 1 \\ 4 \end{bmatrix}.
  \]
\end{itemize}

Matrix form:

\[
A\mathbf{x} = \mathbf{b}.
\]

This single equation captures three equations and three unknowns in one
object.

\subsubsection{Row vs.~Column View}\label{row-vs.-column-view}

\begin{itemize}
\tightlist
\item
  Row view: Each row of \(A\) dotted with x gives one equation.
\item
  Column view: The entire system means b is a linear combination of the
  columns of \(A\).
\end{itemize}

For the 2×2 case earlier:

\[
A\mathbf{x} = \begin{bmatrix} 2 & 1 \\ 3 & -1 \end{bmatrix}  
\begin{bmatrix} x \\ y \end{bmatrix}  
= x \begin{bmatrix} 2 \\ 3 \end{bmatrix} + y \begin{bmatrix} 1 \\ -1 \end{bmatrix}.
\]

So solving the system means finding scalars \(x, y\) that combine the
columns of \(A\) to reach \(\mathbf{b}\).

\subsubsection{Augmented Matrix Form}\label{augmented-matrix-form}

Sometimes we want to save space further. We can put the coefficients and
constants side by side in an augmented matrix:

\[
[A | \mathbf{b}] =  
\begin{bmatrix}  
2 & 1 & | & 5 \\  
3 & -1 & | & 4  
\end{bmatrix}.
\]

This form is especially useful for elimination methods, where we
manipulate rows without writing variables at each step.

\subsubsection{Everyday Analogies}\label{everyday-analogies-17}

\begin{itemize}
\tightlist
\item
  Recipes: A recipe tells you how many units of each ingredient
  (coefficients) produce the dish (constant result). Writing it as a
  matrix is like organizing all recipes into a clean table.
\item
  Schedules: Each row of a timetable (equation) corresponds to one
  condition. Putting it into matrix form organizes it into one big grid.
\item
  Construction: The coefficients are like ``how many bricks per wall,''
  while the vector x is ``number of walls built.'' The product gives
  total bricks, and the system sets the required totals.
\end{itemize}

\subsubsection{Why It Matters}\label{why-it-matters-17}

This step-rewriting equations as matrix form-is the gateway into linear
algebra. Once you can do it, you no longer think of systems of equations
as isolated lines on paper, but as a unified object that can be studied
with general tools. It opens the door to:

\begin{itemize}
\tightlist
\item
  Gaussian elimination,
\item
  rank and null space,
\item
  determinants,
\item
  eigenvalues,
\item
  optimization methods.
\end{itemize}

Every major idea flows from this compact representation.

\subsubsection{Try It Yourself}\label{try-it-yourself-20}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Write the system

  \[
  \begin{cases}  
  4x - y = 7 \\  
  -2x + 3y = 5  
  \end{cases}
  \]

  in matrix form.
\item
  For the system

  \[
  \begin{cases}  
  x + y + z = 6 \\  
  2x - y + z = 3 \\  
  x - y - z = -2  
  \end{cases}
  \]

  build the coefficient matrix, unknown vector, and constant vector.
\item
  Express the augmented matrix for the above system.
\item
  Challenge: Interpret the system in column view. What does it mean
  geometrically to express \((6, 3, -2)\) as a linear combination of the
  columns of the coefficient matrix?
\end{enumerate}

By practicing these rewrites, you will see that linear algebra is not
about juggling many equations-it is about seeing structure in one
compact equation. This step transforms scattered equations into the
language of matrices, where the real power begins.

\subsection{22. Row Operations}\label{row-operations}

Once a system of linear equations has been expressed as a matrix, the
next step is to simplify that matrix into a form where the solutions
become clear. The main tool for this simplification is the set of
elementary row operations. These operations allow us to manipulate the
rows of a matrix in systematic ways that preserve the solution set of
the corresponding system of equations.

\subsubsection{The Three Types of Row
Operations}\label{the-three-types-of-row-operations}

There are exactly three types of legal row operations, each with a clear
algebraic meaning:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Row Swapping (\(R_i \leftrightarrow R_j\)): Exchange two rows. This
  corresponds to reordering equations in a system. Since the order of
  equations doesn't change the solutions, this operation is always
  valid.

  Example:

  \[
  \begin{bmatrix}  
  2 & 1 & | & 5 \\  
  3 & -1 & | & 4  
  \end{bmatrix}  
  \quad \longrightarrow \quad  
  \begin{bmatrix}  
  3 & -1 & | & 4 \\  
  2 & 1 & | & 5  
  \end{bmatrix}.
  \]
\item
  Row Scaling (\(R_i \to cR_i, \; c \neq 0\)): Multiply all entries in a
  row by a nonzero constant. This is like multiplying both sides of an
  equation by the same number, which doesn't change its truth.

  Example:

  \[
  \begin{bmatrix}  
  2 & 1 & | & 5 \\  
  3 & -1 & | & 4  
  \end{bmatrix}  
  \quad \longrightarrow \quad  
  \begin{bmatrix}  
  1 & \tfrac{1}{2} & | & \tfrac{5}{2} \\  
  3 & -1 & | & 4  
  \end{bmatrix}.
  \]
\item
  Row Replacement (\(R_i \to R_i + cR_j\)): Add a multiple of one row to
  another. This corresponds to replacing one equation with a linear
  combination of itself and another, a fundamental elimination step.

  Example:

  \[
  \begin{bmatrix}  
  2 & 1 & | & 5 \\  
  3 & -1 & | & 4  
  \end{bmatrix}  
  \quad \overset{R_2 \to R_2 - \tfrac{3}{2}R_1}{\longrightarrow} \quad  
  \begin{bmatrix}  
  2 & 1 & | & 5 \\  
  0 & -\tfrac{5}{2} & | & -\tfrac{7}{2}  
  \end{bmatrix}.
  \]
\end{enumerate}

\subsubsection{Why These Are the Only Allowed
Operations}\label{why-these-are-the-only-allowed-operations}

These three operations are the backbone of elimination because they do
not alter the solution set of the system. Each is equivalent to applying
an invertible transformation:

\begin{itemize}
\tightlist
\item
  Row swaps are reversible (swap back).
\item
  Row scalings by \(c\) can be undone by scaling by \(1/c\).
\item
  Row replacements can be undone by adding the opposite multiple.
\end{itemize}

Thus, each operation is invertible, and the transformed system is always
equivalent to the original.

\subsubsection{Row Operations as
Matrices}\label{row-operations-as-matrices}

Each elementary row operation can itself be represented by multiplying
on the left with a special matrix called an elementary matrix.

For example:

\begin{itemize}
\item
  Swapping rows 1 and 2 in a 2×2 system is done by

  \[
  E = \begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix}.
  \]
\item
  Scaling row 1 by 3 in a 2×2 system is done by

  \[
  E = \begin{bmatrix} 3 & 0 \\ 0 & 1 \end{bmatrix}.
  \]
\end{itemize}

This perspective is crucial later for factorization methods like LU
decomposition, where elimination is expressed as a product of elementary
matrices.

\subsubsection{Step-by-Step Example}\label{step-by-step-example}

System:

\[
\begin{cases}  
x + 2y = 4 \\  
3x + 4y = 10  
\end{cases}
\]

Augmented matrix:

\[
\begin{bmatrix}  
1 & 2 & | & 4 \\  
3 & 4 & | & 10  
\end{bmatrix}.
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Eliminate the \(3x\) under the first pivot: \(R_2 \to R_2 - 3R_1\).

  \[
  \begin{bmatrix}  
  1 & 2 & | & 4 \\  
  0 & -2 & | & -2  
  \end{bmatrix}.
  \]
\item
  Scale the second row: \(R_2 \to -\tfrac{1}{2}R_2\).

  \[
  \begin{bmatrix}  
  1 & 2 & | & 4 \\  
  0 & 1 & | & 1  
  \end{bmatrix}.
  \]
\item
  Eliminate above the pivot: \(R_1 \to R_1 - 2R_2\).

  \[
  \begin{bmatrix}  
  1 & 0 & | & 2 \\  
  0 & 1 & | & 1  
  \end{bmatrix}.
  \]
\end{enumerate}

Solution: \(x = 2, \; y = 1\).

\subsubsection{Geometry of Row
Operations}\label{geometry-of-row-operations}

Row operations do not alter the solution space:

\begin{itemize}
\tightlist
\item
  Swapping rows reorders equations but keeps the same lines or planes.
\item
  Scaling rows rescales equations but leaves their geometric set
  unchanged.
\item
  Adding rows corresponds to combining constraints, but the shared
  intersection (solution set) is preserved.
\end{itemize}

Thus, row operations act like ``reshaping the system'' while leaving the
intersection intact.

\subsubsection{Everyday Analogies}\label{everyday-analogies-18}

\begin{itemize}
\tightlist
\item
  Cooking recipe: Scaling a row is like doubling all ingredients in one
  recipe-it doesn't change the proportions.
\item
  Task list: Swapping two rows is just reordering tasks; nothing about
  the tasks themselves changes.
\item
  Budgeting: Adding one department's budget equation to another is just
  reorganizing financial records, not altering totals.
\end{itemize}

\subsubsection{Why It Matters}\label{why-it-matters-18}

Row operations are the essential moves in solving linear systems by hand
or computer. They:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Make elimination systematic.
\item
  Preserve solution sets while simplifying structure.
\item
  Lay the groundwork for echelon forms, rank, and factorization.
\item
  Provide the mechanical steps that computers automate in Gaussian
  elimination.
\end{enumerate}

\subsubsection{Try It Yourself}\label{try-it-yourself-21}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Apply row operations to reduce

  \[
  \begin{bmatrix}  
  2 & 1 & | & 7 \\  
  1 & -1 & | & 1  
  \end{bmatrix}
  \]

  to a form where the solution is obvious.
\item
  Show explicitly why swapping two equations in a system doesn't change
  its solutions.
\item
  Construct the elementary matrix for ``add --2 times row 1 to row 3''
  in a 3×3 system.
\item
  Challenge: Prove that any elementary row operation corresponds to
  multiplication by an invertible matrix.
\end{enumerate}

Mastering these operations equips you with the mechanical and conceptual
foundation for the next stage: systematically reducing matrices to
row-echelon form.

\subsection{23. Row-Echelon and Reduced Row-Echelon
Forms}\label{row-echelon-and-reduced-row-echelon-forms}

After introducing row operations, the natural question is: \emph{what
are we trying to achieve by performing them?} The answer is to transform
a matrix into a standardized, simplified form where the solutions to the
corresponding system of equations can be read off directly. Two such
standardized forms are central in linear algebra: row-echelon form (REF)
and reduced row-echelon form (RREF).

\subsubsection{Row-Echelon Form (REF)}\label{row-echelon-form-ref}

A matrix is in row-echelon form if:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  All nonzero rows are above any rows of all zeros.
\item
  In each nonzero row, the first nonzero entry (called the leading entry
  or pivot) is to the right of the leading entry of the row above it.
\item
  All entries below a pivot are zero.
\end{enumerate}

Example of REF:

\[
\begin{bmatrix}  
1 & 2 & 3 & | & 4 \\  
0 & 1 & -1 & | & 2 \\  
0 & 0 & 5 & | & -3 \\  
0 & 0 & 0 & | & 0  
\end{bmatrix}.
\]

Here, the pivots are the first 1 in row 1, the 1 in row 2, and the 5 in
row 3. Each pivot is to the right of the one above it, and all entries
below pivots are zero.

\subsubsection{Reduced Row-Echelon Form
(RREF)}\label{reduced-row-echelon-form-rref}

A matrix is in reduced row-echelon form if, in addition to the rules of
REF:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Each pivot is equal to 1.
\item
  Each pivot is the only nonzero entry in its column (everything above
  and below pivots is zero).
\end{enumerate}

Example of RREF:

\[
\begin{bmatrix}  
1 & 0 & 0 & | & 3 \\  
0 & 1 & 0 & | & -2 \\  
0 & 0 & 1 & | & 1  
\end{bmatrix}.
\]

This form is so simplified that solutions can be read directly: here,
\(x=3\), \(y=-2\), \(z=1\).

\subsubsection{Relationship Between REF and
RREF}\label{relationship-between-ref-and-rref}

\begin{itemize}
\tightlist
\item
  REF is easier to reach-it only requires eliminating entries below
  pivots.
\item
  RREF requires going further-clearing entries above pivots and scaling
  pivots to 1.
\item
  Every matrix can be reduced to REF (many possible versions), but RREF
  is unique: no matter how you proceed, if you carry out all row
  operations fully, you end with the same RREF.
\end{itemize}

\subsubsection{Example: Step-by-Step to
RREF}\label{example-step-by-step-to-rref}

System:

\[
\begin{cases}  
x + 2y + z = 4 \\  
2x + 5y + z = 7 \\  
3x + 6y + 2z = 10  
\end{cases}
\]

Augmented matrix:

\[
\begin{bmatrix}  
1 & 2 & 1 & | & 4 \\  
2 & 5 & 1 & | & 7 \\  
3 & 6 & 2 & | & 10  
\end{bmatrix}.
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Eliminate below first pivot (the 1 in row 1, col 1):

  \begin{itemize}
  \tightlist
  \item
    \(R_2 \to R_2 - 2R_1\)
  \item
    \(R_3 \to R_3 - 3R_1\)
  \end{itemize}

  \[
  \begin{bmatrix}  
  1 & 2 & 1 & | & 4 \\  
  0 & 1 & -1 & | & -1 \\  
  0 & 0 & -1 & | & -2  
  \end{bmatrix}.
  \]

  This is now in REF.
\item
  Scale pivots and eliminate above them:

  \begin{itemize}
  \tightlist
  \item
    \(R_3 \to -R_3\) to make pivot 1.
  \item
    \(R_2 \to R_2 + R_3\).
  \item
    \(R_1 \to R_1 - R_2 - R_3\).
  \end{itemize}

  Final:

  \[
  \begin{bmatrix}  
  1 & 0 & 0 & | & 2 \\  
  0 & 1 & 0 & | & 1 \\  
  0 & 0 & 1 & | & 2  
  \end{bmatrix}.
  \]
\end{enumerate}

Solution: \(x=2, y=1, z=2\).

\subsubsection{Geometry of REF and RREF}\label{geometry-of-ref-and-rref}

\begin{itemize}
\tightlist
\item
  REF corresponds to simplifying the system step by step, making it
  ``triangular'' so variables can be solved one after another.
\item
  RREF corresponds to a system that is fully disentangled-each variable
  isolated, with its value or free-variable relationship explicitly
  visible.
\end{itemize}

\subsubsection{Everyday Analogies}\label{everyday-analogies-19}

\begin{itemize}
\tightlist
\item
  Filing papers: REF is like stacking documents in neat piles, but still
  grouped. RREF is like sorting every document individually into labeled
  folders.
\item
  Solving a puzzle: REF gives you partial progress (pieces are grouped),
  while RREF finishes the puzzle so each piece is in its exact place.
\item
  Cooking: REF is like preparing ingredients into rough categories
  (vegetables chopped, meat trimmed). RREF is the final stage where
  every dish is cooked and plated, fully ready.
\end{itemize}

\subsubsection{Why It Matters}\label{why-it-matters-19}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  REF is the foundation of Gaussian elimination, the workhorse algorithm
  for solving systems.
\item
  RREF gives complete clarity: unique representation of solution sets,
  revealing free and pivot variables.
\item
  RREF underlies algorithms in computer algebra systems, symbolic
  solvers, and educational tools.
\item
  Understanding these forms builds intuition for rank, null space, and
  solution structure.
\end{enumerate}

\subsubsection{Try It Yourself}\label{try-it-yourself-22}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Reduce

  \[
  \begin{bmatrix}  
  2 & 4 & | & 6 \\  
  1 & 3 & | & 5  
  \end{bmatrix}
  \]

  to REF, then RREF.
\item
  Find the RREF of

  \[
  \begin{bmatrix}  
  1 & 1 & 1 & | & 3 \\  
  2 & 3 & 4 & | & 8 \\  
  1 & 2 & 3 & | & 5  
  \end{bmatrix}.
  \]
\item
  Explain why two different elimination sequences can lead to different
  REF but the same RREF.
\item
  Challenge: Prove that every matrix has a unique RREF by considering
  the effect of row operations systematically.
\end{enumerate}

Reaching row-echelon and reduced row-echelon forms transforms messy
systems into structured ones, turning algebraic clutter into an
organized path to solutions.

\subsection{24. Pivots, Free Variables, and Leading
Ones}\label{pivots-free-variables-and-leading-ones}

When reducing a matrix to row-echelon or reduced row-echelon form,
certain positions in the matrix take on a special importance. These are
the pivots-the leading nonzero entries in each row. Around them, the
entire solution structure of a linear system is organized. Understanding
pivots, the variables they anchor, and the freedom that arises from
non-pivot columns is essential to solving linear equations
systematically.

\subsubsection{What is a Pivot?}\label{what-is-a-pivot}

In row-echelon form, a pivot is the first nonzero entry in a row, moving
from left to right. After scaling in reduced row-echelon form, each
pivot is set to exactly 1.

Example:

\[
\begin{bmatrix}  
1 & 2 & 0 & | & 5 \\  
0 & 1 & 3 & | & -2 \\  
0 & 0 & 0 & | & 0  
\end{bmatrix}
\]

\begin{itemize}
\tightlist
\item
  Pivot in row 1: the 1 in column 1.
\item
  Pivot in row 2: the 1 in column 2.
\item
  Column 3 has no pivot.
\end{itemize}

Columns with pivots are pivot columns. Columns without pivots correspond
to free variables.

\subsubsection{Pivot Variables vs.~Free
Variables}\label{pivot-variables-vs.-free-variables}

\begin{itemize}
\tightlist
\item
  Pivot variables: Variables that align with pivot columns. They are
  determined by the equations.
\item
  Free variables: Variables that align with non-pivot columns. They are
  unconstrained and can take arbitrary values.
\end{itemize}

Example:

\[
\begin{bmatrix}  
1 & 0 & 2 & | & 3 \\  
0 & 1 & -1 & | & 4  
\end{bmatrix}.
\]

This corresponds to:

\[
x_1 + 2x_3 = 3, \quad x_2 - x_3 = 4.
\]

Here:

\begin{itemize}
\tightlist
\item
  \(x_1\) and \(x_2\) are pivot variables (from pivot columns 1 and 2).
\item
  \(x_3\) is a free variable.
\end{itemize}

Thus, \(x_1\) and \(x_2\) depend on \(x_3\):

\[
x_1 = 3 - 2x_3, \quad x_2 = 4 + x_3, \quad x_3 \text{ free}.
\]

The solution set is infinite, described by the freedom in \(x_3\).

\subsubsection{Geometric Meaning}\label{geometric-meaning-1}

\begin{itemize}
\tightlist
\item
  Pivot variables represent coordinates that are ``pinned down.''
\item
  Free variables correspond to directions along which the solution can
  extend infinitely.
\end{itemize}

In 2D:

\begin{itemize}
\tightlist
\item
  If there is one pivot variable and one free variable, solutions form a
  line. In 3D:
\item
  Two pivots, one free → solutions form a line.
\item
  One pivot, two free → solutions form a plane.
\end{itemize}

Thus, the number of free variables determines the dimension of the
solution set.

\subsubsection{Rank and Free Variables}\label{rank-and-free-variables}

The number of pivot columns equals the rank of the matrix.

If the coefficient matrix \(A\) is \(m \times n\):

\begin{itemize}
\tightlist
\item
  Rank = number of pivots.
\item
  Number of free variables = \(n - \text{rank}(A)\).
\end{itemize}

This is the rank--nullity connection in action:

\[
\text{number of variables} = \text{rank} + \text{nullity}.
\]

\subsubsection{Step-by-Step Example}\label{step-by-step-example-1}

System:

\[
\begin{cases}  
x + 2y + z = 4 \\  
2x + 5y + z = 7  
\end{cases}
\]

Augmented matrix:

\[
\begin{bmatrix}  
1 & 2 & 1 & | & 4 \\  
2 & 5 & 1 & | & 7  
\end{bmatrix}.
\]

Reduce:

\begin{itemize}
\item
  \(R_2 \to R_2 - 2R_1\) →

  \[
  \begin{bmatrix}  
  1 & 2 & 1 & | & 4 \\  
  0 & 1 & -1 & | & -1  
  \end{bmatrix}.
  \]
\end{itemize}

Now:

\begin{itemize}
\tightlist
\item
  Pivot columns: 1 and 2 → variables \(x, y\).
\item
  Free column: 3 → variable \(z\).
\end{itemize}

Solution:

\[
x = 4 - 2y - z, \quad y = -1 + z, \quad z \text{ free}.
\]

Substitute:

\[
(x, y, z) = (6 - 3z, \; -1 + z, \; z).
\]

Solutions form a line in 3D parameterized by \(z\).

\subsubsection{Why Leading Ones Matter}\label{why-leading-ones-matter}

In RREF, each pivot is scaled to 1, making it easy to isolate pivot
variables. Without leading ones, equations may still be correct but
harder to interpret.

For example:

\[
\begin{bmatrix}  
2 & 0 & | & 6 \\  
0 & -3 & | & 9  
\end{bmatrix}
\]

becomes

\[
\begin{bmatrix}  
1 & 0 & | & 3 \\  
0 & 1 & | & -3  
\end{bmatrix}.
\]

The solutions are immediately visible: \(x=3, y=-3\).

\subsubsection{Everyday Analogies}\label{everyday-analogies-20}

\begin{itemize}
\tightlist
\item
  Team roles: Pivot variables are assigned jobs, while free variables
  can choose freely, shaping the group's flexibility.
\item
  Recipe ingredients: Pivots are required ingredients with fixed
  proportions, free variables are optional additions.
\item
  Construction plans: Pivot columns are load-bearing beams, free columns
  are spaces that can be filled in flexibly.
\end{itemize}

\subsubsection{Why It Matters}\label{why-it-matters-20}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Identifying pivots shows which variables are determined and which are
  free.
\item
  The number of pivots defines rank, a central concept in linear
  algebra.
\item
  Free variables determine whether the system has a unique solution,
  infinitely many, or none.
\item
  Leading ones in RREF give immediate transparency to the solution set.
\end{enumerate}

\subsubsection{Try It Yourself}\label{try-it-yourself-23}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Reduce

  \[
  \begin{bmatrix}  
  1 & 3 & 1 & | & 5 \\  
  2 & 6 & 2 & | & 10  
  \end{bmatrix}
  \]

  and identify pivot and free variables.
\item
  For the system

  \[
  x + y + z = 2, \quad 2x + 3y + 5z = 7,
  \]

  write the RREF and express the solution with free variables.
\item
  Compute the rank and number of free variables of a 3×5 matrix with two
  pivot columns.
\item
  Challenge: Show that if the number of pivots equals the number of
  variables, the system has either no solution or a unique solution, but
  never infinitely many.
\end{enumerate}

Understanding pivots and free variables provides the key to classifying
solution sets: unique, infinite, or none. This classification lies at
the heart of solving linear systems.

\subsection{25. Solving Consistent
Systems}\label{solving-consistent-systems}

A system of linear equations is called consistent if it has at least one
solution. Consistency is the first property to check when working with a
system, because before worrying about uniqueness or parametrization, we
must know whether a solution exists at all. This section explains how to
recognize consistent systems, how to solve them using row-reduction, and
how to describe their solutions in terms of pivots and free variables.

\subsubsection{What Consistency Means}\label{what-consistency-means}

Given a system \(A\mathbf{x} = \mathbf{b}\):

\begin{itemize}
\tightlist
\item
  Consistent: At least one solution \(\mathbf{x}\) satisfies the system.
\item
  Inconsistent: No solution exists.
\end{itemize}

Consistency depends on the relationship between the vector
\(\mathbf{b}\) and the column space of \(A\):

\[
\mathbf{b} \in \text{Col}(A) \quad \iff \quad \text{system is consistent}.
\]

If \(\mathbf{b}\) cannot be written as a linear combination of the
columns of \(A\), the system has no solution.

\subsubsection{Checking Consistency with Row
Reduction}\label{checking-consistency-with-row-reduction}

To test consistency, reduce the augmented matrix \([A | \mathbf{b}]\) to
row-echelon form.

\begin{itemize}
\item
  If you find a row of the form:

  \[
  [0 \;\; 0 \;\; \dots \;\; 0 \;|\; c], \quad c \neq 0,
  \]

  then the system is inconsistent (contradiction: 0 = c).
\item
  If no such contradiction appears, the system is consistent.
\end{itemize}

\subsubsection{Example 1: Consistent System with Unique
Solution}\label{example-1-consistent-system-with-unique-solution}

System:

\[
\begin{cases}  
x + y = 2 \\  
x - y = 0  
\end{cases}
\]

Augmented matrix:

\[
\begin{bmatrix}  
1 & 1 & | & 2 \\  
1 & -1 & | & 0  
\end{bmatrix}.
\]

Row reduce:

\begin{itemize}
\item
  \(R_2 \to R_2 - R_1\):

  \[
  \begin{bmatrix}  
  1 & 1 & | & 2 \\  
  0 & -2 & | & -2  
  \end{bmatrix}.
  \]
\item
  \(R_2 \to -\tfrac{1}{2}R_2\):

  \[
  \begin{bmatrix}  
  1 & 1 & | & 2 \\  
  0 & 1 & | & 1  
  \end{bmatrix}.
  \]
\item
  \(R_1 \to R_1 - R_2\):

  \[
  \begin{bmatrix}  
  1 & 0 & | & 1 \\  
  0 & 1 & | & 1  
  \end{bmatrix}.
  \]
\end{itemize}

Solution: \(x = 1, \; y = 1\). Unique solution.

\subsubsection{Example 2: Consistent System with Infinitely Many
Solutions}\label{example-2-consistent-system-with-infinitely-many-solutions}

System:

\[
\begin{cases}  
x + y + z = 3 \\  
2x + 2y + 2z = 6  
\end{cases}
\]

Augmented matrix:

\[
\begin{bmatrix}  
1 & 1 & 1 & | & 3 \\  
2 & 2 & 2 & | & 6  
\end{bmatrix}.
\]

Row reduce:

\begin{itemize}
\item
  \(R_2 \to R_2 - 2R_1\):

  \[
  \begin{bmatrix}  
  1 & 1 & 1 & | & 3 \\  
  0 & 0 & 0 & | & 0  
  \end{bmatrix}.
  \]
\end{itemize}

No contradiction, so consistent. Solution:

\[
x = 3 - y - z, \quad y \text{ free}, \quad z \text{ free}.
\]

The solution set is a plane in \(\mathbb{R}^3\).

\subsubsection{Example 3: Inconsistent System (for
contrast)}\label{example-3-inconsistent-system-for-contrast}

System:

\[
\begin{cases}  
x + y = 1 \\  
x + y = 2  
\end{cases}
\]

Augmented matrix:

\[
\begin{bmatrix}  
1 & 1 & | & 1 \\  
1 & 1 & | & 2  
\end{bmatrix}.
\]

Row reduce:

\begin{itemize}
\item
  \(R_2 \to R_2 - R_1\):

  \[
  \begin{bmatrix}  
  1 & 1 & | & 1 \\  
  0 & 0 & | & 1  
  \end{bmatrix}.
  \]
\end{itemize}

Contradiction: \(0 = 1\). Inconsistent, no solution.

\subsubsection{Geometric Interpretation of
Consistency}\label{geometric-interpretation-of-consistency}

\begin{itemize}
\item
  In 2D:

  \begin{itemize}
  \tightlist
  \item
    Two lines intersect at a point → consistent, unique solution.
  \item
    Two lines overlap → consistent, infinitely many solutions.
  \item
    Two lines are parallel and distinct → inconsistent, no solution.
  \end{itemize}
\item
  In 3D:

  \begin{itemize}
  \tightlist
  \item
    Three planes intersect at a point → unique solution.
  \item
    Planes intersect along a line or coincide → infinitely many
    solutions.
  \item
    Planes fail to meet (like a triangular ``gap'') → no solution.
  \end{itemize}
\end{itemize}

\subsubsection{Pivot Structure and
Solutions}\label{pivot-structure-and-solutions}

\begin{itemize}
\tightlist
\item
  Unique solution: Every variable is a pivot variable (no free
  variables).
\item
  Infinitely many solutions: At least one free variable exists, but no
  contradiction.
\item
  No solution: Contradictory row appears in augmented matrix.
\end{itemize}

\subsubsection{Everyday Analogies}\label{everyday-analogies-21}

\begin{itemize}
\tightlist
\item
  Meeting point: If everyone agrees on the same café (unique solution),
  the plan is consistent. If they agree on any café along a certain
  street (infinite solutions), it's still consistent. If they give
  completely different addresses, there's no consistent plan.
\item
  Recipes: If the ingredients match exactly one way, unique solution. If
  they can vary while still yielding the dish, infinite solutions. If
  the instructions contradict, no dish is possible.
\end{itemize}

\subsubsection{Why It Matters}\label{why-it-matters-21}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Consistency is the first checkpoint in solving systems.
\item
  The classification into unique, infinite, or none underpins all of
  linear algebra.
\item
  Understanding consistency ties algebra (row operations) to geometry
  (intersections of lines, planes, hyperplanes).
\item
  These ideas scale: in data science and engineering, checking whether
  equations are consistent is equivalent to asking if a model fits
  observed data.
\end{enumerate}

\subsubsection{Try It Yourself}\label{try-it-yourself-24}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Reduce the augmented matrix

  \[
  \begin{bmatrix}  
  1 & 2 & 1 & | & 5 \\  
  2 & 4 & 2 & | & 10 \\  
  3 & 6 & 3 & | & 15  
  \end{bmatrix}
  \]

  and determine if the system is consistent.
\item
  Classify the system as having unique, infinite, or no solutions:

  \[
  \begin{cases}  
  x + y + z = 2 \\  
  x - y + z = 0 \\  
  2x + 0y + 2z = 3  
  \end{cases}
  \]
\item
  Explain geometrically what it means when the augmented matrix has a
  contradictory row.
\item
  Challenge: Show algebraically that a system is consistent if and only
  if \(\mathbf{b}\) lies in the span of the columns of \(A\).
\end{enumerate}

Consistent systems mark the balance point between algebraic rules and
geometric reality: they are where equations and space meet in harmony.

\subsection{26. Detecting Inconsistency}\label{detecting-inconsistency}

Not every system of linear equations has a solution. Some are
inconsistent, meaning the equations contradict one another and no vector
\(\mathbf{x}\) can satisfy them all at once. Detecting such
inconsistency early is crucial: it saves wasted effort trying to solve
an impossible system and reveals important geometric and algebraic
properties.

\subsubsection{What Inconsistency Looks Like
Algebraically}\label{what-inconsistency-looks-like-algebraically}

Consider the system:

\[
\begin{cases}  
x + y = 1 \\  
x + y = 3  
\end{cases}
\]

Clearly, the two equations cannot both be true. In augmented matrix
form:

\[
\begin{bmatrix}  
1 & 1 & | & 1 \\  
1 & 1 & | & 3  
\end{bmatrix}.
\]

Row reduction gives:

\[
\begin{bmatrix}  
1 & 1 & | & 1 \\  
0 & 0 & | & 2  
\end{bmatrix}.
\]

The bottom row says \(0 = 2\), a contradiction. This is the hallmark of
inconsistency: a row of zeros in the coefficient part, with a nonzero
constant in the augmented part.

\subsubsection{General Rule for
Detection}\label{general-rule-for-detection}

A system \(A\mathbf{x} = \mathbf{b}\) is inconsistent if, after row
reduction, the augmented matrix contains a row of the form:

\[
[0 \;\; 0 \;\; \dots \;\; 0 \;|\; c], \quad c \neq 0.
\]

This indicates that all variables vanish from the equation, leaving an
impossible statement like \(0 = c\).

\subsubsection{Example 1: Parallel Lines in
2D}\label{example-1-parallel-lines-in-2d}

\[
\begin{cases}  
x + y = 2 \\  
2x + 2y = 5  
\end{cases}
\]

Augmented matrix:

\[
\begin{bmatrix}  
1 & 1 & | & 2 \\  
2 & 2 & | & 5  
\end{bmatrix}.
\]

Row reduce:

\begin{itemize}
\tightlist
\item
  \(R_2 \to R_2 - 2R_1\):
\end{itemize}

\[
\begin{bmatrix}  
1 & 1 & | & 2 \\  
0 & 0 & | & 1  
\end{bmatrix}.
\]

Contradiction: no solution. Geometrically, the two equations are
parallel lines that never intersect.

\subsubsection{Example 2: Contradictory Planes in
3D}\label{example-2-contradictory-planes-in-3d}

\[
\begin{cases}  
x + y + z = 1 \\  
2x + 2y + 2z = 2 \\  
x + y + z = 3  
\end{cases}
\]

The first and third equations already conflict: the same plane equation
is forced to equal two different constants.

Augmented matrix reduces to:

\[
\begin{bmatrix}  
1 & 1 & 1 & | & 1 \\  
0 & 0 & 0 & | & 0 \\  
0 & 0 & 0 & | & 2  
\end{bmatrix}.
\]

Contradiction: no solution. The ``planes'' fail to intersect in common.

\subsubsection{Geometry of
Inconsistency}\label{geometry-of-inconsistency}

\begin{itemize}
\tightlist
\item
  In 2D: Inconsistent systems correspond to parallel lines with
  different intercepts.
\item
  In 3D: They correspond to planes that are parallel but offset, or
  planes arranged in a way that leaves a ``gap'' (no shared
  intersection).
\item
  In higher dimensions: Inconsistency means the target vector
  \(\mathbf{b}\) lies outside the column space of \(A\).
\end{itemize}

\subsubsection{Rank Test for
Consistency}\label{rank-test-for-consistency}

Another way to detect inconsistency is using ranks.

\begin{itemize}
\tightlist
\item
  Let \(\text{rank}(A)\) be the number of pivots in the coefficient
  matrix.
\item
  Let \(\text{rank}([A|\mathbf{b}])\) be the number of pivots in the
  augmented matrix.
\end{itemize}

Rule:

\begin{itemize}
\tightlist
\item
  If \(\text{rank}(A) = \text{rank}([A|\mathbf{b}])\), the system is
  consistent.
\item
  If \(\text{rank}(A) < \text{rank}([A|\mathbf{b}])\), the system is
  inconsistent.
\end{itemize}

This rank condition is fundamental and works in any dimension.

\subsubsection{Everyday Analogies}\label{everyday-analogies-22}

\begin{itemize}
\tightlist
\item
  Meeting schedules: If one friend says ``meet at 2pm'' and another says
  ``meet at 2pm and 3pm simultaneously,'' the instructions contradict-no
  meeting possible.
\item
  Construction blueprints: If two architects specify identical walls but
  with different lengths, the building plan is impossible.
\item
  Cooking instructions: If one step requires ``boil pasta until firm''
  and another says ``the same pasta must already be raw,'' the recipe
  cannot be carried out.
\end{itemize}

\subsubsection{Why It Matters}\label{why-it-matters-22}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Inconsistency reveals overdetermined or contradictory data in real
  problems (physics, engineering, statistics).
\item
  The ability to detect inconsistency quickly through row reduction or
  rank saves computation.
\item
  It connects geometry (non-intersecting spaces) with algebra
  (contradictory rows).
\item
  It prepares the way for least-squares methods, where inconsistent
  systems are approximated instead of solved exactly.
\end{enumerate}

\subsubsection{Try It Yourself}\label{try-it-yourself-25}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Reduce the augmented matrix
\end{enumerate}

\[
\begin{bmatrix}  
1 & -1 & | & 2 \\  
2 & -2 & | & 5  
\end{bmatrix}
\]

and decide if the system is consistent.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Show geometrically why the system
\end{enumerate}

\[
x + y = 0, \quad x + y = 1
\]

is inconsistent.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Use the rank test to check consistency of
\end{enumerate}

\[
\begin{cases}  
x + y + z = 2 \\  
2x + 2y + 2z = 4 \\  
3x + 3y + 3z = 5  
\end{cases}
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Challenge: Explain why
  \(\text{rank}(A) < \text{rank}([A|\mathbf{b}])\) implies
  inconsistency, using the concept of the column space.
\end{enumerate}

Detecting inconsistency is not just about spotting contradictions-it
connects algebra, geometry, and linear transformations, showing exactly
when a system cannot possibly fit together.

\subsection{27. Gaussian Elimination by
Hand}\label{gaussian-elimination-by-hand}

Gaussian elimination is the systematic procedure for solving systems of
linear equations by using row operations to simplify the augmented
matrix. The goal is to transform the system into row-echelon form (REF)
and then use back substitution to find the solutions. This method is the
backbone of linear algebra computations and is the foundation of most
computer algorithms for solving linear systems.

\subsubsection{The Big Idea}\label{the-big-idea}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Represent the system as an augmented matrix.
\item
  Use row operations to eliminate variables step by step, moving left to
  right, top to bottom.
\item
  Stop when the matrix is in REF.
\item
  Solve the triangular system by back substitution.
\end{enumerate}

\subsubsection{Step-by-Step Recipe}\label{step-by-step-recipe}

Suppose we have \(n\) equations with \(n\) unknowns.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Choose a pivot in the first column (a nonzero entry). If needed, swap
  rows to bring a nonzero entry to the top.
\item
  Eliminate below the pivot by subtracting multiples of the pivot row
  from lower rows so that all entries below the pivot become zero.
\item
  Move to the next row and next column, pick the next pivot, and repeat
  elimination.
\item
  Continue until all pivots are in stair-step form (REF).
\item
  Use back substitution to solve for the unknowns starting from the
  bottom row.
\end{enumerate}

\subsubsection{Example 1: A 2×2 System}\label{example-1-a-22-system}

System:

\[
\begin{cases}  
x + 2y = 5 \\  
3x + 4y = 11  
\end{cases}
\]

Augmented matrix:

\[
\begin{bmatrix}  
1 & 2 & | & 5 \\  
3 & 4 & | & 11  
\end{bmatrix}.
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Pivot at (1,1) = 1.
\item
  Eliminate below: \(R_2 \to R_2 - 3R_1\).

  \[
  \begin{bmatrix}  
  1 & 2 & | & 5 \\  
  0 & -2 & | & -4  
  \end{bmatrix}.
  \]
\item
  Back substitution: From row 2: \(-2y = -4 \implies y = 2\). Substitute
  into row 1: \(x + 2(2) = 5 \implies x = 1\).
\end{enumerate}

Solution: \((x, y) = (1, 2)\).

\subsubsection{Example 2: A 3×3 System}\label{example-2-a-33-system}

System:

\[
\begin{cases}  
x + y + z = 6 \\  
2x + 3y + z = 14 \\  
x - y + 2z = 2  
\end{cases}
\]

Augmented matrix:

\[
\begin{bmatrix}  
1 & 1 & 1 & | & 6 \\  
2 & 3 & 1 & | & 14 \\  
1 & -1 & 2 & | & 2  
\end{bmatrix}.
\]

Step 1: Pivot at (1,1). Eliminate below:

\begin{itemize}
\tightlist
\item
  \(R_2 \to R_2 - 2R_1\).
\item
  \(R_3 \to R_3 - R_1\).
\end{itemize}

\[
\begin{bmatrix}  
1 & 1 & 1 & | & 6 \\  
0 & 1 & -1 & | & 2 \\  
0 & -2 & 1 & | & -4  
\end{bmatrix}.
\]

Step 2: Pivot at (2,2). Eliminate below: \(R_3 \to R_3 + 2R_2\).

\[
\begin{bmatrix}  
1 & 1 & 1 & | & 6 \\  
0 & 1 & -1 & | & 2 \\  
0 & 0 & -1 & | & 0  
\end{bmatrix}.
\]

Step 3: Pivot at (3,3). Scale row: \(R_3 \to -R_3\).

\[
\begin{bmatrix}  
1 & 1 & 1 & | & 6 \\  
0 & 1 & -1 & | & 2 \\  
0 & 0 & 1 & | & 0  
\end{bmatrix}.
\]

Back substitution:

\begin{itemize}
\tightlist
\item
  From row 3: \(z = 0\).
\item
  From row 2: \(y - z = 2 \implies y = 2\).
\item
  From row 1: \(x + y + z = 6 \implies x = 4\).
\end{itemize}

Solution: \((x, y, z) = (4, 2, 0)\).

\subsubsection{Why Gaussian Elimination Always
Works}\label{why-gaussian-elimination-always-works}

\begin{itemize}
\tightlist
\item
  Each step reduces the number of variables in the lower equations.
\item
  Pivoting ensures stability (swap rows to avoid dividing by zero).
\item
  The algorithm either produces a triangular system (solvable by
  substitution) or reveals inconsistency (contradictory row).
\end{itemize}

\subsubsection{Geometric
Interpretation}\label{geometric-interpretation-1}

\begin{itemize}
\item
  Elimination corresponds to progressively restricting the solution set:

  \begin{itemize}
  \tightlist
  \item
    First equation → a plane in \(\mathbb{R}^3\).
  \item
    Add second equation → intersection becomes a line.
  \item
    Add third equation → intersection becomes a point (unique solution)
    or vanishes (inconsistent).
  \end{itemize}
\end{itemize}

\subsubsection{Everyday Analogies}\label{everyday-analogies-23}

\begin{itemize}
\tightlist
\item
  Cleaning a desk: Start with a mess (equations mixed together), then
  eliminate clutter one step at a time until everything is sorted neatly
  into piles.
\item
  Cooking: Elimination is like reducing a sauce-boiling away the excess
  until only the essence remains.
\item
  Detective work: Each equation eliminates possibilities, narrowing down
  suspects until only the guilty party (the solution) remains.
\end{itemize}

\subsubsection{Why It Matters}\label{why-it-matters-23}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Gaussian elimination is the foundation for solving systems by hand and
  by computer.
\item
  It reveals whether a system is consistent and if solutions are unique
  or infinite.
\item
  It is the starting point for advanced methods like LU decomposition,
  QR factorization, and numerical solvers.
\item
  It shows the interplay between algebra (row operations) and geometry
  (intersections of subspaces).
\end{enumerate}

\subsubsection{Try It Yourself}\label{try-it-yourself-26}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Solve the system

  \[
  \begin{cases}  
  2x + y = 7 \\  
  4x + 3y = 15  
  \end{cases}
  \]

  using Gaussian elimination.
\item
  Reduce

  \[
  \begin{bmatrix}  
  1 & 2 & -1 & | & 3 \\  
  3 & 8 & 1 & | & 12 \\  
  2 & 6 & 3 & | & 11  
  \end{bmatrix}
  \]

  to REF and solve.
\item
  Practice with a system that has infinitely many solutions:

  \[
  x + y + z = 4, \quad 2x + 2y + 2z = 8.
  \]
\item
  Challenge: Explain why Gaussian elimination always terminates in at
  most \(n\) pivot steps for an \(n \times n\) system.
\end{enumerate}

Gaussian elimination transforms the complexity of many equations into an
orderly process, making the hidden structure of solutions visible step
by step.

\subsection{28. Back Substitution and Solution
Sets}\label{back-substitution-and-solution-sets}

Once Gaussian elimination reduces a system to row-echelon form (REF),
the next step is to actually solve for the unknowns. This process is
called back substitution: we begin with the bottom equation (which
involves the fewest variables) and work our way upward, solving step by
step. Back substitution is what converts the structured triangular
system into explicit solutions.

\subsubsection{The Structure of Row-Echelon
Form}\label{the-structure-of-row-echelon-form}

A system in REF looks like this:

\[
\begin{bmatrix}  
- & * & * & * & | & * \\  
0 & * & * & * & | & * \\  
0 & 0 & * & * & | & * \\  
0 & 0 & 0 & * & | & *  
\end{bmatrix}
\]

\begin{itemize}
\tightlist
\item
  Each row corresponds to an equation with fewer variables than the row
  above.
\item
  The bottom equation has only one or two variables.
\item
  This triangular form makes it possible to solve ``from the bottom
  up.''
\end{itemize}

\subsubsection{Step-by-Step Example: Unique
Solution}\label{step-by-step-example-unique-solution}

System after elimination:

\[
\begin{bmatrix}  
1 & 2 & -1 & | & 3 \\  
0 & 1 & 2 & | & 4 \\  
0 & 0 & 1 & | & 2  
\end{bmatrix}.
\]

This corresponds to:

\[
\begin{cases}  
x + 2y - z = 3 \\  
y + 2z = 4 \\  
z = 2  
\end{cases}
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  From the last equation: \(z = 2\).
\item
  Substitute into the second: \(y + 2(2) = 4 \implies y = 0\).
\item
  Substitute into the first: \(x + 2(0) - 2 = 3 \implies x = 5\).
\end{enumerate}

Solution: \((x, y, z) = (5, 0, 2)\).

\subsubsection{Infinite Solutions with Free
Variables}\label{infinite-solutions-with-free-variables}

Not all systems reduce to unique solutions. If there are free variables
(non-pivot columns), back substitution expresses pivot variables in
terms of free ones.

Example:

\[
\begin{bmatrix}  
1 & 2 & 1 & | & 4 \\  
0 & 1 & -1 & | & 1 \\  
0 & 0 & 0 & | & 0  
\end{bmatrix}.
\]

Equations:

\[
\begin{cases}  
x + 2y + z = 4 \\  
y - z = 1  
\end{cases}
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  From row 2: \(y = 1 + z\).
\item
  From row 1: \(x + 2(1 + z) + z = 4 \implies x = 2 - 3z\).
\end{enumerate}

Solution set:

\[
(x, y, z) = (2 - 3t, \; 1 + t, \; t), \quad t \in \mathbb{R}.
\]

Here \(z = t\) is the free variable. The solutions form a line in 3D.

\subsubsection{General Solution
Structure}\label{general-solution-structure}

For a consistent system:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Unique solution → every variable is a pivot variable (no free
  variables).
\item
  Infinitely many solutions → some free variables remain. The solution
  set is parametrized by these variables and forms a line, plane, or
  higher-dimensional subspace.
\item
  No solution → contradiction discovered earlier, so back substitution
  is impossible.
\end{enumerate}

\subsubsection{Geometric Meaning}\label{geometric-meaning-2}

\begin{itemize}
\tightlist
\item
  Unique solution → a single intersection point of lines/planes.
\item
  Infinite solutions → overlapping subspaces (e.g., two planes
  intersecting in a line).
\item
  Back substitution describes the exact shape of this intersection.
\end{itemize}

\subsubsection{Example: Parametric Vector
Form}\label{example-parametric-vector-form}

For the infinite-solution example above:

\[
(x, y, z) = (2, 1, 0) + t(-3, 1, 1).
\]

This expresses the solution set as a base point plus a direction vector,
making the geometry clear.

\subsubsection{Everyday Analogies}\label{everyday-analogies-24}

\begin{itemize}
\tightlist
\item
  Stacked tasks: Back substitution is like solving a sequence of tasks
  where each depends on the result of the one below it.
\item
  Domino effect: Once the last domino falls (the last variable solved),
  the rest fall in sequence.
\item
  Filling out a form: Each field depends on earlier answers; once the
  final field is fixed, the others can be determined in order.
\end{itemize}

\subsubsection{Why It Matters}\label{why-it-matters-24}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Back substitution turns row-echelon form into concrete answers.
\item
  It distinguishes unique vs.~infinite solutions.
\item
  It provides a systematic method usable by hand for small systems and
  forms the basis of computer algorithms for large ones.
\item
  It reveals the structure of solution sets-whether a point, line,
  plane, or higher-dimensional object.
\end{enumerate}

\subsubsection{Try It Yourself}\label{try-it-yourself-27}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Solve by back substitution:
\end{enumerate}

\[
\begin{bmatrix}  
1 & -1 & 2 & | & 3 \\  
0 & 1 & 3 & | & 5 \\  
0 & 0 & 1 & | & 2  
\end{bmatrix}.
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Reduce and solve:
\end{enumerate}

\[
x + y + z = 2, \quad 2x + 2y + 2z = 4.
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\item
  Express the solution set of the above system in parametric vector
  form.
\item
  Challenge: For a 4×4 system with two free variables, explain why the
  solution set forms a plane in \(\mathbb{R}^4\).
\end{enumerate}

Back substitution completes the elimination process, translating
triangular structure into explicit solutions, and shows how algebra and
geometry meet in the classification of solution sets.

\subsection{29. Rank and Its First
Meaning}\label{rank-and-its-first-meaning}

The concept of rank lies at the heart of linear algebra. It connects the
algebra of solving systems, the geometry of subspaces, and the structure
of matrices into one unifying idea. Rank measures the amount of
independent information in a matrix: how many rows or columns carry
unique directions instead of being repetitions or combinations of
others.

\subsubsection{Definition of Rank}\label{definition-of-rank}

The rank of a matrix \(A\) is the number of pivots in its row-echelon
form. Equivalently, it is:

\begin{itemize}
\tightlist
\item
  The dimension of the column space (number of independent columns).
\item
  The dimension of the row space (number of independent rows).
\end{itemize}

All these definitions agree.

\subsubsection{First Encounter with Rank: Pivot
Counting}\label{first-encounter-with-rank-pivot-counting}

When solving a system with Gaussian elimination:

\begin{itemize}
\tightlist
\item
  Every pivot corresponds to one determined variable.
\item
  The number of pivots = the rank.
\item
  The number of free variables = total variables -- rank.
\end{itemize}

Example:

\[
\begin{bmatrix}  
1 & 2 & 1 & | & 4 \\  
0 & 1 & -1 & | & 2 \\  
0 & 0 & 0 & | & 0  
\end{bmatrix}.
\]

Here, there are 2 pivots. So:

\begin{itemize}
\tightlist
\item
  Rank = 2.
\item
  With 3 variables total, there is 1 free variable.
\end{itemize}

\subsubsection{Rank in Terms of
Independence}\label{rank-in-terms-of-independence}

A set of vectors is linearly independent if none can be expressed as a
combination of the others.

\begin{itemize}
\tightlist
\item
  The rank of a matrix tells us how many independent rows or columns it
  has.
\item
  If some columns are combinations of others, they do not increase the
  rank.
\end{itemize}

Example:

\[
\begin{bmatrix}  
1 & 2 & 3 \\  
2 & 4 & 6 \\  
3 & 6 & 9  
\end{bmatrix}.
\]

Here, each row is a multiple of the first. Rank = 1, since only one
independent row/column direction exists.

\subsubsection{Rank and Solutions of
Systems}\label{rank-and-solutions-of-systems}

Consider \(A\mathbf{x} = \mathbf{b}\).

\begin{itemize}
\tightlist
\item
  If \(\text{rank}(A) = \text{rank}([A|\mathbf{b}])\), the system is
  consistent.
\item
  If not, inconsistent.
\item
  If rank = number of variables, the system has a unique solution.
\item
  If rank \textless{} number of variables, there are infinitely many
  solutions.
\end{itemize}

Thus, rank classifies solution sets.

\subsubsection{Rank and Geometry}\label{rank-and-geometry}

Rank tells us the dimension of the subspace spanned by rows or columns.

\begin{itemize}
\tightlist
\item
  Rank 1: all information lies along a line.
\item
  Rank 2: lies in a plane.
\item
  Rank 3: fills 3D space.
\end{itemize}

Example:

\begin{itemize}
\tightlist
\item
  In \(\mathbb{R}^3\), a matrix of rank 2 has columns spanning a plane
  through the origin.
\item
  A matrix of rank 1 has all columns on a single line.
\end{itemize}

\subsubsection{Rank and Row vs.~Column
View}\label{rank-and-row-vs.-column-view}

It is a remarkable fact that the number of independent rows = number of
independent columns. This is not obvious at first glance, but it is
always true. So we can define rank either by rows or by columns-it makes
no difference.

\subsubsection{Everyday Analogies}\label{everyday-analogies-25}

\begin{itemize}
\tightlist
\item
  Library books: If three books say the same thing in different words,
  only one adds new knowledge. The others are redundant. Rank = number
  of genuinely independent books.
\item
  Recipes: If you have five recipes but four are just scaled versions of
  the first, you really only have one distinct recipe. Rank = 1.
\item
  Work team: If every team member repeats what another says, the
  effective number of independent voices is low. Rank measures how many
  unique contributions exist.
\end{itemize}

\subsubsection{Why It Matters}\label{why-it-matters-25}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Rank is the bridge between algebra and geometry: pivots ↔ dimension.
\item
  It classifies solutions to systems of equations.
\item
  It measures redundancy in data (important in statistics, machine
  learning, signal processing).
\item
  It prepares the way for advanced concepts like nullity, rank--nullity
  theorem, and singular value decomposition.
\end{enumerate}

\subsubsection{Try It Yourself}\label{try-it-yourself-28}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Find the rank of

  \[
  \begin{bmatrix}  
  1 & 2 & 3 \\  
  2 & 4 & 5 \\  
  3 & 6 & 8  
  \end{bmatrix}.
  \]
\item
  Solve the system

  \[
  x + y + z = 2, \quad 2x + 2y + 2z = 4,
  \]

  and identify the rank of the coefficient matrix.
\item
  In \(\mathbb{R}^3\), what is the geometric meaning of a 3×3 matrix of
  rank 2?
\item
  Challenge: Prove that the row rank always equals the column rank by
  considering the echelon form of the matrix.
\end{enumerate}

Rank is the first truly unifying concept in linear algebra: it tells us
how much independent structure a matrix contains and sets the stage for
understanding spaces, dimensions, and transformations.

\subsection{30. LU Factorization}\label{lu-factorization}

Gaussian elimination not only solves systems but also reveals a deeper
structure: many matrices can be factored into simpler pieces. One of the
most useful is the LU factorization, where a matrix \(A\) is written as
the product of a lower-triangular matrix \(L\) and an upper-triangular
matrix \(U\). This factorization captures all the elimination steps in a
compact form and allows systems to be solved efficiently.

\subsubsection{What is LU
Factorization?}\label{what-is-lu-factorization}

If \(A\) is an \(n \times n\) matrix, then

\[
A = LU,
\]

where:

\begin{itemize}
\tightlist
\item
  \(L\) is lower-triangular (entries below diagonal may be nonzero,
  diagonal entries = 1).
\item
  \(U\) is upper-triangular (entries above diagonal may be nonzero).
\end{itemize}

This means:

\begin{itemize}
\tightlist
\item
  \(U\) stores the result of elimination (the triangular system).
\item
  \(L\) records the multipliers used during elimination.
\end{itemize}

\subsubsection{Example: 2×2 Case}\label{example-22-case}

Take

\[
A = \begin{bmatrix}  
2 & 3 \\  
4 & 7  
\end{bmatrix}.
\]

Elimination: \(R_2 \to R_2 - 2R_1\).

\begin{itemize}
\item
  Multiplier = 2 (used to eliminate entry 4).
\item
  Resulting \(U\):

  \[
  U = \begin{bmatrix}  
  2 & 3 \\  
  0 & 1  
  \end{bmatrix}.
  \]
\item
  \(L\):

  \[
  L = \begin{bmatrix}  
  1 & 0 \\  
  2 & 1  
  \end{bmatrix}.
  \]
\end{itemize}

Check:

\[
LU = \begin{bmatrix}  
1 & 0 \\  
2 & 1  
\end{bmatrix}  
\begin{bmatrix}  
2 & 3 \\  
0 & 1  
\end{bmatrix}  
= \begin{bmatrix}  
2 & 3 \\  
4 & 7  
\end{bmatrix} = A.
\]

\subsubsection{Example: 3×3 Case}\label{example-33-case}

\[
A = \begin{bmatrix}  
2 & 1 & 1 \\  
4 & -6 & 0 \\  
-2 & 7 & 2  
\end{bmatrix}.
\]

Step 1: Eliminate below pivot (row 1).

\begin{itemize}
\tightlist
\item
  Multiplier \(m_{21} = 4/2 = 2\).
\item
  Multiplier \(m_{31} = -2/2 = -1\).
\end{itemize}

Step 2: Eliminate below pivot in column 2.

\begin{itemize}
\tightlist
\item
  After substitutions, multipliers and pivots are collected.
\end{itemize}

Result:

\[
L = \begin{bmatrix}  
1 & 0 & 0 \\  
2 & 1 & 0 \\  
-1 & -1 & 1  
\end{bmatrix}, \quad  
U = \begin{bmatrix}  
2 & 1 & 1 \\  
0 & -8 & -2 \\  
0 & 0 & 1  
\end{bmatrix}.
\]

Thus \(A = LU\).

\subsubsection{Solving Systems with LU}\label{solving-systems-with-lu}

Suppose \(Ax = b\). If \(A = LU\):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Solve \(Ly = b\) by forward substitution (since \(L\) is
  lower-triangular).
\item
  Solve \(Ux = y\) by back substitution (since \(U\) is
  upper-triangular).
\end{enumerate}

This two-step process is much faster than elimination from scratch each
time, especially if solving multiple systems with the same \(A\) but
different \(b\).

\subsubsection{Pivoting and
Permutations}\label{pivoting-and-permutations}

Sometimes elimination requires row swaps (to avoid division by zero or
instability). Then factorization is written as:

\[
PA = LU,
\]

where \(P\) is a permutation matrix recording the row swaps. This is the
practical form used in numerical computing.

\subsubsection{Applications of LU
Factorization}\label{applications-of-lu-factorization}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Efficient solving: Multiple right-hand sides \(Ax = b\). Compute
  \(LU\) once, reuse for each \(b\).
\item
  Determinants: \(\det(A) = \det(L)\det(U)\). Since diagonals of \(L\)
  are 1, this reduces to the product of the diagonal of \(U\).
\item
  Matrix inverse: By solving \(Ax = e_i\) for each column \(e_i\), we
  can compute \(A^{-1}\) efficiently with LU.
\item
  Numerical methods: LU is central in scientific computing, engineering
  simulations, and optimization.
\end{enumerate}

\subsubsection{Geometric Meaning}\label{geometric-meaning-3}

LU decomposition separates the elimination process into:

\begin{itemize}
\tightlist
\item
  \(L\): shear transformations (adding multiples of rows).
\item
  \(U\): scaling and alignment into triangular form.
\end{itemize}

Together, they represent the same linear transformation as \(A\), but
decomposed into simpler building blocks.

\subsubsection{Everyday Analogies}\label{everyday-analogies-26}

\begin{itemize}
\tightlist
\item
  Recipe preparation: \(L\) is the prep work (chopping, mixing), while
  \(U\) is the final cooking (arranging in order). Together, they
  produce the finished dish.
\item
  Assembly line: \(L\) records the sequence of adjustments, \(U\) the
  final structured product.
\item
  Team project: \(L\) shows who contributed what during the work
  process, \(U\) is the neatly organized final report.
\end{itemize}

\subsubsection{Why It Matters}\label{why-it-matters-26}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  LU factorization compresses elimination into a reusable format.
\item
  It is a cornerstone of numerical linear algebra and used in almost
  every solver.
\item
  It links computation (efficient algorithms) with theory (factorization
  of transformations).
\item
  It introduces the broader idea that matrices can be broken into
  simple, interpretable parts.
\end{enumerate}

\subsubsection{Try It Yourself}\label{try-it-yourself-29}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Factor

  \[
  A = \begin{bmatrix}  
  1 & 2 \\  
  3 & 8  
  \end{bmatrix}
  \]

  into \(LU\).
\item
  Solve

  \[
  \begin{bmatrix}  
  2 & 1 \\  
  6 & 3  
  \end{bmatrix}  
  \begin{bmatrix}  
  x \\ y  
  \end{bmatrix} =  
  \begin{bmatrix}  
  5 \\ 15  
  \end{bmatrix}
  \]

  using LU decomposition.
\item
  Compute \(\det(A)\) for

  \[
  A = \begin{bmatrix}  
  2 & 1 & 1 \\  
  4 & -6 & 0 \\  
  -2 & 7 & 2  
  \end{bmatrix}
  \]

  by using its LU factorization.
\item
  Challenge: Prove that if \(A\) is invertible, then it has an LU
  factorization (possibly after row swaps).
\end{enumerate}

LU factorization organizes elimination into a powerful tool: compact,
efficient, and deeply tied to both the theory and practice of linear
algebra.

\subsubsection{Closing}\label{closing-2}

\begin{verbatim}
Paths diverge or merge,
pivots mark the way forward,
truth distilled in rows.
\end{verbatim}

\section{Chapter 4. Vector spaces and
subspaces}\label{chapter-4.-vector-spaces-and-subspaces-1}

\subsubsection{Opening}\label{opening-2}

\begin{verbatim}
Endless skies expand,
spaces within spaces grow,
freedom takes its shape.
\end{verbatim}

\subsection{31. Axioms of Vector Spaces}\label{axioms-of-vector-spaces}

Up to now, we have worked with vectors in \(\mathbb{R}^2\),
\(\mathbb{R}^3\), and higher-dimensional Euclidean spaces. But the true
power of linear algebra comes from abstracting away from coordinates. A
vector space is not tied to arrows in physical space-it is any
collection of objects that behave like vectors, provided they satisfy
certain rules. These rules are called the axioms of vector spaces.

\subsubsection{The Idea of a Vector
Space}\label{the-idea-of-a-vector-space}

A vector space is a set \(V\) equipped with two operations:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Vector addition: Combine two vectors in \(V\) to get another vector in
  \(V\).
\item
  Scalar multiplication: Multiply a vector in \(V\) by a scalar (a
  number from a field, usually \(\mathbb{R}\) or \(\mathbb{C}\)).
\end{enumerate}

The magic is that as long as certain rules (axioms) hold, the objects in
\(V\) can be treated as vectors. They need not be arrows or coordinate
lists-they could be polynomials, functions, matrices, or sequences.

\subsubsection{The Eight Axioms}\label{the-eight-axioms}

Let \(u, v, w \in V\) (vectors) and \(a, b \in \mathbb{R}\) (scalars).
The axioms are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Closure under addition: \(u + v \in V\).
\item
  Commutativity of addition: \(u + v = v + u\).
\item
  Associativity of addition: \((u + v) + w = u + (v + w)\).
\item
  Existence of additive identity: There exists a zero vector \(0 \in V\)
  such that \(v + 0 = v\).
\item
  Existence of additive inverses: For every \(v\), there is \(-v\) such
  that \(v + (-v) = 0\).
\item
  Closure under scalar multiplication: \(a v \in V\).
\item
  Distributivity of scalar multiplication over vector addition:
  \(a(u + v) = au + av\).
\item
  Distributivity of scalar multiplication over scalar addition:
  \((a + b)v = av + bv\).
\item
  Associativity of scalar multiplication: \(a(bv) = (ab)v\).
\item
  Existence of multiplicative identity: \(1 \cdot v = v\).
\end{enumerate}

(These are sometimes listed as eight, with some grouped together, but
the essence is the same.)

\subsubsection{Examples of Vector
Spaces}\label{examples-of-vector-spaces}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Euclidean spaces: \(\mathbb{R}^n\) with standard addition and scalar
  multiplication.
\item
  Polynomials: The set of all polynomials with real coefficients,
  \(\mathbb{R}[x]\).
\item
  Functions: The set of all continuous functions on \([0,1]\), with
  addition of functions and scalar multiplication.
\item
  Matrices: The set of all \(m \times n\) matrices with real entries.
\item
  Sequences: The set of all infinite real sequences
  \((a_1, a_2, \dots)\).
\end{enumerate}

All of these satisfy the vector space axioms.

\subsubsection{Non-Examples}\label{non-examples}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The set of natural numbers \(\mathbb{N}\) is not a vector space (no
  additive inverses).
\item
  The set of positive real numbers \(\mathbb{R}^+\) is not a vector
  space (not closed under scalar multiplication with negative numbers).
\item
  The set of polynomials of degree exactly 2 is not a vector space (not
  closed under addition: \(x^2 + x^2 = 2x^2\) is still degree 2, but
  \(x^2 - x^2 = 0\), which is degree 0, not allowed).
\end{enumerate}

These examples show why the axioms are essential: without them, the
structure breaks.

\subsubsection{The Zero Vector}\label{the-zero-vector}

Every vector space must contain a zero vector. This is not optional. It
is the ``do nothing'' element for addition. In \(\mathbb{R}^n\), this is
\((0,0,\dots,0)\). In polynomials, it is the zero polynomial. In
function spaces, it is the function \(f(x) = 0\).

\subsubsection{Additive Inverses}\label{additive-inverses}

For every vector \(v\), we require \(-v\). This ensures that equations
like \(u+v=w\) can always be rearranged to \(u=w-v\). Without additive
inverses, solving linear equations would not work.

\subsubsection{Scalars and Fields}\label{scalars-and-fields}

Scalars come from a field: usually the real numbers \(\mathbb{R}\) or
the complex numbers \(\mathbb{C}\). The choice of scalars matters:

\begin{itemize}
\tightlist
\item
  Over \(\mathbb{R}\), a polynomial space is different from over
  \(\mathbb{C}\).
\item
  Over finite fields (like integers modulo \(p\)), vector spaces exist
  in discrete mathematics and coding theory.
\end{itemize}

\subsubsection{Geometric
Interpretation}\label{geometric-interpretation-2}

\begin{itemize}
\tightlist
\item
  The axioms guarantee that vectors can be added and scaled in
  predictable ways.
\item
  Closure ensures the space is ``self-contained.''
\item
  Additive inverses ensure symmetry: every direction can be reversed.
\item
  Distributivity ensures consistency between scaling and addition.
\end{itemize}

Together, these rules make vector spaces stable and reliable
mathematical objects.

\subsubsection{Everyday Analogies}\label{everyday-analogies-27}

\begin{itemize}
\tightlist
\item
  Language: Words form sentences by rules of grammar. Vector spaces are
  sets where addition and scaling follow strict grammar-like rules.
\item
  Music: Notes combine (addition) and change pitch (scaling). A musical
  space is meaningful only if these operations stay within the system.
\item
  Construction: Bricks and mortar can be combined (addition) and scaled
  (larger or smaller structures), but only when rules of stability are
  followed.
\end{itemize}

\subsubsection{Why It Matters}\label{why-it-matters-27}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Vector spaces unify many areas of math under a single framework.
\item
  They generalize \(\mathbb{R}^n\) to functions, polynomials, and
  beyond.
\item
  The axioms guarantee that all the tools of linear algebra-span, basis,
  dimension, linear maps-apply.
\item
  Recognizing vector spaces in disguise is a major step in advanced math
  and physics.
\end{enumerate}

\subsubsection{Try It Yourself}\label{try-it-yourself-30}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Verify that the set of all 2×2 matrices is a vector space under matrix
  addition and scalar multiplication.
\item
  Show that the set of polynomials of degree at most 3 is a vector
  space, but the set of polynomials of degree exactly 3 is not.
\item
  Check whether the set of all even functions \(f(-x) = f(x)\) is a
  vector space.
\item
  Challenge: Consider the set of all differentiable functions \(f\) on
  \([0,1]\). Show that this set forms a vector space under the usual
  operations.
\end{enumerate}

The axioms of vector spaces provide the foundation on which the rest of
linear algebra is built. Everything that follows-subspaces,
independence, basis, dimension-grows naturally from this formal
framework.

\subsection{32. Subspaces, Column Space, and Null
Space}\label{subspaces-column-space-and-null-space}

Once the idea of a vector space is in place, the next step is to
recognize smaller vector spaces that live inside bigger ones. These are
called subspaces. Subspaces are central in linear algebra because they
reveal the internal structure of matrices and linear systems. Two
special subspaces-the column space and the null space-play particularly
important roles.

\subsubsection{What Is a Subspace?}\label{what-is-a-subspace}

A subspace \(W\) of a vector space \(V\) is a subset of \(V\) that is
itself a vector space under the same operations. To qualify as a
subspace, \(W\) must satisfy:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The zero vector \(0\) is in \(W\).
\item
  If \(u, v \in W\), then \(u+v \in W\) (closed under addition).
\item
  If \(u \in W\) and \(c\) is a scalar, then \(cu \in W\) (closed under
  scalar multiplication).
\end{enumerate}

That's it-no further checking of all ten vector space axioms is needed,
because those are inherited from \(V\).

\subsubsection{Simple Examples of
Subspaces}\label{simple-examples-of-subspaces}

\begin{itemize}
\item
  In \(\mathbb{R}^3\):

  \begin{itemize}
  \tightlist
  \item
    A line through the origin is a 1-dimensional subspace.
  \item
    A plane through the origin is a 2-dimensional subspace.
  \item
    The whole space itself is a subspace.
  \item
    The trivial subspace \(\{0\}\) contains only the zero vector.
  \end{itemize}
\item
  In the space of polynomials:

  \begin{itemize}
  \tightlist
  \item
    All polynomials of degree ≤ 3 form a subspace.
  \item
    All polynomials with zero constant term form a subspace.
  \end{itemize}
\item
  In function spaces:

  \begin{itemize}
  \tightlist
  \item
    All continuous functions on \([0,1]\) form a subspace of all
    functions on \([0,1]\).
  \item
    All solutions to a linear differential equation form a subspace.
  \end{itemize}
\end{itemize}

\subsubsection{The Column Space of a
Matrix}\label{the-column-space-of-a-matrix}

Given a matrix \(A\), the column space is the set of all linear
combinations of its columns. Formally,

\[
C(A) = \{ A\mathbf{x} : \mathbf{x} \in \mathbb{R}^n \}.
\]

\begin{itemize}
\tightlist
\item
  The column space lives inside \(\mathbb{R}^m\) if \(A\) is
  \(m \times n\).
\item
  It represents all possible outputs of the linear transformation
  defined by \(A\).
\item
  Its dimension is equal to the rank of \(A\).
\end{itemize}

Example:

\[
A = \begin{bmatrix}  
1 & 2 \\  
2 & 4 \\  
3 & 6  
\end{bmatrix}.
\]

The second column is just twice the first. So the column space is all
multiples of \(\begin{bmatrix}1 \\ 2 \\ 3\end{bmatrix}\), which is a
line in \(\mathbb{R}^3\). Rank = 1.

\subsubsection{The Null Space of a
Matrix}\label{the-null-space-of-a-matrix}

The null space (or kernel) of a matrix \(A\) is the set of all vectors
\(\mathbf{x}\) such that

\[
A\mathbf{x} = 0.
\]

\begin{itemize}
\tightlist
\item
  It lives in \(\mathbb{R}^n\) if \(A\) is \(m \times n\).
\item
  It represents the ``invisible'' directions that collapse to zero under
  the transformation.
\item
  Its dimension is the nullity of \(A\).
\end{itemize}

Example:

\[
A = \begin{bmatrix}  
1 & 2 & 3 \\  
4 & 5 & 6  
\end{bmatrix}.
\]

Solve \(A\mathbf{x} = 0\). This yields a null space spanned by one
vector, meaning it is a line through the origin in \(\mathbb{R}^3\).

\subsubsection{Column Space vs.~Null
Space}\label{column-space-vs.-null-space}

\begin{itemize}
\tightlist
\item
  Column space: describes outputs (\(y\)-values that can be reached).
\item
  Null space: describes hidden inputs (directions that vanish).
\end{itemize}

Together, they capture the full behavior of a matrix.

\subsubsection{Geometric
Interpretation}\label{geometric-interpretation-3}

\begin{itemize}
\tightlist
\item
  In \(\mathbb{R}^3\), the column space could be a plane or a line
  inside 3D space.
\item
  The null space is orthogonal (in a precise sense) to the row space,
  which we'll study later.
\item
  Understanding both spaces gives a complete picture of how the matrix
  transforms vectors.
\end{itemize}

\subsubsection{Everyday Analogies}\label{everyday-analogies-28}

\begin{itemize}
\tightlist
\item
  Column space as achievements: Think of the matrix as a machine. The
  column space is the set of all things the machine can produce.
\item
  Null space as wasted effort: Any input vector in the null space
  produces nothing at all-like pressing buttons on a broken remote
  control.
\item
  Teamwork analogy: If each team member contributes along independent
  directions, the column space is large. If some repeat others' work,
  redundancy reduces the column space.
\end{itemize}

\subsubsection{Why It Matters}\label{why-it-matters-28}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Subspaces are the natural habitat of linear algebra: almost everything
  happens inside them.
\item
  The column space explains what systems \(Ax=b\) are solvable.
\item
  The null space explains why some systems have multiple solutions (free
  variables).
\item
  These ideas extend to advanced topics like eigenvectors, SVD, and
  differential equations.
\end{enumerate}

\subsubsection{Try It Yourself}\label{try-it-yourself-31}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Show that the set \(\{(x,y,0) : x,y \in \mathbb{R}\}\) is a subspace
  of \(\mathbb{R}^3\).
\item
  For

  \[
  A = \begin{bmatrix}  
  1 & 2 & 3 \\  
  0 & 0 & 0 \\  
  1 & 2 & 3  
  \end{bmatrix},
  \]

  find the column space and its dimension.
\item
  For the same \(A\), compute the null space and its dimension.
\item
  Challenge: Prove that the null space of \(A\) is always a subspace of
  \(\mathbb{R}^n\).
\end{enumerate}

Subspaces-especially the column space and null space-are the first
glimpse of the hidden geometry inside every matrix, showing us which
directions survive and which vanish.

\subsection{33. Span and Generating
Sets}\label{span-and-generating-sets}

The idea of a span captures the simplest and most powerful way to build
new vectors from old ones: by taking linear combinations. A span is not
just a set of scattered points but a structured, complete collection of
all combinations of a given set of vectors. Understanding span leads
directly to the concepts of bases, dimension, and the structure of
subspaces.

\subsubsection{Definition of Span}\label{definition-of-span}

Given vectors \(v_1, v_2, \dots, v_k \in V\), the span of these vectors
is

\[
\text{span}\{v_1, v_2, \dots, v_k\} = \{a_1 v_1 + a_2 v_2 + \dots + a_k v_k : a_i \in \mathbb{R}\}.
\]

\begin{itemize}
\tightlist
\item
  A span is the set of all possible linear combinations of the vectors.
\item
  It is always a subspace.
\item
  The given vectors are called a generating set.
\end{itemize}

\subsubsection{Simple Examples}\label{simple-examples}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  In \(\mathbb{R}^2\):

  \begin{itemize}
  \tightlist
  \item
    Span of \((1,0)\) = all multiples of the x-axis (a line).
  \item
    Span of \((1,0)\) and \((0,1)\) = the entire plane \(\mathbb{R}^2\).
  \item
    Span of \((1,0)\) and \((2,0)\) = still the x-axis, since the second
    vector is redundant.
  \end{itemize}
\item
  In \(\mathbb{R}^3\):

  \begin{itemize}
  \tightlist
  \item
    Span of a single vector = a line.
  \item
    Span of two independent vectors = a plane through the origin.
  \item
    Span of three independent vectors = the whole space
    \(\mathbb{R}^3\).
  \end{itemize}
\end{enumerate}

\subsubsection{Span as Coverage}\label{span-as-coverage}

\begin{itemize}
\tightlist
\item
  If you think of vectors as ``directions,'' the span is everything you
  can reach by walking in those directions, with any step lengths
  (scalars) allowed.
\item
  If you only have one direction, you can walk back and forth on a line.
\item
  With two independent directions, you can sweep out a plane.
\item
  With three independent directions in 3D, you can move anywhere.
\end{itemize}

\subsubsection{Generating Sets}\label{generating-sets}

A set of vectors is a generating set (or spanning set) for a subspace if
their span equals that subspace.

\begin{itemize}
\tightlist
\item
  Example: \(\{(1,0), (0,1)\}\) generates \(\mathbb{R}^2\).
\item
  Example: \(\{(1,0,0), (0,1,0), (0,0,1)\}\) generates \(\mathbb{R}^3\).
\item
  Example: The columns of a matrix generate its column space.
\end{itemize}

Different generating sets can span the same space. Some may be
redundant, others minimal. Later, the concept of a basis refines this
idea.

\subsubsection{Redundancy in Spanning
Sets}\label{redundancy-in-spanning-sets}

\begin{itemize}
\tightlist
\item
  If one vector is a linear combination of others, it does not enlarge
  the span.
\item
  Example: In \(\mathbb{R}^2\), \(\{(1,0), (0,1), (1,1)\}\) spans the
  same space as \(\{(1,0), (0,1)\}\).
\item
  Eliminating redundancy leads to a more efficient generating set.
\end{itemize}

\subsubsection{Span and Linear Systems}\label{span-and-linear-systems}

Consider the system \(Ax=b\).

\begin{itemize}
\tightlist
\item
  The question ``Is there a solution?'' is equivalent to ``Is \(b\) in
  the span of the columns of \(A\)?''
\item
  Thus, span provides the geometric language for solvability.
\end{itemize}

\subsubsection{Everyday Analogies}\label{everyday-analogies-29}

\begin{itemize}
\tightlist
\item
  Languages: If you have enough letters (generators), you can spell any
  word (vector). Redundant letters don't add power (extra copies of
  ``A'' don't help).
\item
  Recipes: Ingredients are like generators; the span is all dishes you
  can cook from them. Some ingredients are essential, others can be left
  out.
\item
  Travel directions: If you only know how to go north, you're stuck on a
  line. Add east, and you can reach anywhere on the map.
\end{itemize}

\subsubsection{Why It Matters}\label{why-it-matters-29}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Span is the foundation for defining subspaces generated by vectors.
\item
  It connects directly to solvability of linear equations.
\item
  It introduces the notion of redundancy, preparing for bases and
  independence.
\item
  It generalizes naturally to function spaces and abstract vector
  spaces.
\end{enumerate}

\subsubsection{Try It Yourself}\label{try-it-yourself-32}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Find the span of \(\{(1,2), (2,4)\}\) in \(\mathbb{R}^2\).
\item
  Show that the vectors \((1,0,1), (0,1,1), (1,1,2)\) span only a plane
  in \(\mathbb{R}^3\).
\item
  Decide whether \((1,2,3)\) is in the span of \((1,0,1)\) and
  \((0,1,2)\).
\item
  Challenge: Prove that the set of all polynomials
  \(\{1, x, x^2, \dots\}\) spans the space of all polynomials.
\end{enumerate}

The concept of span transforms our perspective: instead of focusing on
single vectors, we see the entire landscape of possibilities they
generate.

\subsection{34. Linear Independence and
Dependence}\label{linear-independence-and-dependence}

Having introduced span and generating sets, the natural question arises:
\emph{when are the vectors in a spanning set truly necessary, and when
are some redundant?} This leads to the idea of linear independence. It
is the precise way to distinguish between essential vectors (those that
add new directions) and dependent vectors (those that can be expressed
in terms of others).

\subsubsection{Definition of Linear
Independence}\label{definition-of-linear-independence}

A set of vectors \(\{v_1, v_2, \dots, v_k\}\) is linearly independent if
the only solution to

\[
a_1 v_1 + a_2 v_2 + \dots + a_k v_k = 0
\]

is

\[
a_1 = a_2 = \dots = a_k = 0.
\]

If there exists a nontrivial solution (some \(a_i \neq 0\)), then the
vectors are linearly dependent.

\subsubsection{Intuition}\label{intuition-1}

\begin{itemize}
\tightlist
\item
  Independent vectors point in genuinely different directions.
\item
  Dependent vectors overlap: at least one can be built from the others.
\item
  In terms of span: removing a dependent vector does not shrink the
  span, because it adds no new direction.
\end{itemize}

\subsubsection{\texorpdfstring{Simple Examples in
\(\mathbb{R}^2\)}{Simple Examples in \textbackslash mathbb\{R\}\^{}2}}\label{simple-examples-in-mathbbr2}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \((1,0)\) and \((0,1)\) are independent.

  \begin{itemize}
  \tightlist
  \item
    Equation \(a(1,0) + b(0,1) = (0,0)\) forces \(a = b = 0\).
  \end{itemize}
\item
  \((1,0)\) and \((2,0)\) are dependent.

  \begin{itemize}
  \tightlist
  \item
    Equation \(2(1,0) - (2,0) = (0,0)\) shows dependence.
  \end{itemize}
\item
  Any set of 3 vectors in \(\mathbb{R}^2\) is dependent, since the
  dimension of the space is 2.
\end{enumerate}

\subsubsection{\texorpdfstring{Examples in
\(\mathbb{R}^3\)}{Examples in \textbackslash mathbb\{R\}\^{}3}}\label{examples-in-mathbbr3}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \((1,0,0), (0,1,0), (0,0,1)\) are independent.
\item
  \((1,2,3), (2,4,6)\) are dependent, since the second is just 2× the
  first.
\item
  \((1,0,1), (0,1,1), (1,1,2)\) are dependent: the third is the sum of
  the first two.
\end{enumerate}

\subsubsection{Detecting Independence with
Matrices}\label{detecting-independence-with-matrices}

Put the vectors as columns in a matrix. Perform row reduction:

\begin{itemize}
\tightlist
\item
  If every column has a pivot → the set is independent.
\item
  If some column is free → the set is dependent.
\end{itemize}

Example:

\[
\begin{bmatrix}  
1 & 2 & 3 \\  
0 & 1 & 4 \\  
0 & 0 & 0  
\end{bmatrix}.
\]

Here the third column has no pivot → the 3rd vector is dependent on the
first two.

\subsubsection{Relationship with
Dimension}\label{relationship-with-dimension}

\begin{itemize}
\tightlist
\item
  In \(\mathbb{R}^n\), at most \(n\) independent vectors exist.
\item
  If you have more than \(n\), dependence is guaranteed.
\item
  A basis of a vector space is simply a maximal independent set that
  spans the space.
\end{itemize}

\subsubsection{Geometric
Interpretation}\label{geometric-interpretation-4}

\begin{itemize}
\tightlist
\item
  Independent vectors = different directions.
\item
  Dependent vectors = one vector lies in the span of others.
\item
  In 2D: two independent vectors span the plane.
\item
  In 3D: three independent vectors span the space.
\end{itemize}

\subsubsection{Everyday Analogies}\label{everyday-analogies-30}

\begin{itemize}
\tightlist
\item
  Ideas in a meeting: Independent ideas contribute new insights.
  Dependent ideas repeat what others already said.
\item
  Recipes: Having salt, sugar, and flour gives three distinct
  ingredients. But having sugar and ``2× sugar'' is not new-it's
  dependent.
\item
  Travel directions: North and east are independent. North and ``2×
  north'' are dependent.
\end{itemize}

\subsubsection{Why It Matters}\label{why-it-matters-30}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Independence ensures a generating set is minimal and efficient.
\item
  It determines whether a system of vectors is a basis.
\item
  It connects directly to rank: rank = number of independent columns (or
  rows).
\item
  It is crucial in geometry, data compression, and machine
  learning-where redundancy must be identified and removed.
\end{enumerate}

\subsubsection{Try It Yourself}\label{try-it-yourself-33}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Test whether \((1,2)\) and \((2,4)\) are independent.
\item
  Are the vectors \((1,0,0), (0,1,0), (1,1,0)\) independent in
  \(\mathbb{R}^3\)?
\item
  Place the vectors \((1,0,1), (0,1,1), (1,1,2)\) into a matrix and
  row-reduce to check independence.
\item
  Challenge: Prove that any set of \(n+1\) vectors in \(\mathbb{R}^n\)
  is linearly dependent.
\end{enumerate}

Linear independence is the tool that separates essential directions from
redundant ones. It is the key to defining bases, counting dimensions,
and understanding the structure of all vector spaces.

\subsection{35. Basis and Coordinates}\label{basis-and-coordinates}

The concepts of span and linear independence come together in the
powerful idea of a basis. A basis gives us the minimal set of building
blocks needed to generate an entire vector space, with no redundancy.
Once a basis is chosen, every vector in the space can be described
uniquely by a list of numbers called its coordinates.

\subsubsection{What Is a Basis?}\label{what-is-a-basis}

A basis of a vector space \(V\) is a set of vectors
\(\{v_1, v_2, \dots, v_k\}\) that satisfies two properties:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Spanning property: \(\text{span}\{v_1, \dots, v_k\} = V\).
\item
  Independence property: The vectors are linearly independent.
\end{enumerate}

In short: a basis is a spanning set with no redundancy.

\subsubsection{Example: Standard Bases}\label{example-standard-bases}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  In \(\mathbb{R}^2\), the standard basis is \(\{(1,0), (0,1)\}\).
\item
  In \(\mathbb{R}^3\), the standard basis is
  \(\{(1,0,0), (0,1,0), (0,0,1)\}\).
\item
  In \(\mathbb{R}^n\), the standard basis is the collection of unit
  vectors, each with a 1 in one position and 0 elsewhere.
\end{enumerate}

These are called standard because they are the default way of describing
coordinates.

\subsubsection{Uniqueness of
Coordinates}\label{uniqueness-of-coordinates}

One of the most important facts about bases is that they provide unique
representations of vectors.

\begin{itemize}
\item
  Given a basis \(\{v_1, \dots, v_k\}\), any vector \(x \in V\) can be
  written uniquely as:

  \[
  x = a_1 v_1 + a_2 v_2 + \dots + a_k v_k.
  \]
\item
  The coefficients \((a_1, a_2, \dots, a_k)\) are the coordinates of
  \(x\) relative to that basis.
\end{itemize}

This uniqueness distinguishes bases from arbitrary spanning sets, where
redundancy allows multiple representations.

\subsubsection{\texorpdfstring{Example in
\(\mathbb{R}^2\)}{Example in \textbackslash mathbb\{R\}\^{}2}}\label{example-in-mathbbr2}

Let basis = \(\{(1,0), (0,1)\}\).

\begin{itemize}
\tightlist
\item
  Vector \((3,5) = 3(1,0) + 5(0,1)\).
\item
  Coordinates relative to this basis: \((3,5)\).
\end{itemize}

If we switch to a different basis, the coordinates change even though
the vector itself does not.

\subsubsection{Example with Non-Standard
Basis}\label{example-with-non-standard-basis}

Basis = \(\{(1,1), (1,-1)\}\) in \(\mathbb{R}^2\). Find coordinates of
\(x = (2,0)\).

Solve \(a(1,1) + b(1,-1) = (2,0)\). This gives system:

\[
a + b = 2, \quad a - b = 0.
\]

So \(a=1, b=1\). Coordinates relative to this basis: \((1,1)\).

Notice: coordinates depend on basis choice.

\subsubsection{Basis of Function Spaces}\label{basis-of-function-spaces}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  For polynomials of degree ≤ 2: basis = \(\{1, x, x^2\}\).

  \begin{itemize}
  \tightlist
  \item
    Example: \(2 + 3x + 5x^2\) has coordinates \((2,3,5)\).
  \end{itemize}
\item
  For continuous functions on \([0,1]\), one possible basis is the
  infinite set \(\{1, x, x^2, \dots\}\).
\end{enumerate}

This shows bases are not restricted to geometric vectors.

\subsubsection{Dimension}\label{dimension}

The number of vectors in a basis is the dimension of the vector space.

\begin{itemize}
\tightlist
\item
  \(\mathbb{R}^2\) has dimension 2.
\item
  \(\mathbb{R}^3\) has dimension 3.
\item
  The space of polynomials of degree ≤ 3 has dimension 4.
\end{itemize}

Dimension tells us how many independent directions exist in the space.

\subsubsection{Change of Basis}\label{change-of-basis}

\begin{itemize}
\tightlist
\item
  Switching from one basis to another is like translating between
  languages.
\item
  The same vector looks different depending on which ``dictionary''
  (basis) you use.
\item
  Change-of-basis matrices allow systematic translation between
  coordinate systems.
\end{itemize}

\subsubsection{Geometric
Interpretation}\label{geometric-interpretation-5}

\begin{itemize}
\tightlist
\item
  A basis is like setting up coordinate axes in a space.
\item
  In 2D, two independent vectors define a grid.
\item
  In 3D, three independent vectors define a full coordinate system.
\item
  Different bases = different grids overlaying the same space.
\end{itemize}

\subsubsection{Everyday Analogies}\label{everyday-analogies-31}

\begin{itemize}
\tightlist
\item
  Language: A basis is like an alphabet. Every word (vector) can be
  spelled uniquely from the letters (basis vectors).
\item
  Colors: RGB is a basis for color space. Any color can be described
  uniquely by mixing red, green, and blue.
\item
  Music: Musical notes form a basis. Every chord or melody is a
  combination of them.
\end{itemize}

\subsubsection{Why It Matters}\label{why-it-matters-31}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Bases provide the simplest possible description of a vector space.
\item
  They allow us to assign unique coordinates to vectors.
\item
  They connect the abstract structure of a space with concrete numerical
  representations.
\item
  The concept underlies almost all of linear algebra: dimension,
  transformations, eigenvectors, and more.
\end{enumerate}

\subsubsection{Try It Yourself}\label{try-it-yourself-34}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Show that \(\{(1,2), (3,4)\}\) is a basis of \(\mathbb{R}^2\).
\item
  Express \((4,5)\) in terms of basis \(\{(1,1), (1,-1)\}\).
\item
  Prove that no basis of \(\mathbb{R}^3\) can have more than 3 vectors.
\item
  Challenge: Show that the set \(\{1, \cos x, \sin x\}\) is a basis for
  the space of all linear combinations of \(1, \cos x, \sin x\).
\end{enumerate}

A basis is the minimal, elegant foundation of a vector space, turning
the infinite into the manageable by providing a finite set of
independent building blocks.

\subsection{36. Dimension}\label{dimension-1}

Dimension is one of the most profound and unifying ideas in linear
algebra. It gives a single number that captures the ``size'' or
``capacity'' of a vector space: how many independent directions it has.
Unlike length, width, or height in everyday geometry, dimension in
linear algebra applies to spaces of any kind-geometric, algebraic, or
even function spaces.

\subsubsection{Definition}\label{definition}

The dimension of a vector space \(V\) is the number of vectors in any
basis of \(V\).

\begin{itemize}
\item
  Since all bases of a vector space have the same number of elements,
  dimension is well-defined.
\item
  If \(\dim V = n\), then:

  \begin{itemize}
  \tightlist
  \item
    Every set of more than \(n\) vectors in \(V\) is dependent.
  \item
    Every set of exactly \(n\) independent vectors forms a basis.
  \end{itemize}
\end{itemize}

\subsubsection{Examples in Familiar
Spaces}\label{examples-in-familiar-spaces}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \(\dim(\mathbb{R}^2) = 2\).

  \begin{itemize}
  \tightlist
  \item
    Basis: \((1,0), (0,1)\).
  \item
    Two directions cover the whole plane.
  \end{itemize}
\item
  \(\dim(\mathbb{R}^3) = 3\).

  \begin{itemize}
  \tightlist
  \item
    Basis: \((1,0,0), (0,1,0), (0,0,1)\).
  \item
    Three independent directions span 3D space.
  \end{itemize}
\item
  The set of all polynomials of degree ≤ 2 has dimension 3.

  \begin{itemize}
  \tightlist
  \item
    Basis: \(\{1, x, x^2\}\).
  \end{itemize}
\item
  The space of all \(m \times n\) matrices has dimension \(mn\).

  \begin{itemize}
  \tightlist
  \item
    Each entry is independent, and the standard basis consists of
    matrices with a single 1 and the rest 0.
  \end{itemize}
\end{enumerate}

\subsubsection{Finite vs.~Infinite
Dimensions}\label{finite-vs.-infinite-dimensions}

\begin{itemize}
\item
  Finite-dimensional spaces: \(\mathbb{R}^n\), polynomials of degree ≤
  \(k\).
\item
  Infinite-dimensional spaces:

  \begin{itemize}
  \tightlist
  \item
    The space of all polynomials (no degree limit).
  \item
    The space of all continuous functions.
  \item
    These cannot be spanned by a finite set of vectors.
  \end{itemize}
\end{itemize}

\subsubsection{Dimension and Subspaces}\label{dimension-and-subspaces}

\begin{itemize}
\tightlist
\item
  Any subspace of \(\mathbb{R}^n\) has dimension ≤ \(n\).
\item
  A line through the origin in \(\mathbb{R}^3\): dimension 1.
\item
  A plane through the origin in \(\mathbb{R}^3\): dimension 2.
\item
  The whole space: dimension 3.
\item
  The trivial subspace \(\{0\}\): dimension 0.
\end{itemize}

\subsubsection{Dimension and Systems of
Equations}\label{dimension-and-systems-of-equations}

When solving \(A\mathbf{x} = \mathbf{b}\):

\begin{itemize}
\item
  The dimension of the column space = rank = number of independent
  directions in the outputs.
\item
  The dimension of the null space = number of free variables.
\item
  By the rank--nullity theorem:

  \[
  \dim(\text{column space}) + \dim(\text{null space}) = \text{number of variables}.
  \]
\end{itemize}

\subsubsection{Geometric Meaning}\label{geometric-meaning-4}

\begin{itemize}
\tightlist
\item
  Dimension counts the minimum number of coordinates needed to describe
  a vector.
\item
  In \(\mathbb{R}^2\), you need 2 numbers.
\item
  In \(\mathbb{R}^3\), you need 3 numbers.
\item
  In the polynomial space of degree ≤ 3, you need 4 coefficients.
\end{itemize}

Thus, dimension = length of coordinate list.

\subsubsection{Everyday Analogies}\label{everyday-analogies-32}

\begin{itemize}
\tightlist
\item
  Maps: A flat map needs 2 coordinates (latitude, longitude). A globe is
  3D and needs 3. Adding altitude makes it 3D as well.
\item
  Languages: To describe meaning, you need enough independent words.
  Redundant words don't add dimension; new independent concepts do.
\item
  Recipes: To describe all possible flavors, you need a certain number
  of independent ingredients. More ingredients = higher-dimensional
  ``flavor space.''
\end{itemize}

\subsubsection{Checking Dimension in
Practice}\label{checking-dimension-in-practice}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Place candidate vectors as columns of a matrix.
\item
  Row reduce to echelon form.
\item
  Count pivots. That number = dimension of the span of those vectors.
\end{enumerate}

\subsubsection{Why It Matters}\label{why-it-matters-32}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Dimension is the most fundamental measure of a vector space.
\item
  It tells us how ``large'' or ``complex'' the space is.
\item
  It sets absolute limits: in \(\mathbb{R}^n\), no more than \(n\)
  independent vectors exist.
\item
  It underlies coordinate systems, bases, and transformations.
\item
  It bridges geometry (lines, planes, volumes) with algebra (solutions,
  equations, matrices).
\end{enumerate}

\subsubsection{Try It Yourself}\label{try-it-yourself-35}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  What is the dimension of the span of \((1,2,3)\), \((2,4,6)\),
  \((0,0,0)\)?
\item
  Find the dimension of the subspace of \(\mathbb{R}^3\) defined by
  \(x+y+z=0\).
\item
  Prove that the set of all \(2 \times 2\) symmetric matrices has
  dimension 3.
\item
  Challenge: Show that the space of polynomials of degree ≤ \(k\) has
  dimension \(k+1\).
\end{enumerate}

Dimension is the measuring stick of linear algebra: it tells us how many
independent pieces of information are needed to describe the whole
space.

\subsection{37. Rank--Nullity Theorem}\label{ranknullity-theorem}

The rank--nullity theorem is one of the central results of linear
algebra. It gives a precise balance between two fundamental aspects of a
matrix: the dimension of its column space (rank) and the dimension of
its null space (nullity). It shows that no matter how complicated a
matrix looks, the distribution of information between its ``visible''
outputs and its ``hidden'' null directions always obeys a strict law.

\subsubsection{Statement of the Theorem}\label{statement-of-the-theorem}

Let \(A\) be an \(m \times n\) matrix (mapping
\(\mathbb{R}^n \to \mathbb{R}^m\)):

\[
\text{rank}(A) + \text{nullity}(A) = n
\]

where:

\begin{itemize}
\tightlist
\item
  rank(A) = dimension of the column space of \(A\).
\item
  nullity(A) = dimension of the null space of \(A\).
\item
  \(n\) = number of columns of \(A\), i.e., the number of variables.
\end{itemize}

\subsubsection{Intuition}\label{intuition-2}

Think of a matrix as a machine that transforms input vectors into
outputs:

\begin{itemize}
\tightlist
\item
  Rank measures how many independent output directions survive.
\item
  Nullity measures how many input directions get ``lost'' (mapped to
  zero).
\item
  The theorem says: total inputs = useful directions (rank) + wasted
  directions (nullity).
\end{itemize}

This ensures nothing disappears mysteriously-every input direction is
accounted for.

\subsubsection{Example 1: Full Rank}\label{example-1-full-rank}

\[
A = \begin{bmatrix}  
1 & 0 \\  
0 & 1 \\  
\end{bmatrix}.
\]

\begin{itemize}
\tightlist
\item
  Rank = 2 (two independent columns).
\item
  Null space = \(\{0\}\), so nullity = 0.
\item
  Rank + nullity = 2 = number of variables.
\end{itemize}

\subsubsection{Example 2: Dependent
Columns}\label{example-2-dependent-columns}

\[
A = \begin{bmatrix}  
1 & 2 \\  
2 & 4 \\  
3 & 6 \\  
\end{bmatrix}.
\]

\begin{itemize}
\tightlist
\item
  Second column is a multiple of the first. Rank = 1.
\item
  Null space contains all vectors \((x,y)\) with \(y = -2x\). Nullity =
  1.
\item
  Rank + nullity = 1 + 1 = 2 = number of variables.
\end{itemize}

\subsubsection{Example 3: Larger System}\label{example-3-larger-system}

\[
A = \begin{bmatrix}  
1 & 0 & 1 \\  
0 & 1 & 1  
\end{bmatrix}.
\]

\begin{itemize}
\tightlist
\item
  Columns: \((1,0), (0,1), (1,1)\).
\item
  Only two independent columns → Rank = 2.
\item
  Null space: solve
  \(x + z = 0, y + z = 0 \Rightarrow (x,y,z) = (-t,-t,t)\). Nullity = 1.
\item
  Rank + nullity = 2 + 1 = 3 = number of variables.
\end{itemize}

\subsubsection{Proof Sketch (Conceptual)}\label{proof-sketch-conceptual}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Row reduce \(A\) to echelon form.
\item
  Pivots correspond to independent columns → count = rank.
\item
  Free variables correspond to null space directions → count = nullity.
\item
  Each column is either a pivot column or corresponds to a free
  variable, so:

  \[
  \text{rank} + \text{nullity} = \text{number of columns}.
  \]
\end{enumerate}

\subsubsection{Geometric Meaning}\label{geometric-meaning-5}

\begin{itemize}
\tightlist
\item
  In \(\mathbb{R}^3\), if a transformation collapses all vectors onto a
  plane (rank = 2), then one direction disappears entirely (nullity =
  1).
\item
  In \(\mathbb{R}^4\), if a matrix has rank 2, then its null space has
  dimension 2, meaning half the input directions vanish.
\end{itemize}

The theorem guarantees the geometry of ``surviving'' and ``vanishing''
directions always adds up consistently.

\subsubsection{Applications}\label{applications-1}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Solving systems \(Ax = b\):

  \begin{itemize}
  \tightlist
  \item
    Rank determines consistency and structure of solutions.
  \item
    Nullity tells how many free parameters exist in the solution.
  \end{itemize}
\item
  Data compression: Rank identifies independent features; nullity shows
  redundancy.
\item
  Computer graphics: Rank--nullity explains how 3D coordinates collapse
  into 2D images: one dimension of depth is lost.
\item
  Machine learning: Rank signals how much real information a dataset
  contains; nullity indicates degrees of freedom that add nothing new.
\end{enumerate}

\subsubsection{Everyday Analogies}\label{everyday-analogies-33}

\begin{itemize}
\tightlist
\item
  Work team: Rank = number of independent workers contributing new
  ideas. Nullity = number of workers repeating what others already said.
  Total team members = contributors + redundant voices.
\item
  Travel: Rank = number of useful directions on a map; nullity =
  directions that lead nowhere.
\item
  Languages: Rank = unique words, nullity = synonyms. Total vocabulary
  size is always the sum.
\end{itemize}

\subsubsection{Why It Matters}\label{why-it-matters-33}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The rank--nullity theorem connects the abstract ideas of rank and
  nullity into a single, elegant formula.
\item
  It ensures conservation of dimension: no information magically appears
  or disappears.
\item
  It is essential in understanding solutions of systems, dimensions of
  subspaces, and the structure of linear transformations.
\item
  It prepares the ground for deeper results in algebra, topology, and
  differential equations.
\end{enumerate}

\subsubsection{Try It Yourself}\label{try-it-yourself-36}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Verify rank--nullity for

  \[
  A = \begin{bmatrix}  
  1 & 2 & 3 \\  
  4 & 5 & 6  
  \end{bmatrix}.
  \]
\item
  For a \(4 \times 5\) matrix of rank 3, what is its nullity?
\item
  In \(\mathbb{R}^3\), suppose a matrix maps all of space onto a line.
  What are its rank and nullity?
\item
  Challenge: Prove rigorously that the row space and null space are
  orthogonal complements, and use this to derive rank--nullity again.
\end{enumerate}

The rank--nullity theorem is the law of balance in linear algebra: every
input dimension is accounted for, either as a surviving direction (rank)
or as one that vanishes (nullity).

\subsection{38. Coordinates Relative to a
Basis}\label{coordinates-relative-to-a-basis}

Once a basis for a vector space is chosen, every vector in that space
can be described uniquely in terms of the basis. These descriptions are
called coordinates. Coordinates transform abstract vectors into concrete
lists of numbers, making computation possible. Changing the basis
changes the coordinates, but the underlying vector remains the same.

\subsubsection{The Core Idea}\label{the-core-idea}

Given a vector space \(V\) and a basis \(B = \{v_1, v_2, \dots, v_n\}\),
every vector \(x \in V\) can be written uniquely as:

\[
x = a_1 v_1 + a_2 v_2 + \dots + a_n v_n.
\]

The coefficients \((a_1, a_2, \dots, a_n)\) are the coordinates of \(x\)
with respect to the basis \(B\).

This representation is unique because basis vectors are independent.

\subsubsection{\texorpdfstring{Example in
\(\mathbb{R}^2\)}{Example in \textbackslash mathbb\{R\}\^{}2}}\label{example-in-mathbbr2-1}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Standard basis: \(B = \{(1,0), (0,1)\}\).

  \begin{itemize}
  \tightlist
  \item
    Vector \(x = (3,5)\).
  \item
    Coordinates relative to \(B\): \((3,5)\).
  \end{itemize}
\item
  Non-standard basis: \(B = \{(1,1), (1,-1)\}\).

  \begin{itemize}
  \item
    Write \(x = (3,5)\) as \(a(1,1) + b(1,-1)\).
  \item
    Solve:

    \[
    a+b = 3, \quad a-b = 5.
    \]

    Adding: \(2a = 8 \implies a = 4\). Subtracting:
    \(2b = -2 \implies b = -1\).
  \item
    Coordinates relative to this basis: \((4, -1)\).
  \end{itemize}
\end{enumerate}

The same vector looks different depending on the chosen basis.

\subsubsection{\texorpdfstring{Example in
\(\mathbb{R}^3\)}{Example in \textbackslash mathbb\{R\}\^{}3}}\label{example-in-mathbbr3}

Let \(B = \{(1,0,0), (1,1,0), (1,1,1)\}\). Find coordinates of
\(x = (2,3,4)\).

Solve \(a(1,0,0) + b(1,1,0) + c(1,1,1) = (2,3,4)\). This gives system:

\[
a+b+c = 2, \quad b+c = 3, \quad c = 4.
\]

From \(c=4\), we get \(b+c=3 \implies b=-1\). Then
\(a+b+c=2 \implies a-1+4=2 \implies a=-1\). Coordinates:
\((-1, -1, 4)\).

\subsubsection{Matrix Formulation}\label{matrix-formulation}

If \(B = \{v_1, \dots, v_n\}\), form the basis matrix

\[
P = [v_1 \ v_2 \ \dots \ v_n].
\]

Then for a vector \(x\), its coordinate vector \([x]_B\) satisfies

\[
P [x]_B = x.
\]

Thus,

\[
[x]_B = P^{-1}x.
\]

This shows coordinate transformation is simply matrix multiplication.

\subsubsection{Changing Coordinates}\label{changing-coordinates}

Suppose a vector has coordinates \([x]_B\) relative to basis \(B\). If
we switch to another basis \(C\), we use a change-of-basis matrix to
convert coordinates:

\[
[x]_C = (P_C^{-1} P_B) [x]_B.
\]

This process is fundamental in computer graphics, robotics, and data
transformations.

\subsubsection{Geometric Meaning}\label{geometric-meaning-6}

\begin{itemize}
\tightlist
\item
  A basis defines a coordinate system: axes in the space.
\item
  Coordinates are the ``addresses'' of vectors relative to those axes.
\item
  Changing basis is like rotating or stretching the grid: the address
  changes, but the point does not.
\end{itemize}

\subsubsection{Everyday Analogies}\label{everyday-analogies-34}

\begin{itemize}
\tightlist
\item
  Maps: Latitude and longitude are coordinates relative to Earth's axis.
  If we change the map projection, the coordinates change, but the
  physical location stays the same.
\item
  Languages: Describing the same object in English or French yields
  different words (coordinates), but the object (vector) is unchanged.
\item
  Music: The same melody can be written in different keys (bases); the
  notes shift, but the tune remains.
\end{itemize}

\subsubsection{Why It Matters}\label{why-it-matters-34}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Coordinates make abstract vectors computable.
\item
  They allow us to represent functions, polynomials, and geometric
  objects numerically.
\item
  Changing basis simplifies problems-e.g., diagonalization makes
  matrices easy to analyze.
\item
  They connect the abstract (spaces, bases) with the concrete (numbers,
  matrices).
\end{enumerate}

\subsubsection{Try It Yourself}\label{try-it-yourself-37}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Express \(x=(4,2)\) relative to basis \(\{(1,1),(1,-1)\}\).
\item
  Find coordinates of \(x=(2,1,3)\) relative to basis
  \(\{(1,0,1),(0,1,1),(1,1,0)\}\).
\item
  If basis \(B\) is the standard basis and basis \(C=\{(1,1),(1,-1)\}\),
  compute the change-of-basis matrix from \(B\) to \(C\).
\item
  Challenge: Show that if \(P\) is invertible, its columns form a basis,
  and explain why this guarantees uniqueness of coordinates.
\end{enumerate}

Coordinates relative to a basis are the bridge between geometry and
algebra: they turn abstract spaces into numerical systems where
computation, reasoning, and transformation become systematic and
precise.

\subsection{39. Change-of-Basis
Matrices}\label{change-of-basis-matrices}

Every vector space allows multiple choices of basis, and each basis
provides a different way of describing the same vectors. The process of
moving from one basis to another is called a change of basis. To perform
this change systematically, we use a change-of-basis matrix. This matrix
acts as a translator between coordinate systems: it converts the
coordinates of a vector relative to one basis into coordinates relative
to another.

\subsubsection{Why Change Bases?}\label{why-change-bases}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Simplicity of computation: Some problems are easier in certain bases.
  For example, diagonalizing a matrix allows us to raise it to powers
  more easily.
\item
  Geometry: Different bases can represent rotated or scaled coordinate
  systems.
\item
  Applications: In physics, computer graphics, robotics, and data
  science, changing bases is equivalent to switching perspectives or
  reference frames.
\end{enumerate}

\subsubsection{The Basic Setup}\label{the-basic-setup}

Let \(V\) be a vector space with two bases:

\begin{itemize}
\tightlist
\item
  \(B = \{b_1, b_2, \dots, b_n\}\)
\item
  \(C = \{c_1, c_2, \dots, c_n\}\)
\end{itemize}

Suppose a vector \(x \in V\) has coordinates \([x]_B\) relative to
\(B\), and \([x]_C\) relative to \(C\).

We want a matrix \(P_{B \to C}\) such that:

\[
[x]_C = P_{B \to C} [x]_B.
\]

This matrix \(P_{B \to C}\) is the change-of-basis matrix from \(B\) to
\(C\).

\subsubsection{Constructing the Change-of-Basis
Matrix}\label{constructing-the-change-of-basis-matrix}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Write each vector in the basis \(B\) in terms of the basis \(C\).
\item
  Place these coordinate vectors as the columns of a matrix.
\item
  The resulting matrix converts coordinates from \(B\) to \(C\).
\end{enumerate}

In matrix form:

\[
P_{B \to C} = \big[ [b_1]_C \ [b_2]_C \ \dots \ [b_n]_C \big].
\]

\subsubsection{\texorpdfstring{Example in
\(\mathbb{R}^2\)}{Example in \textbackslash mathbb\{R\}\^{}2}}\label{example-in-mathbbr2-2}

Let

\begin{itemize}
\tightlist
\item
  \(B = \{(1,0), (0,1)\}\) (standard basis).
\item
  \(C = \{(1,1), (1,-1)\}\).
\end{itemize}

To build \(P_{B \to C}\):

\begin{itemize}
\tightlist
\item
  Express each vector of \(B\) in terms of \(C\).
\end{itemize}

Solve:

\[
(1,0) = a(1,1) + b(1,-1).
\]

This gives system:

\[
a+b=1, \quad a-b=0.
\]

Solution: \(a=\tfrac{1}{2}, b=\tfrac{1}{2}\). So
\((1,0) = \tfrac{1}{2}(1,1) + \tfrac{1}{2}(1,-1)\).

Next:

\[
(0,1) = a(1,1) + b(1,-1).
\]

System:

\[
a+b=0, \quad a-b=1.
\]

Solution: \(a=\tfrac{1}{2}, b=-\tfrac{1}{2}\).

Thus:

\[
P_{B \to C} = \begin{bmatrix}  
\tfrac{1}{2} & \tfrac{1}{2} \\  
\tfrac{1}{2} & -\tfrac{1}{2}  
\end{bmatrix}.
\]

So for any vector \(x\),

\[
[x]_C = P_{B \to C}[x]_B.
\]

\subsubsection{Inverse Change of Basis}\label{inverse-change-of-basis}

If \(P_{B \to C}\) is the change-of-basis matrix from \(B\) to \(C\),
then its inverse is the change-of-basis matrix in the opposite
direction:

\[
P_{C \to B} = (P_{B \to C})^{-1}.
\]

This makes sense: translating back and forth between languages should
undo itself.

\subsubsection{General Formula with Basis
Matrices}\label{general-formula-with-basis-matrices}

Let

\[
P_B = [b_1 \ b_2 \ \dots \ b_n], \quad P_C = [c_1 \ c_2 \ \dots \ c_n],
\]

the matrices whose columns are basis vectors written in standard
coordinates.

Then the change-of-basis matrix from \(B\) to \(C\) is:

\[
P_{B \to C} = P_C^{-1} P_B.
\]

This formula is extremely useful because it reduces the problem to
matrix multiplication.

\subsubsection{Geometric
Interpretation}\label{geometric-interpretation-6}

\begin{itemize}
\tightlist
\item
  Changing basis is like rotating or stretching the grid lines of a
  coordinate system.
\item
  The vector itself (the point in space) does not move. What changes is
  its description in terms of the new grid.
\item
  The change-of-basis matrix is the tool that translates between these
  descriptions.
\end{itemize}

\subsubsection{Applications}\label{applications-2}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Diagonalization: Expressing a matrix in a basis of its eigenvectors
  makes it diagonal, simplifying analysis.
\item
  Computer graphics: Changing camera viewpoints requires change-of-basis
  matrices.
\item
  Robotics: Coordinate transformations connect robot arms, joints, and
  workspace frames.
\item
  Data science: PCA finds a new basis (principal components) where data
  is easier to analyze.
\end{enumerate}

\subsubsection{Everyday Analogies}\label{everyday-analogies-35}

\begin{itemize}
\tightlist
\item
  Languages: The word ``dog'' in English corresponds to ``chien'' in
  French. The change-of-basis matrix is the bilingual dictionary.
\item
  Currencies: Converting from dollars to euros requires an exchange
  rate. The matrix is the exchange table.
\item
  Maps: Switching between Cartesian and polar coordinates is a change of
  basis-different coordinates, same location.
\end{itemize}

\subsubsection{Why It Matters}\label{why-it-matters-35}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Provides a universal method to translate coordinates between bases.
\item
  Makes abstract transformations concrete and computable.
\item
  Forms the backbone of diagonalization, Jordan form, and the spectral
  theorem.
\item
  Connects algebraic manipulations with geometry and real-world
  reference frames.
\end{enumerate}

\subsubsection{Try It Yourself}\label{try-it-yourself-38}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute the change-of-basis matrix from the standard basis to
  \(\{(2,1),(1,1)\}\) in \(\mathbb{R}^2\).
\item
  Find the change-of-basis matrix from basis
  \(\{(1,0,0),(0,1,0),(0,0,1)\}\) to \(\{(1,1,0),(0,1,1),(1,0,1)\}\) in
  \(\mathbb{R}^3\).
\item
  Show that applying \(P_{B \to C}\) then \(P_{C \to B}\) returns the
  original coordinates.
\item
  Challenge: Derive the formula \(P_{B \to C} = P_C^{-1} P_B\) starting
  from the definition of coordinates.
\end{enumerate}

Change-of-basis matrices give us the precise mechanism for switching
perspectives. They ensure that although bases change, vectors remain
invariant, and computations remain consistent.

\subsection{40. Affine Subspaces}\label{affine-subspaces}

So far, vector spaces and subspaces have always passed through the
origin. But in many real-world situations, we deal with shifted versions
of these spaces: planes not passing through the origin, lines offset
from the zero vector, or solution sets to linear equations with nonzero
constants. These structures are called affine subspaces. They extend the
idea of subspaces by allowing ``translation away from the origin.''

\subsubsection{Definition}\label{definition-1}

An affine subspace of a vector space \(V\) is a set of the form

\[
x_0 + W = \{x_0 + w : w \in W\},
\]

where:

\begin{itemize}
\tightlist
\item
  \(x_0 \in V\) is a fixed vector (the ``base point'' or ``anchor''),
\item
  \(W \subseteq V\) is a linear subspace.
\end{itemize}

Thus, an affine subspace is simply a subspace shifted by a vector.

\subsubsection{\texorpdfstring{Examples in
\(\mathbb{R}^2\)}{Examples in \textbackslash mathbb\{R\}\^{}2}}\label{examples-in-mathbbr2}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  A line through the origin: \(\text{span}\{(1,2)\}\). This is a
  subspace.
\item
  A line not through the origin: \((3,1) + \text{span}\{(1,2)\}\). This
  is an affine subspace.
\item
  The entire plane: \(\mathbb{R}^2\), which is both a subspace and an
  affine subspace.
\end{enumerate}

\subsubsection{\texorpdfstring{Examples in
\(\mathbb{R}^3\)}{Examples in \textbackslash mathbb\{R\}\^{}3}}\label{examples-in-mathbbr3-1}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Plane through the origin: \(\text{span}\{(1,0,0),(0,1,0)\}\).
\item
  Plane not through the origin:
  \((2,3,4) + \text{span}\{(1,0,0),(0,1,0)\}\).
\item
  Line parallel to the z-axis but passing through \((1,1,5)\):
  \((1,1,5) + \text{span}\{(0,0,1)\}\).
\end{enumerate}

\subsubsection{Relation to Linear
Systems}\label{relation-to-linear-systems}

Affine subspaces naturally arise as solution sets of linear equations.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Homogeneous system: \(Ax = 0\).

  \begin{itemize}
  \tightlist
  \item
    Solution set is a subspace (the null space).
  \end{itemize}
\item
  Non-homogeneous system: \(Ax = b\) with \(b \neq 0\).

  \begin{itemize}
  \item
    Solution set is affine.
  \item
    If \(x_p\) is one particular solution, then the general solution is:

    \[
    x = x_p + N(A),
    \]

    where \(N(A)\) is the null space.
  \end{itemize}
\end{enumerate}

Thus, the geometry of solving equations leads naturally to affine
subspaces.

\subsubsection{Affine Dimension}\label{affine-dimension}

The dimension of an affine subspace is defined as the dimension of its
direction subspace \(W\).

\begin{itemize}
\tightlist
\item
  A point: affine subspace of dimension 0.
\item
  A line: dimension 1.
\item
  A plane: dimension 2.
\item
  Higher analogs continue in \(\mathbb{R}^n\).
\end{itemize}

\subsubsection{Difference Between Subspaces and Affine
Subspaces}\label{difference-between-subspaces-and-affine-subspaces}

\begin{itemize}
\tightlist
\item
  Subspaces always contain the origin.
\item
  Affine subspaces may or may not pass through the origin.
\item
  Every subspace is an affine subspace (with base point \(x_0 = 0\)).
\end{itemize}

\subsubsection{Geometric Intuition}\label{geometric-intuition-2}

Think of affine subspaces as ``flat sheets'' floating in space:

\begin{itemize}
\tightlist
\item
  A line through the origin is a rope tied at the center.
\item
  A line parallel to it but offset is the same rope moved to the side.
\item
  Affine subspaces preserve shape and direction, but not position.
\end{itemize}

\subsubsection{Everyday Analogies}\label{everyday-analogies-36}

\begin{itemize}
\tightlist
\item
  Railway tracks: A railway line is straight (like a subspace), but it
  doesn't need to pass through the city center (origin).
\item
  Office floors: Each floor in a building is a plane parallel to the
  ground, offset vertically. The ground floor is like the subspace
  through the origin; higher floors are affine subspaces.
\item
  Schedules: A repeating pattern (like working hours 9--5) is the
  subspace. Starting the shift at 10 instead of 9 shifts it into an
  affine version.
\end{itemize}

\subsubsection{Applications}\label{applications-3}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Linear equations: General solutions are affine subspaces.
\item
  Optimization: Feasible regions in linear programming are affine
  subspaces (intersected with inequalities).
\item
  Computer graphics: Affine transformations map affine subspaces to
  affine subspaces, preserving straightness and parallelism.
\item
  Machine learning: Affine decision boundaries (like hyperplanes)
  separate data into classes.
\end{enumerate}

\subsubsection{Why It Matters}\label{why-it-matters-36}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Affine subspaces generalize subspaces, making linear algebra more
  flexible.
\item
  They allow us to describe solution sets that don't include the origin.
\item
  They provide the geometric foundation for affine geometry, computer
  graphics, and optimization.
\item
  They serve as the bridge from pure linear algebra to applied modeling.
\end{enumerate}

\subsubsection{Try It Yourself}\label{try-it-yourself-39}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Show that the set of solutions to

  \[
  x+y+z=1
  \]

  is an affine subspace of \(\mathbb{R}^3\). Identify its dimension.
\item
  Find the general solution to

  \[
  x+2y=3
  \]

  and describe it as an affine subspace.
\item
  Prove that the intersection of two affine subspaces is either empty or
  another affine subspace.
\item
  Challenge: Show that every affine subspace can be written uniquely as
  \(x_0 + W\) with \(W\) a subspace.
\end{enumerate}

Affine subspaces are the natural setting for most real-world linear
problems: they combine the strict structure of subspaces with the
freedom of translation, capturing both direction and position.

\subsubsection{Closing}\label{closing-3}

\begin{verbatim}
Each basis a song,
dimension counts melodies,
the space breathes its form.
\end{verbatim}

\section{Chapter 5. Linear Transformation and
Structure}\label{chapter-5.-linear-transformation-and-structure}

\subsubsection{Opening}\label{opening-3}

\begin{verbatim}
Maps preserve the line,
reflections ripple outward,
motion kept in frame.
\end{verbatim}

\subsection{41. Linear Transformations}\label{linear-transformations}

A linear transformation is the heart of linear algebra. It is the rule
that connects two vector spaces in a way that respects their linear
structure: addition and scalar multiplication. Instead of thinking of
vectors as static objects, linear transformations let us study how
vectors move, stretch, rotate, project, or reflect. They give linear
algebra its dynamic power and are the bridge between abstract theory and
concrete applications.

\subsubsection{Definition}\label{definition-2}

A function \(T: V \to W\) between vector spaces is called a linear
transformation if for all \(u, v \in V\) and scalars
\(a, b \in \mathbb{R}\) (or another field),

\[
T(au + bv) = aT(u) + bT(v).
\]

This single condition encodes two rules:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Additivity: \(T(u+v) = T(u) + T(v)\).
\item
  Homogeneity: \(T(av) = aT(v)\).
\end{enumerate}

If both are satisfied, the transformation is linear.

\subsubsection{Examples of Linear
Transformations}\label{examples-of-linear-transformations}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Scaling: \(T(x) = 3x\) in \(\mathbb{R}\). Every number is stretched
  threefold.
\item
  Rotation in the plane:
  \(T(x,y) = (x\cos\theta - y\sin\theta, \, x\sin\theta + y\cos\theta)\).
\item
  Projection: Projecting \((x,y,z)\) onto the \(xy\)-plane:
  \(T(x,y,z) = (x,y,0)\).
\item
  Differentiation: On the space of polynomials, \(T(p(x)) = p'(x)\).
\item
  Integration: On continuous functions,
  \(T(f)(x) = \int_0^x f(t) \, dt\).
\end{enumerate}

All these are linear because they preserve addition and scaling.

\subsubsection{Non-Examples}\label{non-examples-1}

\begin{itemize}
\tightlist
\item
  \(T(x) = x^2\) is not linear, because \((x+y)^2 \neq x^2 + y^2\).
\item
  \(T(x,y) = (x+1, y)\) is not linear, because it fails homogeneity:
  scaling doesn't preserve the ``+1.''
\end{itemize}

Nonlinear rules break the structure of vector spaces.

\subsubsection{Matrix Representation}\label{matrix-representation}

Every linear transformation from \(\mathbb{R}^n\) to \(\mathbb{R}^m\)
can be represented by a matrix.

If \(T: \mathbb{R}^n \to \mathbb{R}^m\), then there exists an
\(m \times n\) matrix \(A\) such that:

\[
T(x) = Ax.
\]

The columns of \(A\) are simply \(T(e_1), T(e_2), \dots, T(e_n)\), where
\(e_i\) are the standard basis vectors.

Example: Let \(T(x,y) = (2x+y, x-y)\).

\begin{itemize}
\tightlist
\item
  \(T(e_1) = T(1,0) = (2,1)\).
\item
  \(T(e_2) = T(0,1) = (1,-1)\). So
\end{itemize}

\[
A = \begin{bmatrix} 2 & 1 \\ 1 & -1 \end{bmatrix}.
\]

Then \(T(x,y) = A \begin{bmatrix} x \\ y \end{bmatrix}\).

\subsubsection{Properties of Linear
Transformations}\label{properties-of-linear-transformations}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The image of the zero vector is always zero: \(T(0) = 0\).
\item
  The image of a line through the origin is again a line (or collapsed
  to a point).
\item
  Composition of linear transformations is linear.
\item
  Every linear transformation preserves the structure of subspaces.
\end{enumerate}

\subsubsection{Kernel and Image
(Preview)}\label{kernel-and-image-preview}

For \(T: V \to W\):

\begin{itemize}
\tightlist
\item
  The kernel (or null space) is all vectors mapped to zero:
  \(\ker T = \{v \in V : T(v) = 0\}\).
\item
  The image (or range) is all outputs that can be achieved:
  \(\text{im}(T) = \{T(v) : v \in V\}\). The rank--nullity theorem
  applies here:
\end{itemize}

\[
\dim(\ker T) + \dim(\text{im}(T)) = \dim(V).
\]

\subsubsection{Geometric
Interpretation}\label{geometric-interpretation-7}

Linear transformations reshape space:

\begin{itemize}
\tightlist
\item
  Scaling stretches space uniformly in one direction.
\item
  Rotation spins space while preserving lengths.
\item
  Projection flattens space onto lower dimensions.
\item
  Reflection flips space across a line or plane.
\end{itemize}

The key feature: straight lines remain straight, and the origin stays
fixed.

\subsubsection{Everyday Analogies}\label{everyday-analogies-37}

\begin{itemize}
\tightlist
\item
  Maps and coordinates: A GPS system transforms geographic positions
  (lat/long) into screen positions (x,y).
\item
  Shadows: Projecting a 3D object onto the ground is a linear
  transformation.
\item
  Economics: A matrix that takes production levels (inputs) and outputs
  profits is a linear transformation of data.
\item
  Music: An equalizer linearly adjusts different frequencies: the
  structure of the sound is preserved but reshaped.
\end{itemize}

\subsubsection{Applications}\label{applications-4}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Computer graphics: Scaling, rotating, projecting 3D objects onto 2D
  screens.
\item
  Robotics: Transformations between joint coordinates and workspace
  positions.
\item
  Data science: Linear mappings represent dimensionality reduction and
  feature extraction.
\item
  Differential equations: Solutions often involve linear operators
  acting on function spaces.
\item
  Machine learning: Weight matrices in neural networks are stacked
  linear transformations, interspersed with nonlinearities.
\end{enumerate}

\subsubsection{Why It Matters}\label{why-it-matters-37}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Linear transformations generalize matrices to any vector space.
\item
  They unify geometry, algebra, and applications under one concept.
\item
  They provide the natural framework for studying eigenvalues,
  eigenvectors, and decompositions.
\item
  They model countless real-world processes: physical, computational,
  and abstract.
\end{enumerate}

\subsubsection{Try It Yourself}\label{try-it-yourself-40}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Prove that \(T(x,y,z) = (x+2y, z, x-y+z)\) is linear.
\item
  Find the matrix representation of the transformation that reflects
  vectors in \(\mathbb{R}^2\) across the line \(y=x\).
\item
  Show why \(T(x,y) = (x^2,y)\) is not linear.
\item
  Challenge: For the differentiation operator \(D: P_3 \to P_2\) on
  polynomials of degree ≤ 3, find its matrix relative to the basis
  \(\{1,x,x^2,x^3\}\) in the domain and \(\{1,x,x^2\}\) in the codomain.
\end{enumerate}

Linear transformations are the language of linear algebra. They capture
the essence of symmetry, motion, and structure in spaces of any kind,
making them indispensable for both theory and practice.

\subsection{42. Matrix Representation of a Linear
Map}\label{matrix-representation-of-a-linear-map}

Every linear transformation can be expressed concretely as a matrix.
This is one of the most powerful bridges in mathematics: it translates
abstract functional rules into arrays of numbers that can be calculated,
manipulated, and visualized.

\subsubsection{From Abstract Rule to Concrete
Numbers}\label{from-abstract-rule-to-concrete-numbers}

Suppose \(T: V \to W\) is a linear transformation between two
finite-dimensional vector spaces. To represent \(T\) as a matrix, we
first select bases:

\begin{itemize}
\tightlist
\item
  \(B = \{v_1, v_2, \dots, v_n\}\) for the domain \(V\).
\item
  \(C = \{w_1, w_2, \dots, w_m\}\) for the codomain \(W\).
\end{itemize}

For each basis vector \(v_j\), compute \(T(v_j)\). Each image \(T(v_j)\)
is a vector in \(W\), so it can be written as a combination of the basis
\(C\):

\[
T(v_j) = a_{1j}w_1 + a_{2j}w_2 + \dots + a_{mj}w_m.
\]

The coefficients \((a_{1j}, a_{2j}, \dots, a_{mj})\) become the j-th
column of the matrix representing \(T\).

Thus, the matrix of \(T\) relative to bases \(B\) and \(C\) is

\[
[T]_{B \to C} = \begin{bmatrix} a_{11} & a_{12} & \dots & a_{1n} \\ a_{21} & a_{22} & \dots & a_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{m1} & a_{m2} & \dots & a_{mn} \end{bmatrix}.
\]

This guarantees that for any vector \(x\) in coordinates relative to
\(B\),

\[
[T(x)]_C = [T]_{B \to C}[x]_B.
\]

\subsubsection{Standard Basis Case}\label{standard-basis-case}

When both \(B\) and \(C\) are the standard bases, the process
simplifies:

\begin{itemize}
\tightlist
\item
  Take \(T(e_1), T(e_2), \dots, T(e_n)\).
\item
  Place them as columns in a matrix.
\end{itemize}

That matrix directly represents \(T\).

Example: Let \(T(x,y) = (2x+y, x-y)\).

\begin{itemize}
\tightlist
\item
  \(T(e_1) = (2,1)\).
\item
  \(T(e_2) = (1,-1)\).
\end{itemize}

So the standard matrix is

\[
A = \begin{bmatrix} 2 & 1 \\ 1 & -1 \end{bmatrix}.
\]

For any vector \(\begin{bmatrix} x \\ y \end{bmatrix}\),

\[
T(x,y) = A \begin{bmatrix} x \\ y \end{bmatrix}.
\]

\subsubsection{Multiple Perspectives}\label{multiple-perspectives}

\begin{itemize}
\tightlist
\item
  Columns-as-images: Each column shows where a basis vector goes.
\item
  Row view: Each row encodes how to compute one coordinate of the
  output.
\item
  Operator view: The matrix acts like a machine: input vector → multiply
  → output vector.
\end{itemize}

\subsubsection{Geometric Insight}\label{geometric-insight}

Matrices reshape space. In \(\mathbb{R}^2\):

\begin{itemize}
\tightlist
\item
  The first column shows where the x-axis goes.
\item
  The second column shows where the y-axis goes. The entire grid is
  determined by these two images.
\end{itemize}

In \(\mathbb{R}^3\), the three columns are the images of the unit
coordinate directions, defining how the whole space twists, rotates, or
compresses.

\subsubsection{Everyday Analogies}\label{everyday-analogies-38}

\begin{itemize}
\tightlist
\item
  Translation dictionary: The abstract transformation is like a language
  teacher; the matrix is the bilingual dictionary that makes every
  translation calculable.
\item
  Blueprint: The linear rule is the design; the matrix is the
  instruction sheet that workers can follow step by step.
\item
  Mixing recipe: Each column tells you exactly how much of each
  ``ingredient'' (basis vector) contributes to the new mixture.
\end{itemize}

\subsubsection{Applications}\label{applications-5}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Computer graphics: Rotations, scaling, and projections are represented
  by small matrices.
\item
  Robotics: Coordinate changes between joints and workspaces rely on
  transformation matrices.
\item
  Data science: Linear maps such as PCA are implemented with matrices
  that project data into lower dimensions.
\item
  Physics: Linear operators like rotations, boosts, and stress tensors
  are matrix representations.
\end{enumerate}

\subsubsection{Why It Matters}\label{why-it-matters-38}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Matrices are computational tools: we can add, multiply, invert them.
\item
  They let us use algorithms like Gaussian elimination, LU/QR/SVD to
  study transformations.
\item
  They link abstract vector space theory to hands-on numerical
  calculation.
\item
  They reveal the structure of transformations at a glance, just by
  inspecting columns and rows.
\end{enumerate}

\subsubsection{Try It Yourself}\label{try-it-yourself-41}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Find the matrix for the transformation \(T(x,y,z) = (x+2y, y+z, x+z)\)
  in the standard basis.
\item
  Compute the matrix of \(T: \mathbb{R}^2 \to \mathbb{R}^2\), where
  \(T(x,y) = (x-y, x+y)\).
\item
  Using the basis \(B=\{(1,1), (1,-1)\}\) for \(\mathbb{R}^2\), find the
  matrix of \(T(x,y) = (2x, y)\) relative to \(B\).
\item
  Challenge: Show that matrix multiplication corresponds to composition
  of transformations, i.e.~\([S \circ T] = [S][T]\).
\end{enumerate}

Matrix representations are the practical form of linear transformations,
turning elegant definitions into something we can compute, visualize,
and apply across science and engineering.

\subsection{43. Kernel and Image}\label{kernel-and-image}

Every linear transformation hides two essential structures: the set of
vectors that collapse to zero, and the set of all possible outputs.
These are called the kernel and the image. They are the DNA of a linear
map, revealing its internal structure, its strengths, and its
limitations.

\subsubsection{The Kernel}\label{the-kernel}

The kernel (or null space) of a linear transformation \(T: V \to W\) is
defined as:

\[
\ker(T) = \{ v \in V : T(v) = 0 \}.
\]

\begin{itemize}
\tightlist
\item
  It is the set of all vectors that the transformation sends to the zero
  vector.
\item
  It measures how much information is ``lost'' under the transformation.
\item
  The kernel is always a subspace of the domain \(V\).
\end{itemize}

Examples:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  For \(T: \mathbb{R}^2 \to \mathbb{R}^2\), \(T(x,y) = (x,0)\).

  \begin{itemize}
  \tightlist
  \item
    Kernel: all vectors of the form \((0,y)\). This is the y-axis.
  \end{itemize}
\item
  For \(T: \mathbb{R}^3 \to \mathbb{R}^2\), \(T(x,y,z) = (x,y)\).

  \begin{itemize}
  \tightlist
  \item
    Kernel: all vectors of the form \((0,0,z)\). This is the z-axis.
  \end{itemize}
\end{enumerate}

The kernel tells us which directions in the domain vanish under \(T\).

\subsubsection{The Image}\label{the-image}

The image (or range) of a linear transformation is defined as:

\[
\text{im}(T) = \{ T(v) : v \in V \}.
\]

\begin{itemize}
\tightlist
\item
  It is the set of all vectors that can actually be reached by applying
  \(T\).
\item
  It describes the ``output space'' of the transformation.
\item
  The image is always a subspace of the codomain \(W\).
\end{itemize}

Examples:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  For \(T(x,y) = (x,0)\):

  \begin{itemize}
  \tightlist
  \item
    Image: all vectors of the form \((a,0)\). This is the x-axis.
  \end{itemize}
\item
  For \(T(x,y,z) = (x+y, y+z)\):

  \begin{itemize}
  \tightlist
  \item
    Image: all of \(\mathbb{R}^2\). Any vector \((u,v)\) can be achieved
    by solving equations for \((x,y,z)\).
  \end{itemize}
\end{enumerate}

\subsubsection{Kernel and Image
Together}\label{kernel-and-image-together}

These two subspaces reflect two aspects of \(T\):

\begin{itemize}
\tightlist
\item
  The kernel measures the collapse in dimension.
\item
  The image measures the preserved and transmitted directions.
\end{itemize}

A central result is the Rank--Nullity Theorem:

\[
\dim(\ker T) + \dim(\text{im }T) = \dim(V).
\]

\begin{itemize}
\tightlist
\item
  \(\dim(\ker T)\) is the nullity.
\item
  \(\dim(\text{im }T)\) is the rank.
\end{itemize}

This theorem guarantees a perfect balance: the domain splits into lost
directions (kernel) and active directions (image).

\subsubsection{Matrix View}\label{matrix-view}

For a matrix \(A\), the linear map is \(T(x) = Ax\).

\begin{itemize}
\tightlist
\item
  The kernel is the solution set of \(Ax = 0\).
\item
  The image is the column space of \(A\).
\end{itemize}

Example:

\[
A = \begin{bmatrix} 1 & 2 & 3 \\ 0 & 1 & 1 \end{bmatrix}.
\]

\begin{itemize}
\tightlist
\item
  Image: span of the columns
\end{itemize}

\[
\text{im}(A) = \text{span}\{ (1,0), (2,1), (3,1) \}.
\]

\begin{itemize}
\tightlist
\item
  Kernel: solve
\end{itemize}

\[
\begin{bmatrix} 1 & 2 & 3 \\ 0 & 1 & 1 \end{bmatrix} \begin{bmatrix} x \\ y \\ z \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \end{bmatrix}.
\]

This leads to solutions like \(x=-y-2z\). So the kernel is
1-dimensional, the image is 2-dimensional, and the domain (3D) splits as
\(1+2=3\).

\subsubsection{Geometric Intuition}\label{geometric-intuition-3}

\begin{itemize}
\tightlist
\item
  The kernel is the set of invisible directions, like shadows
  disappearing in projection.
\item
  The image is the set of all shadows that can appear.
\item
  Together they describe projection, flattening, stretching, or
  collapsing.
\end{itemize}

Example: Projecting \(\mathbb{R}^3\) onto the xy-plane:

\begin{itemize}
\tightlist
\item
  Kernel: the z-axis (all points collapsed to zero height).
\item
  Image: the entire xy-plane (all possible shadows).
\end{itemize}

\subsubsection{Everyday Analogies}\label{everyday-analogies-39}

\begin{itemize}
\tightlist
\item
  Camera lens: The kernel is the lost depth when turning 3D scenes into
  2D images; the image is the actual picture captured.
\item
  Compression: A lossy file compressor sends some details (kernel) to
  oblivion, but keeps the rest (image).
\item
  Business: A company's hiring system transforms applicants into
  employees. The kernel is rejected candidates; the image is those who
  make it through.
\end{itemize}

\subsubsection{Applications}\label{applications-6}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Solving equations: Kernel describes all solutions to \(Ax=0\). Image
  describes what right-hand sides \(b\) make \(Ax=b\) solvable.
\item
  Data science: Nullity corresponds to redundant features; rank
  corresponds to useful independent features.
\item
  Physics: In mechanics, symmetries often form the kernel of a
  transformation, while observable quantities form the image.
\item
  Control theory: The kernel and image determine controllability and
  observability of systems.
\end{enumerate}

\subsubsection{Why It Matters}\label{why-it-matters-39}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Kernel and image classify transformations into invertible or not.
\item
  They give a precise language to describe dimension changes.
\item
  They are the foundation of rank, nullity, and invertibility.
\item
  They generalize far beyond matrices: to polynomials, functions,
  operators, and differential equations.
\end{enumerate}

\subsubsection{Try It Yourself}\label{try-it-yourself-42}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute the kernel and image of \(T(x,y,z) = (x+y, y+z)\).
\item
  For the projection \(T(x,y,z) = (x,y,0)\), identify kernel and image.
\item
  Show that if the kernel is trivial (\(\{0\}\)), then the
  transformation is injective.
\item
  Challenge: Prove the rank--nullity theorem for a \(3\times 3\) matrix
  by working through examples.
\end{enumerate}

The kernel and image are the twin lenses through which linear
transformations are understood. One tells us what disappears, the other
what remains. Together, they give the clearest picture of a
transformation's essence.

\subsection{44. Invertibility and
Isomorphisms}\label{invertibility-and-isomorphisms}

Linear transformations come in many forms: some collapse space into
lower dimensions, others stretch it, and a special group preserves all
information perfectly. These special transformations are invertible,
meaning they can be reversed exactly. When two vector spaces are related
by such a transformation, we say they are isomorphic-structurally
identical, even if they look different on the surface.

\subsubsection{Invertibility of Linear
Transformations}\label{invertibility-of-linear-transformations}

A linear transformation \(T: V \to W\) is invertible if there exists
another linear transformation \(S: W \to V\) such that:

\[
S \circ T = I_V \quad \text{and} \quad T \circ S = I_W,
\]

where \(I_V\) and \(I_W\) are identity maps on \(V\) and \(W\).

\begin{itemize}
\tightlist
\item
  \(S\) is called the inverse of \(T\).
\item
  If such an inverse exists, \(T\) is a bijection: both one-to-one
  (injective) and onto (surjective).
\item
  In finite-dimensional spaces, this is equivalent to saying that \(T\)
  is represented by an invertible matrix.
\end{itemize}

\subsubsection{Invertible Matrices}\label{invertible-matrices}

An \(n \times n\) matrix \(A\) is invertible if there exists another
\(n \times n\) matrix \(A^{-1}\) such that:

\[
AA^{-1} = A^{-1}A = I.
\]

Characterizations of Invertibility:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(A\) is invertible ⇔ \(\det(A) \neq 0\).
\item
  ⇔ Columns of \(A\) are linearly independent.
\item
  ⇔ Columns of \(A\) span \(\mathbb{R}^n\).
\item
  ⇔ Rank of \(A\) is \(n\).
\item
  ⇔ The system \(Ax=b\) has exactly one solution for every \(b\).
\end{enumerate}

All these properties tie together: invertibility means no information is
lost when transforming vectors.

\subsubsection{Isomorphisms of Vector
Spaces}\label{isomorphisms-of-vector-spaces}

Two vector spaces \(V\) and \(W\) are isomorphic if there exists a
bijective linear transformation \(T: V \to W\).

\begin{itemize}
\item
  This means \(V\) and \(W\) are ``the same'' in structure, though they
  may look different.
\item
  For finite-dimensional spaces:

  \[
  V \cong W \quad \text{if and only if} \quad \dim(V) = \dim(W).
  \]
\item
  Example: \(\mathbb{R}^2\) and the set of all polynomials of degree ≤ 1
  are isomorphic, because both have dimension 2.
\end{itemize}

\subsubsection{Examples of
Invertibility}\label{examples-of-invertibility}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Rotation in the plane: Every rotation matrix has an inverse (rotation
  by the opposite angle).

  \[
  R(\theta) = \begin{bmatrix} \cos\theta & -\sin\theta \\ \sin\theta & \cos\theta \end{bmatrix}, \quad R(\theta)^{-1} = R(-\theta).
  \]
\item
  Scaling by nonzero factor: \(T(x) = ax\) with \(a \neq 0\). Inverse is
  \(T^{-1}(x) = \tfrac{1}{a}x\).
\item
  Projection onto a line: Not invertible, because depth is lost. The
  kernel is nontrivial.
\item
  Differentiation on polynomials of degree ≤ n: Not invertible, since
  constant terms vanish in the kernel.
\item
  Differentiation on exponential functions: Invertible: the inverse is
  integration (up to constants).
\end{enumerate}

\subsubsection{Geometric
Interpretation}\label{geometric-interpretation-8}

\begin{itemize}
\tightlist
\item
  Invertible transformations preserve dimension: no flattening or
  collapsing occurs.
\item
  They may rotate, shear, stretch, or reflect, but every input vector
  can be uniquely recovered.
\item
  The determinant tells the ``volume scaling'' of the transformation:
  invertibility requires this volume not to collapse to zero.
\end{itemize}

\subsubsection{Everyday Analogies}\label{everyday-analogies-40}

\begin{itemize}
\tightlist
\item
  Secret codes: An invertible transformation is like an encryption that
  can be decrypted; non-invertible codes lose information permanently.
\item
  Recipes: Doubling a recipe is invertible (just halve it back); baking
  a cake is not invertible (you can't get raw ingredients back).
\item
  Translations: Moving every point on a map 5 units east is invertible
  (move 5 units west to undo). Flattening a 3D globe into a 2D map loses
  depth and is not invertible.
\end{itemize}

\subsubsection{Applications}\label{applications-7}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Computer graphics: Invertible matrices allow smooth transformations
  where no information is lost. Non-invertible maps (like projections)
  create 2D renderings from 3D worlds.
\item
  Cryptography: Encryption systems rely on invertible linear maps for
  encoding/decoding.
\item
  Robotics: Transformations between joint and workspace coordinates must
  often be invertible for precise control.
\item
  Data science: PCA often reduces dimension (non-invertible), but
  whitening transformations are invertible within the chosen subspace.
\item
  Physics: Coordinate changes (e.g., Galilean or Lorentz
  transformations) are invertible, ensuring that physical laws remain
  consistent.
\end{enumerate}

\subsubsection{Why It Matters}\label{why-it-matters-40}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Invertible maps preserve the entire structure of a vector space.
\item
  They classify vector spaces: if two have the same dimension, they are
  fundamentally the same via isomorphism.
\item
  They allow reversible modeling, essential in physics, cryptography,
  and computation.
\item
  They highlight the delicate balance between lossless transformations
  (invertible) and lossy ones (non-invertible).
\end{enumerate}

\subsubsection{Try It Yourself}\label{try-it-yourself-43}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Prove that the matrix \(\begin{bmatrix} 2 & 1 \\ 3 & 2 \end{bmatrix}\)
  is invertible by computing its determinant and its inverse.
\item
  Show that projection onto the x-axis in \(\mathbb{R}^2\) is not
  invertible. Identify its kernel.
\item
  Construct an explicit isomorphism between \(\mathbb{R}^3\) and the
  space of polynomials of degree ≤ 2.
\item
  Challenge: Prove that if \(T\) is an isomorphism, then it maps bases
  to bases.
\end{enumerate}

Invertibility and isomorphism are the gateways from ``linear rules'' to
the grand idea of equivalence. They allow us to say, with mathematical
precision, when two spaces are truly the same in structure-different
clothes, same skeleton.

\subsection{45. Composition, Powers, and
Iteration}\label{composition-powers-and-iteration}

Linear transformations are not isolated operations-they can be combined,
repeated, and layered to build more complex effects. This leads us to
the ideas of composition, powers of transformations, and iteration.
These concepts form the backbone of linear dynamics, algorithms, and
many real-world systems where repeated actions accumulate into
surprising results.

\subsubsection{Composition of Linear
Transformations}\label{composition-of-linear-transformations}

If \(T: U \to V\) and \(S: V \to W\) are linear transformations, then
their composition is another transformation

\[
S \circ T : U \to W, \quad (S \circ T)(u) = S(T(u)).
\]

\begin{itemize}
\item
  Composition is associative:
  \((R \circ S) \circ T = R \circ (S \circ T)\).
\item
  Composition is linear: the result of composing two linear maps is
  still linear.
\item
  In terms of matrices, if \(T(x) = Ax\) and \(S(x) = Bx\), then

  \[
  (S \circ T)(x) = B(Ax) = (BA)x.
  \]

  Notice that the order matters: composition corresponds to matrix
  multiplication.
\end{itemize}

Example:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(T(x,y) = (x+2y, y)\).
\item
  \(S(x,y) = (2x, x-y)\). Then
  \((S \circ T)(x,y) = S(x+2y,y) = (2(x+2y), (x+2y)-y) = (2x+4y, x+y)\).
  Matrix multiplication confirms the same result.
\end{enumerate}

\subsubsection{Powers of
Transformations}\label{powers-of-transformations}

If \(T: V \to V\), we can apply it repeatedly:

\[
T^2 = T \circ T, \quad T^3 = T \circ T \circ T, \quad \dots
\]

\begin{itemize}
\tightlist
\item
  These are called powers of \(T\).
\item
  If \(T(x) = Ax\), then \(T^k(x) = A^k x\).
\item
  Powers of transformations capture repeated processes, like compounding
  interest, population growth, or iterative algorithms.
\end{itemize}

Example: Let \(T(x,y) = (2x, 3y)\). Then

\[
T^n(x,y) = (2^n x, 3^n y).
\]

Each iteration amplifies the scaling along different directions.

\subsubsection{Iteration and Dynamical
Systems}\label{iteration-and-dynamical-systems}

Iteration means applying the same transformation repeatedly to study
long-term behavior:

\[
x_{k+1} = T(x_k), \quad x_0 \text{ given}.
\]

\begin{itemize}
\tightlist
\item
  This creates a discrete dynamical system.
\item
  Depending on \(T\), vectors may grow, shrink, oscillate, or stabilize.
\end{itemize}

Example 1 (Markov Chains): If \(T\) is a stochastic matrix, iteration
describes probability evolution over time. Eventually, the system may
converge to a steady-state distribution.

Example 2 (Population Models): If \(T\) describes how sub-populations
interact, iteration simulates generations. Eigenvalues dictate whether
populations explode, stabilize, or vanish.

Example 3 (Computer Graphics): Repeated affine transformations create
fractals like the Sierpinski triangle.

\subsubsection{Stability and
Eigenvalues}\label{stability-and-eigenvalues}

The behavior of \(T^n(x)\) depends heavily on eigenvalues of the
transformation.

\begin{itemize}
\tightlist
\item
  If \(|\lambda| < 1\), repeated application shrinks vectors in that
  direction to zero.
\item
  If \(|\lambda| > 1\), repeated application causes exponential growth.
\item
  If \(|\lambda| = 1\), vectors rotate or oscillate without changing
  length.
\end{itemize}

This link between powers and eigenvalues underpins many algorithms in
numerical analysis and physics.

\subsubsection{Geometric
Interpretation}\label{geometric-interpretation-9}

\begin{itemize}
\tightlist
\item
  Composition = chaining geometric actions (rotate then reflect, scale
  then shear).
\item
  Powers = applying the same action repeatedly (rotating 90° four times
  = identity).
\item
  Iteration = exploring the ``orbit'' of a vector under repeated
  transformations.
\end{itemize}

\subsubsection{Everyday Analogies}\label{everyday-analogies-41}

\begin{itemize}
\tightlist
\item
  Assembly line: Each station performs a transformation. The entire
  process is the composition of all steps.
\item
  Compound interest: Repeated multiplication by a growth factor is
  iteration.
\item
  Dance choreography: One step repeated many times creates a rhythm-like
  powers of a transformation.
\item
  Social media algorithms: Iterative recommendations amplify some
  signals while suppressing others-much like eigenvalues controlling
  growth or decay.
\end{itemize}

\subsubsection{Applications}\label{applications-8}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Search engines: PageRank is computed by iterating a linear
  transformation until it stabilizes.
\item
  Economics: Input--output models iterate to predict long-term
  equilibrium of industries.
\item
  Physics: Time evolution of quantum states is modeled by repeated
  application of unitary operators.
\item
  Numerical methods: Iterative solvers (like power iteration)
  approximate eigenvectors.
\item
  Computer graphics: Iterated function systems generate self-similar
  fractals.
\end{enumerate}

\subsubsection{Why It Matters}\label{why-it-matters-41}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Composition unifies matrix multiplication and transformation chaining.
\item
  Powers reveal exponential growth, decay, and oscillation.
\item
  Iteration is the core of modeling dynamic processes in mathematics,
  science, and engineering.
\item
  The link to eigenvalues makes these ideas the foundation of stability
  analysis.
\end{enumerate}

\subsubsection{Try It Yourself}\label{try-it-yourself-44}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Let \(T(x,y) = (x+y, y)\). Compute \(T^2(x,y)\) and \(T^3(x,y)\). What
  happens as \(n \to \infty\)?
\item
  Consider rotation by 90° in \(\mathbb{R}^2\). Show that \(T^4 = I\).
\item
  For matrix
  \(A = \begin{bmatrix} 0.5 & 0.5 \\ 0.5 & 0.5 \end{bmatrix}\), iterate
  \(A^n\). What happens to arbitrary vectors?
\item
  Challenge: Prove that if \(A\) is diagonalizable as \(A = PDP^{-1}\),
  then \(A^n = PD^nP^{-1}\). Use this to analyze long-term behavior.
\end{enumerate}

Composition, powers, and iteration take linear algebra beyond static
equations into the world of processes over time. They explain how small,
repeated steps shape long-term outcomes-whether stabilizing systems,
amplifying signals, or creating infinite complexity.

\subsection{46. Similarity and
Conjugation}\label{similarity-and-conjugation}

In linear algebra, different matrices can represent the same underlying
transformation when written in different coordinate systems. This
relationship is captured by the idea of similarity. Two matrices are
similar if one is obtained from the other by a conjugation with an
invertible change-of-basis matrix. This concept is central to
understanding canonical forms, eigenvalue decompositions, and the deep
structure of linear operators.

\subsubsection{Definition of Similarity}\label{definition-of-similarity}

Two \(n \times n\) matrices \(A\) and \(B\) are called similar if there
exists an invertible matrix \(P\) such that:

\[
B = P^{-1}AP.
\]

\begin{itemize}
\tightlist
\item
  Here, \(P\) represents a change of basis.
\item
  \(A\) and \(B\) describe the same linear transformation, but expressed
  relative to different bases.
\end{itemize}

\subsubsection{Conjugation as Change of
Basis}\label{conjugation-as-change-of-basis}

Suppose \(T: V \to V\) is a linear transformation and \(A\) is its
matrix in basis \(B\). If we switch to a new basis \(C\), the matrix
becomes \(B\). The conversion is:

\[
B = P^{-1}AP,
\]

where \(P\) is the change-of-basis matrix from basis \(B\) to basis
\(C\).

This shows that similarity is not just algebraic coincidence-it's
geometric: the operator is the same, but our perspective (basis) has
changed.

\subsubsection{Properties Preserved Under
Similarity}\label{properties-preserved-under-similarity}

If \(A\) and \(B\) are similar, they share many key properties:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Determinant: \(\det(A) = \det(B)\).
\item
  Trace: \(\text{tr}(A) = \text{tr}(B)\).
\item
  Rank: \(\text{rank}(A) = \text{rank}(B)\).
\item
  Eigenvalues: Same set of eigenvalues (with multiplicity).
\item
  Characteristic polynomial: Identical.
\item
  Minimal polynomial: Identical.
\end{enumerate}

These invariants define the ``skeleton'' of a linear operator,
unaffected by coordinate changes.

\subsubsection{Examples}\label{examples-2}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Rotation in the plane: The matrix for rotation by 90° is

  \[
  A = \begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix}.
  \]

  In another basis, the rotation might be represented by a more
  complicated-looking matrix, but all such matrices are similar to
  \(A\).
\item
  Diagonalization: A matrix \(A\) is diagonalizable if it is similar to
  a diagonal matrix \(D\). That is,

  \[
  A = PDP^{-1}.
  \]

  Here, similarity reduces \(A\) to its simplest form.
\item
  Shear transformation: A shear matrix
  \(\begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix}\) is not
  diagonalizable, but it may be similar to a Jordan block.
\end{enumerate}

\subsubsection{Geometric
Interpretation}\label{geometric-interpretation-10}

\begin{itemize}
\tightlist
\item
  Similarity says: two matrices may look different, but they are ``the
  same'' transformation seen from different coordinate systems.
\item
  Conjugation is the mathematical act of relabeling coordinates.
\item
  Think of shifting your camera angle: the scene hasn't changed, only
  the perspective has.
\end{itemize}

\subsubsection{Everyday Analogies}\label{everyday-analogies-42}

\begin{itemize}
\tightlist
\item
  Translations of stories: The same plot can be told in different
  languages (bases). The words differ (matrices), but the story remains
  identical (transformation).
\item
  Maps and coordinates: The shape of a mountain range doesn't depend on
  whether we use latitude/longitude or UTM coordinates.
\item
  Software code: The same algorithm can be written in Python or
  C++-different symbols, same behavior.
\end{itemize}

\subsubsection{Applications}\label{applications-9}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Diagonalization: Reducing a matrix to diagonal form (when possible)
  uses similarity. This simplifies powers, exponentials, and iterative
  analysis.
\item
  Jordan canonical form: Every square matrix is similar to a Jordan
  form, giving a complete structural classification.
\item
  Quantum mechanics: Operators on state spaces often change
  representation, but similarity guarantees invariance of spectra.
\item
  Control theory: Canonical forms simplify analysis of system stability
  and controllability.
\item
  Numerical methods: Eigenvalue algorithms rely on repeated similarity
  transformations (e.g., QR algorithm).
\end{enumerate}

\subsubsection{Why It Matters}\label{why-it-matters-42}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Similarity reveals the true identity of a linear operator, independent
  of coordinates.
\item
  It allows simplification: many problems become easier in the right
  basis.
\item
  It preserves invariants, giving us tools to classify and compare
  operators.
\item
  It connects abstract algebra with concrete computations in geometry,
  physics, and engineering.
\end{enumerate}

\subsubsection{Try It Yourself}\label{try-it-yourself-45}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Show that \(\begin{bmatrix} 2 & 1 \\ 0 & 2 \end{bmatrix}\) is similar
  to \(\begin{bmatrix} 2 & 0 \\ 0 & 2 \end{bmatrix}\). Why or why not?
\item
  Compute \(P^{-1}AP\) for
  \(A = \begin{bmatrix} 1 & 2 \\ 0 & 1 \end{bmatrix}\) and
  \(P = \begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix}\). Interpret the
  result.
\item
  Prove that if two matrices are similar, they must have the same trace.
\item
  Challenge: Show that if \(A\) and \(B\) are similar, then \(A^k\) and
  \(B^k\) are also similar for all integers \(k \geq 0\).
\end{enumerate}

Similarity and conjugation elevate linear algebra from mere calculation
to structural understanding. They tell us when two seemingly different
matrices are just different ``faces'' of the same underlying
transformation.

\subsection{47. Projections and
Reflections}\label{projections-and-reflections}

Among the many transformations in linear algebra, two stand out for
their geometric clarity and practical importance: projections and
reflections. These operations reshape vectors in simple but powerful
ways, and they form the building blocks of algorithms in statistics,
optimization, graphics, and physics.

\subsubsection{Projection: Flattening onto a
Subspace}\label{projection-flattening-onto-a-subspace}

A projection is a linear transformation that takes a vector and drops it
onto a subspace, like casting a shadow.

Formally, if \(W\) is a subspace of \(V\), the projection of a vector
\(v\) onto \(W\) is the unique vector \(w \in W\) that is closest to
\(v\).

In \(\mathbb{R}^2\): projecting onto the x-axis takes \((x,y)\) and
produces \((x,0)\).

\paragraph{Orthogonal Projection
Formula}\label{orthogonal-projection-formula}

Suppose \(u\) is a nonzero vector. The projection of \(v\) onto the line
spanned by \(u\) is:

\[
\text{proj}_u(v) = \frac{v \cdot u}{u \cdot u} u.
\]

This formula works in any dimension. It uses the dot product to measure
how much of \(v\) points in the direction of \(u\).

Example: Project \((2,3)\) onto \(u=(1,1)\):

\[
\text{proj}_u(2,3) = \frac{(2,3)\cdot(1,1)}{(1,1)\cdot(1,1)} (1,1) = \frac{5}{2}(1,1) = (2.5,2.5).
\]

The vector \((2,3)\) splits into \((2.5,2.5)\) along the line plus
\((-0.5,0.5)\) orthogonal to it.

\paragraph{Projection Matrices}\label{projection-matrices}

For unit vector \(u\):

\[
P = uu^T
\]

is the projection matrix onto the span of \(u\).

For a general subspace with orthonormal basis columns in matrix \(Q\):

\[
P = QQ^T
\]

projects any vector onto that subspace.

Properties:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(P^2 = P\) (idempotent).
\item
  \(P^T = P\) (symmetric, for orthogonal projections).
\end{enumerate}

\subsubsection{Reflection: Flipping Across a
Subspace}\label{reflection-flipping-across-a-subspace}

A reflection takes a vector and flips it across a line or plane.
Geometrically, it's like a mirror.

Reflection across a line spanned by unit vector \(u\):

\[
R(v) = 2\text{proj}_u(v) - v.
\]

Matrix form:

\[
R = 2uu^T - I.
\]

Example: Reflect \((2,3)\) across the line \(y=x\). With
\(u=(1/\sqrt{2},1/\sqrt{2})\):

\[
R = \begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix}.
\]

So reflection swaps coordinates: \((2,3) \mapsto (3,2)\).

\subsubsection{Geometric Insight}\label{geometric-insight-1}

\begin{itemize}
\tightlist
\item
  Projection shortens vectors by removing components orthogonal to the
  subspace.
\item
  Reflection preserves length but flips orientation relative to the
  subspace.
\item
  Projection is about approximation (``closest point''), reflection is
  about symmetry.
\end{itemize}

\subsubsection{Everyday Analogies}\label{everyday-analogies-43}

\begin{itemize}
\tightlist
\item
  Projection: A flashlight casts the 3D shape of your hand onto a 2D
  wall.
\item
  Reflection: A mirror flips your left and right sides.
\item
  Statistics: Projection corresponds to regression-finding the best
  linear approximation of data.
\item
  Design: Reflection symmetry is everywhere in art and architecture.
\end{itemize}

\subsubsection{Applications}\label{applications-10}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Statistics \& Machine Learning: Least-squares regression is projection
  of data onto the span of predictor variables.
\item
  Computer Graphics: Projection transforms 3D scenes into 2D screen
  images. Reflections simulate mirrors and shiny surfaces.
\item
  Optimization: Projections enforce constraints by bringing guesses back
  into feasible regions.
\item
  Physics: Reflections describe wave behavior, optics, and particle
  interactions.
\item
  Numerical Methods: Projection operators are key to iterative
  algorithms (like Krylov subspace methods).
\end{enumerate}

\subsubsection{Why It Matters}\label{why-it-matters-43}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Projection captures the essence of approximation: keeping what fits,
  discarding what doesn't.
\item
  Reflection embodies symmetry and invariance, key to geometry and
  physics.
\item
  Both are linear, with elegant matrix representations.
\item
  They combine easily with other transformations, making them versatile
  in computation.
\end{enumerate}

\subsubsection{Try It Yourself}\label{try-it-yourself-46}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Find the projection matrix onto the line spanned by \((3,4)\). Verify
  it is idempotent.
\item
  Compute the reflection of \((1,2)\) across the x-axis.
\item
  Show that reflection matrices are orthogonal (\(R^T R = I\)).
\item
  Challenge: For subspace \(W\) with orthonormal basis \(Q\), derive the
  reflection matrix \(R = 2QQ^T - I\).
\end{enumerate}

Projections and reflections are two of the purest examples of how linear
transformations embody geometric ideas. One approximates, the other
symmetrizes-but both expose the deep structure of space through the lens
of linear algebra.

\subsection{48. Rotations and Shear}\label{rotations-and-shear}

Linear transformations can twist, turn, and distort space in strikingly
different ways. Two of the most fundamental examples are rotations-which
preserve lengths and angles while turning vectors-and shears-which slide
one part of space relative to another, distorting shape while often
preserving area. These two transformations form the geometric heart of
linear algebra, and they are indispensable in graphics, physics, and
engineering.

\subsubsection{Rotations in the Plane}\label{rotations-in-the-plane}

A rotation in \(\mathbb{R}^2\) by an angle \(\theta\) is defined as:

\[
R_\theta = \begin{bmatrix} \cos\theta & -\sin\theta \\ \sin\theta & \cos\theta \end{bmatrix}.
\]

For any vector \((x,y)\):

\[
R_\theta \begin{bmatrix} x \\ y \end{bmatrix} = \begin{bmatrix} x\cos\theta - y\sin\theta \\ x\sin\theta + y\cos\theta \end{bmatrix}.
\]

Properties:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Preserves lengths: \(\|R_\theta v\| = \|v\|\).
\item
  Preserves angles: the dot product is unchanged.
\item
  Determinant = \(+1\), so it preserves orientation and area.
\item
  Inverse: \(R_\theta^{-1} = R_{-\theta}\).
\end{enumerate}

Example: A 90° rotation:

\[
R_{90^\circ} = \begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix}, \quad (1,0) \mapsto (0,1).
\]

\subsubsection{Rotations in Three
Dimensions}\label{rotations-in-three-dimensions}

Rotations in \(\mathbb{R}^3\) occur around an axis. For example,
rotation by angle \(\theta\) around the z-axis:

\[
R_z(\theta) = \begin{bmatrix} \cos\theta & -\sin\theta & 0 \\ \sin\theta & \cos\theta & 0 \\ 0 & 0 & 1 \end{bmatrix}.
\]

\begin{itemize}
\tightlist
\item
  Leaves the z-axis fixed.
\item
  Rotates the xy-plane like a 2D rotation.
\end{itemize}

General rotations in 3D are described by orthogonal matrices with
determinant +1, forming the group \(SO(3)\).

\subsubsection{Shear Transformations}\label{shear-transformations}

A shear slides one coordinate direction while keeping another fixed,
distorting shapes.

In \(\mathbb{R}^2\):

\[
S = \begin{bmatrix} 1 & k \\ 0 & 1 \end{bmatrix} \quad \text{or} \quad \begin{bmatrix} 1 & 0 \\ k & 1 \end{bmatrix}.
\]

\begin{itemize}
\tightlist
\item
  The first form ``slides'' x-coordinates depending on y.
\item
  The second form slides y-coordinates depending on x.
\end{itemize}

Example:

\[
S = \begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix}, \quad (x,y) \mapsto (x+y, y).
\]

\begin{itemize}
\tightlist
\item
  Squares become parallelograms.
\item
  Areas are preserved if \(\det(S) = 1\).
\end{itemize}

In \(\mathbb{R}^3\): shears distort volumes while preserving parallelism
of faces.

\subsubsection{Geometric Comparison}\label{geometric-comparison}

\begin{itemize}
\tightlist
\item
  Rotation: Preserves size and shape exactly, only changes orientation.
  Circles remain circles.
\item
  Shear: Distorts shape but often preserves area (in 2D) or volume (in
  3D). Circles become ellipses or slanted figures.
\end{itemize}

Together, rotations and shears can generate a vast variety of linear
distortions.

\subsubsection{Everyday Analogies}\label{everyday-analogies-44}

\begin{itemize}
\tightlist
\item
  Rotation: The motion of a spinning wheel, turning a camera, or
  rotating an image on your phone.
\item
  Shear: Sliding cards in a deck so the top shifts relative to the
  bottom, or slanting a stack of books without changing their height.
\item
  Combination: Buildings in perspective drawings are sheared versions of
  rectangles, while wheels rotate cleanly.
\end{itemize}

\subsubsection{Applications}\label{applications-11}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Computer Graphics: Rotations orient objects; shears simulate
  perspective.
\item
  Engineering: Shear stresses deform materials; rotations model
  rigid-body motion.
\item
  Robotics: Rotations define arm orientation; shears approximate local
  deformations.
\item
  Physics: Rotations are symmetries of space; shears appear in fluid
  flows and elasticity.
\item
  Data Science: Shears represent changes of variables that preserve
  volume but distort distributions.
\end{enumerate}

\subsubsection{Why It Matters}\label{why-it-matters-44}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Rotations model pure symmetry-no distortion, just reorientation.
\item
  Shears show how geometry can be distorted while preserving volume or
  area.
\item
  Both are building blocks: any invertible matrix in \(\mathbb{R}^2\)
  can be factored into rotations, shears, and scalings.
\item
  They bridge algebra and geometry, giving visual meaning to abstract
  matrices.
\end{enumerate}

\subsubsection{Try It Yourself}\label{try-it-yourself-47}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Rotate \((1,0)\) by 60° and compute the result explicitly.
\item
  Apply the shear \(S=\begin{bmatrix} 1 & 2 \\ 0 & 1 \end{bmatrix}\) to
  the square with vertices \((0,0),(1,0),(0,1),(1,1)\). What shape
  results?
\item
  Show that rotation matrices are orthogonal (\(R^TR=I\)).
\item
  Challenge: Prove that any area-preserving \(2\times2\) matrix with
  determinant 1 can be decomposed into a product of rotations and
  shears.
\end{enumerate}

Rotations and shears highlight two complementary sides of linear
algebra: symmetry versus distortion. Together, they show how
transformations can either preserve the essence of space or bend it into
new shapes while keeping its structure intact.

\subsection{49. Rank and Operator
Viewpoint}\label{rank-and-operator-viewpoint}

The rank of a linear transformation or matrix is one of the most
important measures of its power. It captures how many independent
directions a transformation preserves, how much information it carries
from input to output, and how ``full'' its action on space is. Thinking
of rank not just as a number, but as a description of an operator, gives
us a clearer picture of what transformations really do.

\subsubsection{Definition of Rank}\label{definition-of-rank-1}

For a matrix \(A\) representing a linear transformation \(T: V \to W\):

\[
\text{rank}(A) = \dim(\text{im}(A)) = \dim(\text{im}(T)).
\]

That is, the rank is the dimension of the image (or column space). It
counts the maximum number of linearly independent columns.

\subsubsection{Basic Properties}\label{basic-properties}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \(\text{rank}(A) \leq \min(m,n)\) for an \(m \times n\) matrix.
\item
  \(\text{rank}(A) = \text{rank}(A^T)\).
\item
  Rank is equal to the number of pivot columns in row-reduced form.
\item
  Rank links directly with nullity via the rank--nullity theorem:

  \[
  \text{rank}(A) + \text{nullity}(A) = n.
  \]
\end{enumerate}

\subsubsection{Operator Perspective}\label{operator-perspective}

Instead of focusing on rows and columns, imagine rank as a measure of
how much of the domain is transmitted faithfully to the codomain.

\begin{itemize}
\tightlist
\item
  If rank = full (\(n\)), the transformation is injective: nothing
  collapses.
\item
  If rank = dimension of codomain (\(m\)), the transformation is
  surjective: every target vector can be reached.
\item
  If rank is smaller, the transformation compresses space: parts of the
  domain are ``invisible'' and collapse into the kernel.
\end{itemize}

Example 1 (Projection): Projection from \(\mathbb{R}^3\) onto the
xy-plane has rank 2. It annihilates the z-direction but preserves two
independent directions.

Example 2 (Rotation): Rotation in \(\mathbb{R}^2\) has rank 2. No
directions are lost.

Example 3 (Zero map): The transformation sending everything to zero has
rank 0.

\subsubsection{Geometric Meaning}\label{geometric-meaning-7}

\begin{itemize}
\tightlist
\item
  Rank = number of independent directions preserved.
\item
  A rank-1 transformation maps all of space onto a single line.
\item
  Rank-2 in \(\mathbb{R}^3\) maps space onto a plane.
\item
  Rank-full maps space onto its entire dimension without collapse.
\end{itemize}

Visually: rank describes the ``dimensional thickness'' of the image.

\subsubsection{Rank and Matrix
Factorizations}\label{rank-and-matrix-factorizations}

Rank reveals hidden structure:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  LU factorization: Rank determines the number of nonzero pivots.
\item
  QR factorization: Rank controls the number of orthogonal directions.
\item
  SVD (Singular Value Decomposition): The number of nonzero singular
  values equals the rank.
\end{enumerate}

SVD in particular gives a geometric operator view: each nonzero singular
value corresponds to a preserved dimension, while zeros indicate
collapsed directions.

\subsubsection{Rank in Applications}\label{rank-in-applications}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Data compression: Low-rank approximations reduce storage (e.g., image
  compression with SVD).
\item
  Statistics: Rank of the design matrix determines identifiability of
  regression coefficients.
\item
  Machine learning: Rank of weight matrices controls expressive power of
  models.
\item
  Control theory: Rank conditions ensure controllability and
  observability of systems.
\item
  Network analysis: Rank of adjacency or Laplacian matrices reflects
  connectivity of graphs.
\end{enumerate}

\subsubsection{Rank Deficiency}\label{rank-deficiency}

If a transformation has less than full rank, it is rank-deficient. This
means:

\begin{itemize}
\tightlist
\item
  Some directions are lost (kernel nontrivial).
\item
  Some outputs are unreachable (image smaller than codomain).
\item
  Equations \(Ax=b\) may be inconsistent or underdetermined.
\end{itemize}

Detecting and handling rank deficiency is crucial in numerical linear
algebra, where ill-conditioning can hide in nearly dependent columns.

\subsubsection{Everyday Analogies}\label{everyday-analogies-45}

\begin{itemize}
\tightlist
\item
  Photography: A 3D scene projected to 2D loses depth: rank drops from 3
  to 2.
\item
  Business pipeline: If only 2 of 5 departments actually influence
  profits, the effective ``rank'' is 2.
\item
  Conversation: If two people keep repeating the same idea, they don't
  add rank-the discussion's dimension hasn't increased.
\end{itemize}

\subsubsection{Why It Matters}\label{why-it-matters-45}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Rank measures the true dimensional effect of a transformation.
\item
  It distinguishes between full-strength operators and those that
  collapse information.
\item
  It connects row space, column space, image, and kernel under one
  number.
\item
  It underpins algorithms for regression, decomposition, and
  dimensionality reduction.
\end{enumerate}

\subsubsection{Try It Yourself}\label{try-it-yourself-48}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Find the rank of
  \(\begin{bmatrix} 1 & 2 & 3 \\ 2 & 4 & 6 \end{bmatrix}\). Why is it
  less than 2?
\item
  Describe geometrically the image of a rank-1 transformation in
  \(\mathbb{R}^3\).
\item
  For a \(5 \times 5\) diagonal matrix with diagonal entries
  \((2,0,3,0,5)\), compute rank and nullity.
\item
  Challenge: Show that for any matrix \(A\), the rank equals the number
  of nonzero singular values of \(A\).
\end{enumerate}

Rank tells us not just how many independent vectors survive a
transformation, but also how much structure the operator truly
preserves. It is the bridge between abstract linear maps and their
practical power.

\subsection{50. Block Matrices and Block
Maps}\label{block-matrices-and-block-maps}

As problems grow in size, matrices become large and difficult to manage
element by element. A powerful strategy is to organize matrices into
blocks-submatrices grouped together like tiles in a mosaic. This allows
us to treat large transformations as compositions of smaller, more
understandable ones. Block matrices preserve structure, simplify
computations, and reveal deep insights into how transformations act on
subspaces.

\subsubsection{What Are Block Matrices?}\label{what-are-block-matrices}

A block matrix partitions a matrix into rectangular submatrices. Each
block is itself a matrix, and the entire matrix can be manipulated using
block rules.

Example: a \(4 \times 4\) matrix divided into four \(2 \times 2\)
blocks:

\[
A = \begin{bmatrix} 
A_{11} & A_{12} \\ 
A_{21} & A_{22} 
\end{bmatrix},
\]

where each \(A_{ij}\) is \(2 \times 2\).

Instead of thinking in terms of 16 entries, we work with 4 blocks.

\subsubsection{Block Maps as Linear
Transformations}\label{block-maps-as-linear-transformations}

Suppose \(V = V_1 \oplus V_2\) is decomposed into two subspaces. A
linear map \(T: V \to V\) can be described in terms of how it acts on
each component. Relative to this decomposition, the matrix of \(T\) has
block form:

\[
[T] = \begin{bmatrix} 
T_{11} & T_{12} \\ 
T_{21} & T_{22} 
\end{bmatrix}.
\]

\begin{itemize}
\tightlist
\item
  \(T_{11}\): how \(V_1\) maps into itself.
\item
  \(T_{12}\): how \(V_2\) contributes to \(V_1\).
\item
  \(T_{21}\): how \(V_1\) contributes to \(V_2\).
\item
  \(T_{22}\): how \(V_2\) maps into itself.
\end{itemize}

This decomposition highlights how subspaces interact under the
transformation.

\subsubsection{Block Matrix Operations}\label{block-matrix-operations}

Block matrices obey the same rules as normal matrices, but operations
are done block by block.

Addition:

\[
\begin{bmatrix} A & B \\ C & D \end{bmatrix} + 
\begin{bmatrix} E & F \\ G & H \end{bmatrix} =
\begin{bmatrix} A+E & B+F \\ C+G & D+H \end{bmatrix}.
\]

Multiplication:

\[
\begin{bmatrix} A & B \\ C & D \end{bmatrix}
\begin{bmatrix} E & F \\ G & H \end{bmatrix} =
\begin{bmatrix} AE+BG & AF+BH \\ CE+DG & CF+DH \end{bmatrix}.
\]

The formulas look like ordinary multiplication, but each term is itself
a product of submatrices.

\subsubsection{Special Block Structures}\label{special-block-structures}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Block Diagonal Matrices:

  \[
  \begin{bmatrix} A & 0 \\ 0 & D \end{bmatrix}.
  \]

  Independent actions on subspaces-no mixing between them.
\item
  Block Upper Triangular:

  \[
  \begin{bmatrix} A & B \\ 0 & D \end{bmatrix}.
  \]

  Subspace \(V_1\) influences \(V_2\), but not vice versa.
\item
  Block Symmetric: If overall matrix is symmetric, so are certain block
  relationships: \(A^T=A, D^T=D, B^T=C\).
\end{enumerate}

These structures appear naturally in decomposition and iterative
algorithms.

\subsubsection{Block Matrix Inverses}\label{block-matrix-inverses}

Some block matrices can be inverted using special formulas. For

\[
M = \begin{bmatrix} A & B \\ C & D \end{bmatrix},
\]

if \(A\) is invertible, the inverse can be expressed using the Schur
complement:

\[
M^{-1} = \begin{bmatrix} 
A^{-1} + A^{-1}B(D-CA^{-1}B)^{-1}CA^{-1} & -A^{-1}B(D-CA^{-1}B)^{-1} \\ 
-(D-CA^{-1}B)^{-1}CA^{-1} & (D-CA^{-1}B)^{-1} 
\end{bmatrix}.
\]

This formula is central in statistics, optimization, and numerical
analysis.

\subsubsection{Geometric
Interpretation}\label{geometric-interpretation-11}

\begin{itemize}
\tightlist
\item
  A block diagonal matrix acts like two independent transformations
  operating side by side.
\item
  A block triangular matrix shows a ``hierarchy'': one subspace
  influences the other but not the reverse.
\item
  This decomposition mirrors how systems can be separated into smaller
  interacting parts.
\end{itemize}

\subsubsection{Everyday Analogies}\label{everyday-analogies-46}

\begin{itemize}
\tightlist
\item
  Organizations: Departments (subspaces) with internal operations
  (diagonal blocks) and communication between them (off-diagonal
  blocks).
\item
  Cities: Neighborhoods evolve independently (diagonal blocks) but also
  exchange resources (off-diagonal blocks).
\item
  Software systems: Independent modules with interfaces to exchange
  data.
\end{itemize}

\subsubsection{Applications}\label{applications-12}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Numerical Linear Algebra: Block operations optimize computation on
  large sparse matrices.
\item
  Control Theory: State-space models are naturally expressed in block
  form.
\item
  Statistics: Partitioned covariance matrices rely on block inversion
  formulas.
\item
  Machine Learning: Neural networks layer transformations, often
  structured into blocks for efficiency.
\item
  Parallel Computing: Block decomposition distributes large matrix
  problems across processors.
\end{enumerate}

\subsubsection{Why It Matters}\label{why-it-matters-46}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Block matrices turn big problems into manageable smaller ones.
\item
  They reflect natural decompositions of systems into interacting parts.
\item
  They make explicit the geometry of subspace interactions.
\item
  They provide efficient algorithms, especially for large-scale
  scientific computing.
\end{enumerate}

\subsubsection{Try It Yourself}\label{try-it-yourself-49}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Multiply two \(4 \times 4\) matrices written as \(2 \times 2\) block
  matrices and confirm the block multiplication rule.
\item
  Write the projection matrix onto a 2D subspace in \(\mathbb{R}^4\)
  using block form.
\item
  Compute the Schur complement of

  \[
  \begin{bmatrix} 2 & 1 \\ 1 & 3 \end{bmatrix}.
  \]
\item
  Challenge: Show that the determinant of a block triangular matrix
  equals the product of the determinants of its diagonal blocks.
\end{enumerate}

Block matrices and block maps show how complexity can be organized.
Instead of drowning in thousands of entries, we see structure,
interaction, and hierarchy-revealing how large systems can be built from
simple linear pieces.

\subsubsection{Closing}\label{closing-4}

\begin{verbatim}
Shadows twist and turn,
kernels hide and images flow,
form remains within.
\end{verbatim}

\section{Chapter 6. Determinants and
volume}\label{chapter-6.-determinants-and-volume-1}

\subsubsection{Opening}\label{opening-4}

\begin{verbatim}
Areas unfold,
parallels stretch into waves,
scale whispers in signs.
\end{verbatim}

\subsection{51. Areas, Volumes, and Signed Scale
Factors}\label{areas-volumes-and-signed-scale-factors}

Determinants often feel like an abstract formula until we see their
geometric meaning: they measure area in 2D, volume in 3D, and, in higher
dimensions, the general ``size'' of a transformed shape. Even more,
determinants encode whether orientation is preserved or flipped, giving
them a ``signed'' interpretation. This perspective transforms
determinants from algebraic curiosities into geometric tools.

\subsubsection{Transformations and Scaling of
Space}\label{transformations-and-scaling-of-space}

Consider a linear transformation \(T: \mathbb{R}^n \to \mathbb{R}^n\)
represented by a square matrix \(A\). When \(A\) acts on vectors, it
reshapes space: it stretches, compresses, rotates, reflects, or shears
regions.

\begin{itemize}
\tightlist
\item
  If you apply \(A\) to a unit square in \(\mathbb{R}^2\), the image is
  a parallelogram.
\item
  If you apply \(A\) to a unit cube in \(\mathbb{R}^3\), the image is a
  parallelepiped.
\item
  In general, the determinant of \(A\) tells us how the measure (area,
  volume, hyper-volume) of the shape has changed.
\end{itemize}

\subsubsection{Determinant as Signed Scale
Factor}\label{determinant-as-signed-scale-factor}

\begin{itemize}
\tightlist
\item
  \(|\det(A)|\) = the scale factor for areas (2D), volumes (3D), or
  n-dimensional content.
\item
  If \(\det(A) = 0\), the transformation collapses space into a lower
  dimension, flattening all volume away.
\item
  If \(\det(A) > 0\), the orientation of space is preserved.
\item
  If \(\det(A) < 0\), the orientation is flipped (like a reflection in a
  mirror).
\end{itemize}

Thus, determinants are not just numbers-they carry both magnitude and
sign, telling us about size and handedness.

\subsubsection{2D Case: Area of
Parallelogram}\label{d-case-area-of-parallelogram}

Take two column vectors \(u,v \in \mathbb{R}^2\). Place them as columns
in a matrix:

\[
A = \begin{bmatrix} u & v \end{bmatrix}.
\]

The absolute value of the determinant gives the area of the
parallelogram spanned by \(u\) and \(v\):

\[
\text{Area} = |\det(A)|.
\]

Example:

\[
A = \begin{bmatrix} 2 & 1 \\ 1 & 3 \end{bmatrix}.
\]

Then \(\det(A) = (2)(3) - (1)(1) = 5\). The unit square maps to a
parallelogram of area 5.

\subsubsection{3D Case: Volume of
Parallelepiped}\label{d-case-volume-of-parallelepiped}

For three vectors \(u,v,w \in \mathbb{R}^3\), form a matrix

\[
A = \begin{bmatrix} u & v & w \end{bmatrix}.
\]

Then the absolute determinant gives the volume of the parallelepiped:

\[
\text{Volume} = |\det(A)|.
\]

Geometrically, this is the scalar triple product:

\[
\det(A) = u \cdot (v \times w).
\]

Example:

\[
A = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 2 & 0 \\ 0 & 0 & 3 \end{bmatrix}, \quad \det(A) = 6.
\]

So the unit cube is stretched into a box with volume 6.

\subsubsection{Orientation and Signed
Measure}\label{orientation-and-signed-measure}

Determinants do more than measure size-they also detect orientation:

\begin{itemize}
\tightlist
\item
  In 2D, flipping x and y axes changes the sign of the determinant.
\item
  In 3D, swapping two vectors changes the ``handedness'' (right-hand
  rule becomes left-hand rule).
\end{itemize}

This explains why determinants can be negative: they mark
transformations that reverse orientation.

\subsubsection{Higher Dimensions}\label{higher-dimensions}

In \(\mathbb{R}^n\), determinants extend the same idea. A unit hypercube
(side length 1) is transformed into an n-dimensional parallelotope,
whose volume is given by \(|\det(A)|\).

Though we cannot visualize beyond 3D, the concept generalizes smoothly:
determinants encode how much an n-dimensional object is stretched or
collapsed.

\subsubsection{Everyday Analogies}\label{everyday-analogies-47}

\begin{itemize}
\tightlist
\item
  Maps and scaling: A map might scale a square kilometer of land into a
  rectangle on paper. The determinant of the transformation encodes that
  scale factor.
\item
  3D printing: Scaling a digital object before printing changes its
  volume by the determinant of the scaling matrix.
\item
  Cooking: Doubling a recipe doubles volume; scaling each ingredient in
  three dimensions multiplies volume by \(\det(A)\).
\item
  Mirrors: Looking into a mirror reverses left and right-mathematically,
  this is a transformation with determinant \(-1\).
\end{itemize}

\subsubsection{Applications}\label{applications-13}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Geometry: Computing areas, volumes, and orientation directly from
  vectors.
\item
  Computer Graphics: Determinants detect whether a transformation
  preserves or flips orientation, useful in rendering.
\item
  Physics: Determinants describe Jacobians for coordinate changes in
  integrals, adjusting volume elements.
\item
  Engineering: Determinants quantify deformation and stress in materials
  (strain tensors).
\item
  Data Science: Determinants of covariance matrices encode ``volume'' of
  uncertainty ellipsoids.
\end{enumerate}

\subsubsection{Why It Matters}\label{why-it-matters-47}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Determinants connect algebra (formulas) to geometry (shapes).
\item
  They explain why some transformations lose information: \(\det=0\).
\item
  They preserve orientation, key for consistent physical laws and
  geometry.
\item
  They prepare us for advanced tools like Jacobians, eigenvalues, and
  volume-preserving maps.
\end{enumerate}

\subsubsection{Try It Yourself}\label{try-it-yourself-50}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute the area of the parallelogram spanned by \((1,2)\) and
  \((3,1)\).
\item
  Find the volume of the parallelepiped defined by vectors
  \((1,0,0),(0,1,0),(1,1,1)\).
\item
  Show that swapping two columns of a matrix flips the sign of the
  determinant but keeps absolute value unchanged.
\item
  Challenge: Explain why \(\det(A)\) gives the scaling factor for
  integrals under change of variables.
\end{enumerate}

Determinants begin as algebraic formulas, but their real meaning lies in
geometry: they measure how linear transformations scale, compress, or
flip space itself.

\subsection{52. Determinant via Linear
Rules}\label{determinant-via-linear-rules}

The determinant is not just a mysterious formula-it is a function built
from a few simple rules that uniquely determine its behavior. These
rules, often called determinant axioms, allow us to see the determinant
as the only measure of ``signed volume'' compatible with linear algebra.
Understanding these rules gives clarity: instead of memorizing expansion
formulas, we see why determinants behave as they do.

\subsubsection{The Setup}\label{the-setup}

Take a square matrix \(A \in \mathbb{R}^{n \times n}\). Think of \(A\)
as a list of \(n\) column vectors:

\[
A = \begin{bmatrix} a_1 & a_2 & \cdots & a_n \end{bmatrix}.
\]

The determinant is a function
\(\det: \mathbb{R}^{n \times n} \to \mathbb{R}\) that assigns a single
number to \(A\). Geometrically, it gives the signed volume of the
parallelotope spanned by \((a_1, \dots, a_n)\). Algebraically, it
follows three key rules.

\subsubsection{Rule 1: Linearity in Each
Column}\label{rule-1-linearity-in-each-column}

If you scale one column by a scalar \(c\), the determinant scales by
\(c\).

\[
\det(a_1, \dots, c a_j, \dots, a_n) = c \cdot \det(a_1, \dots, a_j, \dots, a_n).
\]

If you replace a column with a sum, the determinant splits:

\[
\det(a_1, \dots, (b+c), \dots, a_n) = \det(a_1, \dots, b, \dots, a_n) + \det(a_1, \dots, c, \dots, a_n).
\]

This linearity means determinants behave predictably with respect to
scaling and addition.

\subsubsection{Rule 2: Alternating
Property}\label{rule-2-alternating-property}

If two columns are the same, the determinant is zero:

\[
\det(\dots, a_i, \dots, a_i, \dots) = 0.
\]

This makes sense geometrically: if two spanning vectors are identical,
they collapse the volume to zero.

Equivalently: if you swap two columns, the determinant flips sign:

\[
\det(\dots, a_i, \dots, a_j, \dots) = -\det(\dots, a_j, \dots, a_i, \dots).
\]

\subsubsection{Rule 3: Normalization}\label{rule-3-normalization}

The determinant of the identity matrix is 1:

\[
\det(I_n) = 1.
\]

This anchors the function: the unit cube has volume 1, with positive
orientation.

\subsubsection{Consequence: Uniqueness}\label{consequence-uniqueness}

These three rules (linearity, alternating, normalization) uniquely
define the determinant. Any function satisfying them must be the
determinant. This makes it less of an arbitrary formula and more of a
natural consequence of linear structure.

\subsubsection{Small Cases: Explicit
Formulas}\label{small-cases-explicit-formulas}

\begin{itemize}
\item
  2×2 matrices:

  \[
  \det \begin{bmatrix} a & b \\ c & d \end{bmatrix} = ad - bc.
  \]

  This formula arises directly from the rules: linearity in columns and
  alternating sign when swapping them.
\item
  3×3 matrices: Expansion formula:

  \[
  \det \begin{bmatrix} 
  a & b & c \\ 
  d & e & f \\ 
  g & h & i 
  \end{bmatrix} 
  = aei + bfg + cdh - ceg - bdi - afh.
  \]
\end{itemize}

This looks complicated, but it comes from systematically applying the
rules to break down the volume.

\subsubsection{Geometric Interpretation of the
Rules}\label{geometric-interpretation-of-the-rules}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Linearity: Stretching one side of a parallelogram or parallelepiped
  scales the area or volume.
\item
  Alternating: If two sides collapse into the same direction, the
  area/volume vanishes. Swapping sides flips orientation.
\item
  Normalization: The unit cube has size 1 by definition.
\end{enumerate}

Together, these mirror geometric intuition exactly.

\subsubsection{Higher-Dimensional
Generalization}\label{higher-dimensional-generalization}

In \(\mathbb{R}^n\), determinants measure oriented hyper-volume. For
example, in 4D, determinants give the ``4-volume'' of a parallelotope.
Though impossible to picture, the same rules apply.

\subsubsection{Everyday Analogies}\label{everyday-analogies-48}

\begin{itemize}
\tightlist
\item
  Blueprint scaling: Doubling the width of a rectangle doubles the area.
  Linearity captures this.
\item
  DNA strands: If two strands are identical, the structure collapses;
  alternating captures redundancy.
\item
  Standard ruler: Calibration requires a reference-normalization fixes
  the unit cube's volume at 1.
\end{itemize}

\subsubsection{Applications}\label{applications-14}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Defining area and volume: Determinants provide a universal formula for
  computing geometric sizes from coordinates.
\item
  Jacobian determinants: Used in calculus when changing variables in
  multiple integrals.
\item
  Orientation detection: Whether transformations preserve handedness in
  geometry or physics.
\item
  Computer graphics: Ensuring consistent orientation of polygons and
  meshes.
\end{enumerate}

\subsubsection{Why It Matters}\label{why-it-matters-48}

Determinants are not arbitrary. They arise naturally once we demand a
function that is linear in columns, alternating, and normalized. This
explains why so many different formulas and properties agree: they are
all shadows of the same underlying definition.

\subsubsection{Try It Yourself}\label{try-it-yourself-51}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Show that scaling one column by 3 multiplies the determinant by 3.
\item
  Compute the determinant of
  \(\begin{bmatrix} 1 & 2 \\ 2 & 4 \end{bmatrix}\) and explain why it is
  zero.
\item
  Swap two columns in \(\begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix}\)
  and confirm the determinant changes sign.
\item
  Challenge: Use only the three rules to derive the \(2 \times 2\)
  determinant formula.
\end{enumerate}

The determinant is the unique bridge between algebra and geometry, born
from a handful of simple but powerful rules.

\subsection{53. Determinant and Row
Operations}\label{determinant-and-row-operations}

One of the most practical ways to compute determinants is by using row
operations, the same tools used in Gaussian elimination. Determinants
interact with these operations in very structured ways. By understanding
the rules, we can compute determinants systematically without resorting
to long expansion formulas.

\subsubsection{Row Operations Recap}\label{row-operations-recap}

There are three elementary row operations:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Row Swap (R\(_i \leftrightarrow\) R\(_j\)) -- exchange two rows.
\item
  Row Scaling (c·R\(_i\)) -- multiply a row by a scalar \(c\).
\item
  Row Replacement (R\(_i\) + c·R\(_j\)) -- replace one row with itself
  plus a multiple of another row.
\end{enumerate}

Since the determinant is defined in terms of linearity and alternation
of rows (or columns), each operation has a clear effect.

\subsubsection{Rule 1: Row Swap Changes
Sign}\label{rule-1-row-swap-changes-sign}

If you swap two rows, the determinant changes sign:

\[
\det(A \text{ with } R_i \leftrightarrow R_j) = -\det(A).
\]

Reason: Swapping two spanning vectors flips orientation. In 2D, swapping
basis vectors flips a parallelogram across the diagonal, reversing
handedness.

\subsubsection{Rule 2: Row Scaling Multiplies
Determinant}\label{rule-2-row-scaling-multiplies-determinant}

If you multiply a row by a scalar \(c\):

\[
\det(A \text{ with } cR_i) = c \cdot \det(A).
\]

Reason: Scaling one side of a parallelogram multiplies its area; scaling
one dimension of a cube multiplies its volume.

\subsubsection{Rule 3: Row Replacement Leaves Determinant
Unchanged}\label{rule-3-row-replacement-leaves-determinant-unchanged}

If you replace one row with itself plus a multiple of another row:

\[
\det(A \text{ with } R_i \to R_i + cR_j) = \det(A).
\]

Reason: Adding a multiple of one spanning vector to another doesn't
change the spanned volume. The parallelogram or parallelepiped is
sheared, but its area or volume remains the same.

\subsubsection{Why These Rules Work
Together}\label{why-these-rules-work-together}

These three rules align perfectly with the determinant axioms:

\begin{itemize}
\tightlist
\item
  Alternating → row swaps flip sign.
\item
  Linearity → scaling multiplies by scalar.
\item
  Normalization → row replacement preserves measure.
\end{itemize}

Thus, row operations provide a complete framework for computing
determinants.

\subsubsection{Computing Determinants with
Elimination}\label{computing-determinants-with-elimination}

To compute \(\det(A)\):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Perform Gaussian elimination to reduce \(A\) to an upper triangular
  matrix \(U\).
\item
  Track how row swaps and scalings affect the determinant.
\item
  Use the fact that the determinant of a triangular matrix is the
  product of its diagonal entries.
\end{enumerate}

Example:

\[
A = \begin{bmatrix} 2 & 1 & 3 \\ 4 & 1 & 7 \\ -2 & 5 & 1 \end{bmatrix}.
\]

\begin{itemize}
\item
  Step 1: \(R_2 \to R_2 - 2R_1\), \(R_3 \to R_3 + R_1\). No determinant
  change.
\item
  Step 2: Upper triangular form emerges:

  \[
  U = \begin{bmatrix} 2 & 1 & 3 \\ 0 & -1 & 1 \\ 0 & 0 & -5 \end{bmatrix}.
  \]
\item
  Step 3: Determinant is product of diagonals:
  \(\det(A) = 2 \cdot (-1) \cdot (-5) = 10.\)
\end{itemize}

Efficient, clear, and no messy cofactor expansions.

\subsubsection{Geometric View}\label{geometric-view}

\begin{itemize}
\tightlist
\item
  Row swap: Flips orientation of the volume.
\item
  Row scaling: Stretches or compresses one dimension of the volume.
\item
  Row replacement: Slides faces of the volume without changing its size.
\end{itemize}

This geometric reasoning reinforces why the rules are natural.

\subsubsection{Everyday Analogies}\label{everyday-analogies-49}

\begin{itemize}
\tightlist
\item
  Cooking: Doubling one ingredient (scaling) doubles the total mixture
  if it's the only varying part.
\item
  Team projects: Swapping two roles reverses order but doesn't change
  overall group size.
\item
  Architecture: Shifting a wall while keeping the same base area
  preserves the floor plan (row replacement).
\end{itemize}

\subsubsection{Applications}\label{applications-15}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Efficient computation: Algorithms for large determinants (LU
  decomposition) are based on row operations.
\item
  Numerical analysis: Determinant rules help detect stability and
  singularity.
\item
  Geometry: Orientation tests for polygons rely on row swap rules.
\item
  Theoretical results: Many determinant identities are derived directly
  from row operation behavior.
\end{enumerate}

\subsubsection{Why It Matters}\label{why-it-matters-49}

\begin{itemize}
\tightlist
\item
  Determinants link algebra to geometry, but computation requires
  efficient methods.
\item
  Row operations give a hands-on toolkit: they're the backbone of
  practical determinant computation.
\item
  Understanding these rules explains why algorithms like LU
  factorization work so well.
\end{itemize}

\subsubsection{Try It Yourself}\label{try-it-yourself-52}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute the determinant of
  \(\begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9 \end{bmatrix}\)
  using elimination.
\item
  Verify that replacing \(R_2 \to R_2 + 3R_1\) does not change the
  determinant.
\item
  Check how many sign flips occur if you reorder rows into strictly
  increasing order.
\item
  Challenge: Prove that elimination combined with these rules always
  leads to the triangular product formula.
\end{enumerate}

Determinants are not meant to be expanded by brute force; row operations
transform the problem into a clear sequence of steps, connecting
algebraic efficiency with geometric intuition.

\subsection{54. Triangular Matrices and Product of
Diagonals}\label{triangular-matrices-and-product-of-diagonals}

Among all types of matrices, triangular matrices stand out for their
simplicity. These are matrices where every entry either above or below
the main diagonal is zero. What makes them especially important is that
their determinants can be computed almost instantly: the determinant of
a triangular matrix is simply the product of its diagonal entries. This
property is not only computationally convenient, it also reveals deep
connections between determinants, row operations, and structure in
linear algebra.

\subsubsection{Triangular Matrices
Defined}\label{triangular-matrices-defined}

A square matrix is called upper triangular if all entries below the main
diagonal are zero, and lower triangular if all entries above the
diagonal are zero.

\begin{itemize}
\item
  Upper triangular example:

  \[
  U = \begin{bmatrix} 
  2 & 5 & -1 \\ 
  0 & 3 & 4 \\ 
  0 & 0 & 7 
  \end{bmatrix}.
  \]
\item
  Lower triangular example:

  \[
  L = \begin{bmatrix} 
  4 & 0 & 0 \\ 
  -2 & 5 & 0 \\ 
  1 & 3 & 6 
  \end{bmatrix}.
  \]
\end{itemize}

Both share the key feature: ``everything off one side of the diagonal
vanishes.''

\subsubsection{Determinant Rule}\label{determinant-rule}

For any triangular matrix,

\[
\det(T) = \prod_{i=1}^n t_{ii},
\]

where \(t_{ii}\) are the diagonal entries.

So for the upper triangular \(U\) above,

\[
\det(U) = 2 \times 3 \times 7 = 42.
\]

\subsubsection{Why This Works}\label{why-this-works}

The determinant is multilinear and alternating. When you expand it
(e.g., via cofactor expansion), only one product of entries survives in
the expansion: the one that picks exactly the diagonal terms.

\begin{itemize}
\tightlist
\item
  If you try to pick an off-diagonal entry in a row, you eventually get
  stuck with a zero entry because of the triangular shape.
\item
  The only surviving term is the product of the diagonals, with sign
  \(+1\).
\end{itemize}

This elegant reasoning explains why the rule holds universally.

\subsubsection{Connection to Row
Operations}\label{connection-to-row-operations}

Recall: elimination reduces any square matrix to an upper triangular
form. Once triangular, the determinant is simply the product of the
diagonals, adjusted for row swaps and scalings.

Thus, triangular matrices are not just simple-they are the end goal of
elimination algorithms for determinant computation.

\subsubsection{Geometric Meaning}\label{geometric-meaning-8}

In geometric terms:

\begin{itemize}
\tightlist
\item
  A triangular matrix represents a transformation where each coordinate
  direction depends only on itself and earlier coordinates.
\item
  The determinant equals the product of scaling along each axis.
\item
  Example: In 3D, scaling x by 2, y by 3, and z by 7 gives a volume
  scaling of \(2 \cdot 3 \cdot 7 = 42\).
\end{itemize}

Even if shear is present in the upper entries, the determinant ignores
it-it only cares about the pure diagonal scaling.

\subsubsection{Everyday Analogies}\label{everyday-analogies-50}

\begin{itemize}
\tightlist
\item
  Business growth: If three independent factors (sales, marketing,
  operations) scale profits by 2, 3, and 7, the combined scaling is
  their product, 42.
\item
  Cooking layers: Scaling each layer of a recipe multiplies effects-2×
  saltiness, 3× spiciness, 7× sweetness → 42× overall intensity.
\item
  Pipeline processes: If each stage of a pipeline scales throughput
  independently, the total scaling is the product of individual factors.
\end{itemize}

\subsubsection{Applications}\label{applications-16}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Efficient computation: LU decomposition reduces determinants to
  diagonal product form.
\item
  Theoretical proofs: Many determinant identities reduce to triangular
  cases.
\item
  Numerical stability: Triangular matrices are well-behaved in
  computation, crucial for algorithms in numerical linear algebra.
\item
  Eigenvalues: For triangular matrices, eigenvalues are exactly the
  diagonal entries; thus determinant = product of eigenvalues.
\item
  Computer graphics: Triangular forms simplify geometric
  transformations.
\end{enumerate}

\subsubsection{Why It Matters}\label{why-it-matters-50}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Provides the fastest way to compute determinants in special cases.
\item
  Serves as the computational foundation for general determinant
  algorithms.
\item
  Connects determinants directly to eigenvalues and scaling factors.
\item
  Illustrates how elimination transforms complexity into simplicity.
\end{enumerate}

\subsubsection{Try It Yourself}\label{try-it-yourself-53}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Compute the determinant of

  \[
  \begin{bmatrix} 
  1 & 2 & 3 \\ 
  0 & 4 & 5 \\ 
  0 & 0 & 6 
  \end{bmatrix}.
  \]

  (Check: it should equal \(1 \cdot 4 \cdot 6\)).
\item
  Verify that a lower triangular matrix with diagonal entries
  \((2, -1, 5)\) has determinant \(-10\).
\item
  Explain why an upper triangular matrix with a zero on the diagonal
  must have determinant 0.
\item
  Challenge: Prove that every square matrix can be reduced to triangular
  form with determinant tracked by elimination steps.
\end{enumerate}

The triangular case reveals the heart of determinants: a product of
diagonal scalings, stripped of all extra noise. It is the simplest lens
through which determinants become transparent.

\subsection{\texorpdfstring{55. The Multiplicative Property of
Determinants:
\(\det(AB) = \det(A)\det(B)\)}{55. The Multiplicative Property of Determinants: \textbackslash det(AB) = \textbackslash det(A)\textbackslash det(B)}}\label{the-multiplicative-property-of-determinants-detab-detadetb}

One of the most remarkable and useful facts about determinants is that
they multiply across matrix products. For two square matrices of the
same size,

\[
\det(AB) = \det(A) \cdot \det(B).
\]

This property is fundamental: it connects algebra (matrix
multiplication) with geometry (scaling volumes) and is essential for
proofs, computations, and applications across mathematics, physics, and
engineering.

\subsubsection{The Statement in Words}\label{the-statement-in-words}

\begin{itemize}
\tightlist
\item
  If you first apply a linear transformation \(B\), and then apply
  \(A\), the total scaling of space is the product of their individual
  scalings.
\item
  Determinants track exactly this: the signed volume change under linear
  transformations.
\end{itemize}

\subsubsection{Geometric Intuition}\label{geometric-intuition-4}

Think of \(\det(A)\) as the signed scale factor by which \(A\) changes
volume.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Apply \(B\): a unit cube becomes some parallelepiped with volume
  \(|\det(B)|\).
\item
  Apply \(A\): the new parallelepiped scales again by \(|\det(A)|\).
\item
  Total effect: volume scales by \(|\det(A)| \times |\det(B)|\).
\end{enumerate}

The orientation flips are also consistent: if both flip (negative
determinants), the total orientation is preserved (positive product).

\subsubsection{Algebraic Reasoning}\label{algebraic-reasoning}

The proof can be approached in multiple ways:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Row Operations and Elimination:

  \begin{itemize}
  \tightlist
  \item
    \(A\) and \(B\) can be factored into elementary matrices (row swaps,
    scalings, replacements).
  \item
    Determinants behave predictably for each operation.
  \item
    Since both sides agree for elementary operations and determinant is
    multiplicative, the identity holds in general.
  \end{itemize}
\item
  Abstract Characterization:

  \begin{itemize}
  \tightlist
  \item
    Determinants are the unique multilinear alternating functions
    normalized at the identity.
  \item
    Composition of linear maps preserves this property, so
    multiplicativity follows.
  \end{itemize}
\end{enumerate}

\subsubsection{Small Cases}\label{small-cases}

\begin{itemize}
\item
  2×2 matrices:

  \[
  A = \begin{bmatrix} a & b \\ c & d \end{bmatrix}, \quad 
  B = \begin{bmatrix} e & f \\ g & h \end{bmatrix}.
  \]

  Compute \(AB\), then \(\det(AB)\). After expansion, you find:
  \(\det(AB) = (ad - bc)(eh - fg) = \det(A)\det(B).\)
\item
  3×3 matrices: A direct computation is messy, but the property still
  holds and is consistent with elimination proofs.
\end{itemize}

\subsubsection{Key Consequences}\label{key-consequences}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Determinant of a Power:

  \[
  \det(A^k) = (\det(A))^k.
  \]

  Geometric meaning: applying the same transformation \(k\) times
  multiplies volume scale repeatedly.
\item
  Inverse Matrix: If \(A\) is invertible,

  \[
  \det(A^{-1}) = \frac{1}{\det(A)}.
  \]
\item
  Eigenvalues: Since \(\det(A)\) is the product of eigenvalues, this
  property matches the fact that eigenvalues multiply under matrix
  multiplication (when considered via characteristic polynomials).
\end{enumerate}

\subsubsection{Geometric Meaning in Higher
Dimensions}\label{geometric-meaning-in-higher-dimensions}

\begin{itemize}
\tightlist
\item
  If \(B\) scales space by 3 and flips it (det = --3), and \(A\) scales
  by 2 without flipping (det = 2), then \(AB\) scales by --6, consistent
  with the rule.
\item
  Determinants encapsulate both magnitude (volume scaling) and sign
  (orientation). Multiplicativity ensures these combine correctly.
\end{itemize}

\subsubsection{Everyday Analogies}\label{everyday-analogies-51}

\begin{itemize}
\tightlist
\item
  Economics: If a policy doubles output (factor 2) and another policy
  triples it (factor 3), combined effect is sixfold.
\item
  Cooking: Scaling a recipe by 2, then again by 3, multiplies the final
  volume by 6.
\item
  Maps: If one map projection scales area by 4 and another zooms out by
  1/2, the total scaling is 2.
\end{itemize}

\subsubsection{Applications}\label{applications-17}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Change of Variables in Calculus: The Jacobian determinant follows this
  multiplicative rule, ensuring transformations compose consistently.
\item
  Group Theory: \(\det\) defines a group homomorphism from the general
  linear group \(GL_n\) to the nonzero reals under multiplication.
\item
  Numerical Analysis: Determinant multiplicativity underlies LU
  decomposition methods.
\item
  Physics: In mechanics and relativity, volume elements transform
  consistently under successive transformations.
\item
  Cryptography and Coding Theory: Determinants in modular arithmetic
  rely on this multiplicative property to preserve structure.
\end{enumerate}

\subsubsection{Why It Matters}\label{why-it-matters-51}

\begin{itemize}
\tightlist
\item
  Guarantees consistency: determinants match our intuition about
  scaling.
\item
  Simplifies computation: determinants of factorizations can be obtained
  by multiplying smaller pieces.
\item
  Provides theoretical structure: \(\det\) is a homomorphism, embedding
  linear algebra into the algebra of scalars.
\end{itemize}

\subsubsection{Try It Yourself}\label{try-it-yourself-54}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Verify \(\det(AB) = \det(A)\det(B)\) for

  \[
  A = \begin{bmatrix} 2 & 1 \\ 0 & 3 \end{bmatrix}, \quad 
  B = \begin{bmatrix} 1 & 4 \\ 0 & -2 \end{bmatrix}.
  \]
\item
  Prove that \(\det(A^{-1}) = 1/\det(A)\) using the multiplicative rule.
\item
  Show that if \(\det(A)=0\), then \(\det(AB)=0\) for any \(B\). Explain
  why this makes sense geometrically.
\item
  Challenge: Using row operations, show explicitly how multiplicativity
  emerges from properties of elementary matrices.
\end{enumerate}

The rule \(\det(AB) = \det(A)\det(B)\) transforms determinants from a
mysterious calculation into a natural and consistent measure of how
linear transformations combine.

\subsection{56. Invertibility and Zero
Determinant}\label{invertibility-and-zero-determinant}

The determinant is more than a geometric scale factor-it is the ultimate
test of whether a matrix is invertible. A square matrix
\(A \in \mathbb{R}^{n \times n}\) has an inverse if and only if its
determinant is nonzero. When the determinant vanishes, the matrix
collapses space into a lower dimension, losing information that no
transformation can undo.

\subsubsection{The Criterion}\label{the-criterion}

\[
A \text{ invertible } \iff \det(A) \neq 0.
\]

\begin{itemize}
\tightlist
\item
  If \(\det(A) \neq 0\), the transformation stretches or shrinks space
  but never flattens it. Every output corresponds to exactly one input,
  so \(A^{-1}\) exists.
\item
  If \(\det(A) = 0\), some directions are squashed into lower
  dimensions. Information is destroyed, so no inverse exists.
\end{itemize}

\subsubsection{Geometric Meaning}\label{geometric-meaning-9}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  In 2D:

  \begin{itemize}
  \tightlist
  \item
    A nonzero determinant means the unit square is sent to a
    parallelogram with nonzero area.
  \item
    A zero determinant means the square collapses into a line segment or
    a point.
  \end{itemize}
\item
  In 3D:

  \begin{itemize}
  \tightlist
  \item
    Nonzero determinant → unit cube becomes a 3D parallelepiped with
    volume.
  \item
    Zero determinant → cube flattens into a sheet or a line; 3D volume
    is lost.
  \end{itemize}
\item
  In Higher Dimensions:

  \begin{itemize}
  \tightlist
  \item
    Nonzero determinant preserves n-dimensional volume.
  \item
    Zero determinant collapses dimension, destroying invertibility.
  \end{itemize}
\end{enumerate}

\subsubsection{Algebraic Meaning}\label{algebraic-meaning}

\begin{itemize}
\item
  The determinant is the product of eigenvalues:

  \[
  \det(A) = \lambda_1 \lambda_2 \cdots \lambda_n.
  \]

  If any eigenvalue is zero, then \(\det(A) = 0\) and the matrix is
  singular (not invertible).
\item
  Equivalently, a zero determinant means the matrix has linearly
  dependent columns or rows. This dependence implies redundancy: not all
  directions are independent, so the mapping cannot be one-to-one.
\end{itemize}

\subsubsection{Connection with Linear
Systems}\label{connection-with-linear-systems}

\begin{itemize}
\item
  If \(\det(A) \neq 0\):

  \begin{itemize}
  \tightlist
  \item
    The system \(Ax = b\) has a unique solution for every \(b\).
  \item
    The inverse matrix \(A^{-1}\) exists and satisfies \(x = A^{-1}b\).
  \end{itemize}
\item
  If \(\det(A) = 0\):

  \begin{itemize}
  \tightlist
  \item
    Either no solutions (inconsistent system) or infinitely many
    solutions (dependent equations).
  \item
    The mapping \(x \mapsto Ax\) cannot be reversed.
  \end{itemize}
\end{itemize}

\subsubsection{Example: Invertible
vs.~Singular}\label{example-invertible-vs.-singular}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
\end{enumerate}

\[
A = \begin{bmatrix} 2 & 1 \\ 1 & 3 \end{bmatrix}, \quad \det(A) = 5 \neq 0.
\]

Invertible.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
\end{enumerate}

\[
B = \begin{bmatrix} 2 & 4 \\ 1 & 2 \end{bmatrix}, \quad \det(B) = 0.
\]

Not invertible, since the second column is just twice the first.

\subsubsection{Everyday Analogies}\label{everyday-analogies-52}

\begin{itemize}
\tightlist
\item
  Shadows: Projecting a 3D object onto a 2D wall loses depth
  information. This is like a determinant of 0: flattening makes
  inversion impossible.
\item
  Recipes: If two ingredients are exact multiples of each other, you
  don't really have two independent ``flavors.'' The recipe loses
  uniqueness.
\item
  Maps: A detailed city map shrunk to a single line loses all spatial
  information. You cannot reconstruct the city from the line.
\end{itemize}

\subsubsection{Applications}\label{applications-18}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Solving Systems: Inverse-based methods rely on nonzero determinants.
\item
  Numerical Methods: Detecting near-singularity warns of unstable
  solutions.
\item
  Geometry: A singular matrix corresponds to degenerate shapes
  (flattened, collapsed).
\item
  Physics: In mechanics and relativity, invertibility ensures that
  transformations can be reversed.
\item
  Computer Graphics: Non-invertible transformations crush dimensions,
  breaking rendering pipelines.
\end{enumerate}

\subsubsection{Why It Matters}\label{why-it-matters-52}

\begin{itemize}
\tightlist
\item
  Determinants provide a single scalar test for invertibility.
\item
  This connects geometry (volume collapse), algebra (linear dependence),
  and analysis (solvability of systems).
\item
  The zero/nonzero divide is one of the sharpest and most important in
  all of linear algebra.
\end{itemize}

\subsubsection{Try It Yourself}\label{try-it-yourself-55}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Determine whether

  \[
  \begin{bmatrix} 1 & 2 \\ 3 & 6 \end{bmatrix}
  \]

  is invertible. Explain both geometrically and algebraically.
\item
  For

  \[
  \begin{bmatrix} 1 & 0 & 0 \\ 0 & 2 & 0 \\ 0 & 0 & 0 \end{bmatrix},
  \]

  compute the determinant and describe the geometric transformation.
\item
  Challenge: Show that if \(\det(A)=0\), the rows (or columns) of \(A\)
  are linearly dependent.
\end{enumerate}

The determinant acts as the ultimate yes-or-no test: nonzero means
full-dimensional, reversible transformation; zero means collapse and
irreversibility.

\subsection{57. Cofactor Expansion}\label{cofactor-expansion}

While elimination gives a practical way to compute determinants, the
cofactor expansion (also called Laplace expansion) offers a recursive
definition that works for all square matrices. It expresses the
determinant of an \(n \times n\) matrix in terms of determinants of
smaller \((n-1) \times (n-1)\) matrices. This method reveals the
internal structure of determinants and serves as a bridge between theory
and computation.

\subsubsection{Minors and Cofactors}\label{minors-and-cofactors}

\begin{itemize}
\item
  The minor \(M_{ij}\) of an entry \(a_{ij}\) is the determinant of the
  submatrix obtained by deleting the \(i\)-th row and \(j\)-th column
  from \(A\).
\item
  The cofactor \(C_{ij}\) adds a sign factor:

  \[
  C_{ij} = (-1)^{i+j} M_{ij}.
  \]
\end{itemize}

Thus each entry contributes to the determinant through its cofactor,
with alternating signs arranged in a checkerboard pattern:

\[
\begin{bmatrix} 
+ & - & + & - & \cdots \\ 
- & + & - & + & \cdots \\ 
+ & - & + & - & \cdots \\ 
\vdots & \vdots & \vdots & \vdots & \ddots 
\end{bmatrix}.
\]

\subsubsection{The Expansion Formula}\label{the-expansion-formula}

For any row \(i\):

\[
\det(A) = \sum_{j=1}^n a_{ij} C_{ij}.
\]

Or for any column \(j\):

\[
\det(A) = \sum_{i=1}^n a_{ij} C_{ij}.
\]

That is, the determinant can be computed by expanding along any row or
column.

\subsubsection{Example: 3×3 Case}\label{example-33-case-1}

Let

\[
A = \begin{bmatrix} 
a & b & c \\ 
d & e & f \\ 
g & h & i 
\end{bmatrix}.
\]

Expanding along the first row:

\[
\det(A) = a \cdot \det \begin{bmatrix} e & f \\ h & i \end{bmatrix} 
- b \cdot \det \begin{bmatrix} d & f \\ g & i \end{bmatrix} 
+ c \cdot \det \begin{bmatrix} d & e \\ g & h \end{bmatrix}.
\]

Simplify each 2×2 determinant:

\[
= a(ei - fh) - b(di - fg) + c(dh - eg).
\]

This matches the familiar expansion formula for 3×3 determinants.

\subsubsection{Why It Works}\label{why-it-works}

Cofactor expansion follows directly from the multilinearity and
alternating rules of determinants:

\begin{itemize}
\tightlist
\item
  Only one element per row and per column contributes to each term.
\item
  Signs alternate because swapping rows/columns reverses orientation.
\item
  Recursive expansion reduces the problem size until reaching 2×2
  determinants, where the formula is simple.
\end{itemize}

\subsubsection{Computational Complexity}\label{computational-complexity}

\begin{itemize}
\tightlist
\item
  For \(n=2\), expansion is immediate.
\item
  For \(n=3\), expansion is manageable.
\item
  For large \(n\), expansion is very inefficient: computing \(\det(A)\)
  via cofactors requires \(O(n!)\) operations.
\end{itemize}

That's why in practice, elimination or LU decomposition is preferred.
Cofactor expansion is best for theory, proofs, and small matrices.

\subsubsection{Geometric
Interpretation}\label{geometric-interpretation-12}

Each cofactor corresponds to excluding one direction (row/column),
measuring the volume of the remaining sub-parallelotope. The alternating
sign keeps track of orientation. Thus the determinant is a weighted
combination of contributions from all entries along a chosen row or
column.

\subsubsection{Everyday Analogies}\label{everyday-analogies-53}

\begin{itemize}
\tightlist
\item
  Voting system: Each candidate (matrix entry) contributes a weighted
  score (cofactor) to the final outcome.
\item
  Team project: Removing one member (row/column) still leaves a sub-team
  whose structure influences the whole.
\item
  Recipe adjustments: Leaving out one ingredient and adjusting the
  others changes the flavor, but the original dish can be reconstructed
  from these partial contributions.
\end{itemize}

\subsubsection{Applications}\label{applications-19}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Theoretical proofs: Cofactor expansion underlies many determinant
  identities.
\item
  Adjugate matrix: Cofactors form the adjugate used in the explicit
  formula for matrix inverses.
\item
  Eigenvalues: Characteristic polynomials use cofactor expansion.
\item
  Geometry: Cofactors describe signed volumes of faces of
  higher-dimensional shapes.
\end{enumerate}

\subsubsection{Why It Matters}\label{why-it-matters-53}

\begin{itemize}
\tightlist
\item
  Cofactor expansion connects determinants across dimensions.
\item
  It provides a universal definition independent of row operations.
\item
  It explains why determinants behave consistently with volume,
  orientation, and algebraic rules.
\end{itemize}

\subsubsection{Try It Yourself}\label{try-it-yourself-56}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Expand the determinant of

  \[
  \begin{bmatrix} 
  2 & 1 & 3 \\ 
  0 & -1 & 4 \\ 
  1 & 2 & 0 
  \end{bmatrix}
  \]

  along the first row.
\item
  Compute the same determinant by expanding along the second column.
  Verify the result matches.
\item
  Show that expanding along two different rows gives the same
  determinant.
\item
  Challenge: Prove by induction that cofactor expansion works for all
  \(n \times n\) matrices.
\end{enumerate}

Cofactor expansion is not the fastest method, but it reveals the
recursive structure of determinants and explains why they hold their
rich algebraic and geometric meaning.

\subsection{58. Permutations and the Sign of the
Determinant}\label{permutations-and-the-sign-of-the-determinant}

Behind every determinant formula lies a hidden structure: permutations.
Determinants can be expressed as a weighted sum over all possible ways
of selecting one entry from each row and each column of a matrix. The
weight for each selection is determined by the sign of the permutation
used. This viewpoint reveals why determinants encode orientation and why
their formulas alternate between positive and negative terms.

\subsubsection{The Permutation
Definition}\label{the-permutation-definition}

Let \(S_n\) denote the set of all permutations of \(n\) elements. Each
permutation \(\sigma \in S_n\) rearranges the numbers
\(\{1, 2, \ldots, n\}\).

The determinant of an \(n \times n\) matrix \(A = [a_{ij}]\) is defined
as:

\[
\det(A) = \sum_{\sigma \in S_n} \text{sgn}(\sigma) \prod_{i=1}^n a_{i, \sigma(i)}.
\]

\begin{itemize}
\tightlist
\item
  Each product \(\prod_{i=1}^n a_{i, \sigma(i)}\) picks one entry from
  each row and each column, according to \(\sigma\).
\item
  The factor \(\text{sgn}(\sigma)\) is \(+1\) if \(\sigma\) is an even
  permutation (achieved by an even number of swaps), and \(-1\) if it is
  odd.
\end{itemize}

\subsubsection{Why Permutations Appear}\label{why-permutations-appear}

A determinant requires:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Linearity in each row.
\item
  Alternating property (row swaps flip the sign).
\item
  Normalization (\(\det(I)=1\)).
\end{enumerate}

When you expand by multilinearity, all possible combinations of choosing
one entry per row and column arise. The alternating rule enforces that
terms with repeated columns vanish, leaving only permutations. The sign
of each permutation enforces the orientation flip.

\subsubsection{Example: 2×2 Case}\label{example-22-case-1}

\[
A = \begin{bmatrix} a & b \\ c & d \end{bmatrix}.
\]

There are two permutations in \(S_2\):

\begin{itemize}
\tightlist
\item
  Identity \((1,2)\): sign \(+1\), contributes \(a \cdot d\).
\item
  Swap \((2,1)\): sign \(-1\), contributes \(-bc\).
\end{itemize}

So,

\[
\det(A) = ad - bc.
\]

\subsubsection{Example: 3×3 Case}\label{example-33-case-2}

\[
A = \begin{bmatrix} a & b & c \\ d & e & f \\ g & h & i \end{bmatrix}.
\]

There are \(3! = 6\) permutations:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \((1,2,3)\): even, \(+aei\).
\item
  \((1,3,2)\): odd, \(-afh\).
\item
  \((2,1,3)\): odd, \(-bdi\).
\item
  \((2,3,1)\): even, \(+bfg\).
\item
  \((3,1,2)\): even, \(+cdh\).
\item
  \((3,2,1)\): odd, \(-ceg\).
\end{enumerate}

So,

\[
\det(A) = aei + bfg + cdh - ceg - bdi - afh.
\]

This is exactly the cofactor expansion result, but now explained as a
permutation sum.

\subsubsection{Geometric Meaning of
Signs}\label{geometric-meaning-of-signs}

\begin{itemize}
\tightlist
\item
  Even permutations correspond to consistent orientation of basis
  vectors.
\item
  Odd permutations correspond to flipped orientation.
\item
  The determinant alternates signs because flipping axes reverses
  handedness.
\end{itemize}

\subsubsection{Counting Growth}\label{counting-growth}

\begin{itemize}
\tightlist
\item
  For \(n=4\), there are \(4! = 24\) terms.
\item
  For \(n=5\), \(5! = 120\) terms.
\item
  In general, \(n!\) terms make this formula impractical for large
  matrices.
\item
  Still, it gives the deepest definition of determinants, from which all
  other rules follow.
\end{itemize}

\subsubsection{Everyday Analogies}\label{everyday-analogies-54}

\begin{itemize}
\tightlist
\item
  Seating arrangements: Each way to assign seats corresponds to a
  permutation; the determinant weights each arrangement with a sign.
\item
  Logistics: Assigning workers to tasks one-to-one has many
  possibilities; some align consistently, others ``flip'' the structure.
\item
  DNA sequences: Permutations rearrange bases; the sign tracks whether
  the sequence orientation is preserved.
\end{itemize}

\subsubsection{Applications}\label{applications-20}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Abstract algebra: Determinant definition via permutations works over
  any field.
\item
  Combinatorics: Determinants encode signed sums over permutations,
  connecting to permanents.
\item
  Theoretical proofs: Many determinant properties, like
  multiplicativity, emerge cleanly from the permutation definition.
\item
  Leibniz formula: Explicit but impractical formula for computation.
\item
  Advanced math: Determinants generalize to alternating multilinear
  forms in linear algebra and differential geometry.
\end{enumerate}

\subsubsection{Why It Matters}\label{why-it-matters-54}

\begin{itemize}
\tightlist
\item
  Provides the most fundamental definition of determinants.
\item
  Explains alternating signs in formulas naturally.
\item
  Bridges algebra, geometry, and combinatorics.
\item
  Shows how orientation emerges from row/column arrangements.
\end{itemize}

\subsubsection{Try It Yourself}\label{try-it-yourself-57}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Write out all 6 terms in the 3×3 determinant expansion and verify the
  sign of each permutation.
\item
  Compute the determinant of
  \(\begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9 \end{bmatrix}\)
  using the permutation definition.
\item
  Show that if two columns are equal, all permutation terms cancel,
  giving \(\det(A)=0\).
\item
  Challenge: Prove that swapping two rows changes the sign of every
  permutation term, flipping the total determinant.
\end{enumerate}

Determinants may look like algebraic puzzles, but the permutation
formula reveals their true nature: a grand sum over all possible ways of
matching rows to columns, with signs recording whether orientation is
preserved or reversed.

\subsection{59. Cramer's Rule}\label{cramers-rule}

Cramer's Rule is a classical method for solving systems of linear
equations using determinants. While rarely used in large-scale
computation due to inefficiency, it offers deep theoretical insights
into the relationship between determinants, invertibility, and linear
systems. It shows how the determinant of a matrix encodes not only
volume scaling but also the exact solution to equations.

\subsubsection{The Setup}\label{the-setup-1}

Consider a system of \(n\) linear equations with \(n\) unknowns:

\[
Ax = b,
\]

where \(A\) is an invertible \(n \times n\) matrix, \(x\) is the vector
of unknowns, and \(b\) is the right-hand side vector.

Cramer's Rule states:

\[
x_i = \frac{\det(A_i)}{\det(A)},
\]

where \(A_i\) is the matrix \(A\) with its \(i\)-th column replaced by
\(b\).

\subsubsection{Example: 2×2 Case}\label{example-22-case-2}

Solve:

\[
\begin{cases} 
2x + y = 5 \\ 
x + 3y = 7 
\end{cases}
\]

Matrix form:

\[
A = \begin{bmatrix} 2 & 1 \\ 1 & 3 \end{bmatrix}, \quad b = \begin{bmatrix} 5 \\ 7 \end{bmatrix}.
\]

Determinant of \(A\):

\[
\det(A) = 2\cdot 3 - 1\cdot 1 = 5.
\]

\begin{itemize}
\tightlist
\item
  For \(x_1\): replace first column with \(b\):
\end{itemize}

\[
A_1 = \begin{bmatrix} 5 & 1 \\ 7 & 3 \end{bmatrix}, \quad \det(A_1) = 15 - 7 = 8.
\]

So \(x_1 = 8/5\).

\begin{itemize}
\tightlist
\item
  For \(x_2\): replace second column with \(b\):
\end{itemize}

\[
A_2 = \begin{bmatrix} 2 & 5 \\ 1 & 7 \end{bmatrix}, \quad \det(A_2) = 14 - 5 = 9.
\]

So \(x_2 = 9/5\).

Solution: \((x,y) = (8/5, 9/5)\).

\subsubsection{Why It Works}\label{why-it-works-1}

Since \(A\) is invertible,

\[
x = A^{-1}b.
\]

But recall the formula for the inverse:

\[
A^{-1} = \frac{1}{\det(A)} \text{adj}(A),
\]

where \(\text{adj}(A)\) is the adjugate (transpose of the cofactor
matrix). When we multiply \(\text{adj}(A)b\), each component naturally
becomes a determinant with one column replaced by \(b\). This is exactly
Cramer's Rule.

\subsubsection{Geometric
Interpretation}\label{geometric-interpretation-13}

\begin{itemize}
\tightlist
\item
  The denominator \(\det(A)\) represents the volume of the parallelotope
  spanned by the columns of \(A\).
\item
  The numerator \(\det(A_i)\) represents the volume when the \(i\)-th
  column is replaced by \(b\).
\item
  The ratio tells how much of the volume contribution is aligned with
  the \(i\)-th direction, giving the solution coordinate.
\end{itemize}

\subsubsection{Efficiency and
Limitations}\label{efficiency-and-limitations}

\begin{itemize}
\tightlist
\item
  Good for small \(n\): Elegant for 2×2 or 3×3 systems.
\item
  Inefficient for large \(n\): Requires computing \(n+1\) determinants,
  each with factorial complexity if done by cofactor expansion.
\item
  Numerical instability: Determinants can be sensitive to rounding
  errors.
\item
  In practice, Gaussian elimination or LU decomposition is far superior.
\end{itemize}

\subsubsection{Everyday Analogies}\label{everyday-analogies-55}

\begin{itemize}
\tightlist
\item
  Recipe substitution: Replacing one ingredient (column) with the
  desired outcome (b) gives the exact proportion for that component.
\item
  Voting systems: The total determinant is the baseline ``influence
  volume,'' and replacing a column measures one voter's impact on the
  outcome.
\item
  Maps: Changing one coordinate axis to match a destination vector shows
  the exact contribution of that axis to reaching the destination.
\end{itemize}

\subsubsection{Applications}\label{applications-21}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Theoretical proofs: Establishes uniqueness of solutions for small
  systems.
\item
  Geometry: Connects solutions to ratios of volumes of parallelotopes.
\item
  Symbolic algebra: Useful for deriving closed-form expressions.
\item
  Control theory: Sometimes applied in proofs of
  controllability/observability.
\end{enumerate}

\subsubsection{Why It Matters}\label{why-it-matters-55}

\begin{itemize}
\tightlist
\item
  Provides a clear formula linking determinants and solutions of linear
  systems.
\item
  Demonstrates the power of determinants as more than just volume
  measures.
\item
  Acts as a conceptual bridge between algebraic solutions and geometric
  interpretations.
\end{itemize}

\subsubsection{Try It Yourself}\label{try-it-yourself-58}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Solve \(\begin{cases} x + 2y = 3 \\ 4x + 5y = 6 \end{cases}\) using
  Cramer's Rule.
\item
  For the 3×3 system with matrix
  \(\begin{bmatrix} 1 & 2 & 3 \\ 0 & 1 & 4 \\ 5 & 6 & 0 \end{bmatrix}\),
  compute \(x_1\) using Cramer's Rule.
\item
  Verify that when \(\det(A)=0\), Cramer's Rule breaks down, matching
  the fact that the system is either inconsistent or has infinitely many
  solutions.
\item
  Challenge: Derive Cramer's Rule from the adjugate matrix formula.
\end{enumerate}

Cramer's Rule is not a computational workhorse, but it elegantly ties
together determinants, invertibility, and the solution of linear
systems-showing how geometry, algebra, and computation meet in one neat
formula.

\subsection{60. Computing Determinants in
Practice}\label{computing-determinants-in-practice}

Determinants carry deep meaning, but when it comes to actual
computation, the method you choose makes all the difference. For small
matrices, formulas like cofactor expansion or Cramer's Rule are
manageable. For larger systems, however, these direct approaches quickly
become inefficient. Practical computation relies on systematic
algorithms that exploit structure-especially elimination and matrix
factorizations.

\subsubsection{Small Matrices (n ≤ 3)}\label{small-matrices-n-3}

\begin{itemize}
\item
  2×2 case:

  \[
  \det \begin{bmatrix} a & b \\ c & d \end{bmatrix} = ad - bc.
  \]
\item
  3×3 case: Either expand by cofactors or use the ``rule of Sarrus'':

  \[
  \det \begin{bmatrix} 
  a & b & c \\ 
  d & e & f \\ 
  g & h & i 
  \end{bmatrix} = aei + bfg + cdh - ceg - bdi - afh.
  \]
\end{itemize}

These formulas are compact, but do not generalize well beyond
\(3 \times 3\).

\subsubsection{Large Matrices: Elimination and LU
Decomposition}\label{large-matrices-elimination-and-lu-decomposition}

For \(n > 3\), practical methods revolve around Gaussian elimination.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Row Reduction:

  \begin{itemize}
  \item
    Reduce \(A\) to an upper triangular matrix \(U\) using row
    operations.
  \item
    Keep track of operations:

    \begin{itemize}
    \tightlist
    \item
      Row swaps → flip sign of determinant.
    \item
      Row scaling → multiply determinant by the scaling factor.
    \item
      Row replacements → no effect.
    \end{itemize}
  \item
    Once triangular, compute determinant as the product of diagonal
    entries.
  \end{itemize}
\item
  LU Factorization:

  \begin{itemize}
  \tightlist
  \item
    Express \(A = LU\), where \(L\) is lower triangular and \(U\) is
    upper triangular.
  \item
    Then \(\det(A) = \det(L)\det(U)\).
  \item
    Since \(L\) has 1s on its diagonal, \(\det(L)=1\), so the
    determinant is just the product of diagonals of \(U\).
  \end{itemize}
\end{enumerate}

This approach reduces the complexity to \(O(n^3)\), far more efficient
than the factorial growth of cofactor expansion.

\subsubsection{Numerical Considerations}\label{numerical-considerations}

\begin{itemize}
\tightlist
\item
  Floating-Point Stability: Determinants can be very large or very
  small, leading to overflow or underflow in computers.
\item
  Pivoting: In practice, partial pivoting ensures stability during
  elimination.
\item
  Condition Number: If a matrix is nearly singular (\(\det(A)\) close to
  0), computed determinants may be highly inaccurate.
\end{itemize}

For these reasons, in numerical linear algebra, determinants are rarely
computed directly; instead, properties of LU or QR factorizations are
used.

\subsubsection{Determinant via
Eigenvalues}\label{determinant-via-eigenvalues}

Since the determinant equals the product of eigenvalues,

\[
\det(A) = \lambda_1 \lambda_2 \cdots \lambda_n,
\]

it can be computed by finding eigenvalues (numerically via QR iteration
or other methods). This is useful when eigenvalues are already needed,
but computing them just for the determinant is often more expensive than
elimination.

\subsubsection{Special Matrices}\label{special-matrices}

\begin{itemize}
\tightlist
\item
  Diagonal or triangular matrices: Determinant is product of
  diagonals-fastest case.
\item
  Block diagonal matrices: Determinant is the product of determinants of
  blocks.
\item
  Sparse matrices: Exploit structure-only nonzero patterns matter.
\item
  Orthogonal matrices: Determinant is always \(+1\) or \(-1\).
\end{itemize}

\subsubsection{Everyday Analogies}\label{everyday-analogies-56}

\begin{itemize}
\tightlist
\item
  Scaling recipes: For a small dish, you can calculate portions by hand.
  For a banquet, you need a system (LU decomposition).
\item
  Construction: Measuring the area of a single tile is simple; measuring
  the area of a complex floor plan requires breaking it into manageable
  blocks.
\item
  Finance: Computing compound growth for one investment is
  straightforward; doing it for a large, interconnected portfolio
  requires systematic methods.
\end{itemize}

\subsubsection{Applications}\label{applications-22}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  System solving: Determinants test invertibility, but actual solving
  uses elimination.
\item
  Computer graphics: Determinants detect orientation flips (useful for
  rendering).
\item
  Optimization: Determinants of Hessians signal curvature and stability.
\item
  Statistics: Determinants of covariance matrices measure uncertainty
  volumes.
\item
  Physics: Determinants appear in Jacobians for change of variables in
  integrals.
\end{enumerate}

\subsubsection{Why It Matters}\label{why-it-matters-56}

\begin{itemize}
\tightlist
\item
  Determinants provide a global property of matrices, but computation
  must be efficient.
\item
  Direct expansion is elegant but impractical.
\item
  Elimination-based methods balance theory, speed, and reliability,
  forming the backbone of modern computational linear algebra.
\end{itemize}

\subsubsection{Try It Yourself}\label{try-it-yourself-59}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute the determinant of
  \(\begin{bmatrix} 2 & 1 & 3 \\ 4 & 1 & 7 \\ -2 & 5 & 1 \end{bmatrix}\)
  using elimination, confirming the diagonal product method.
\item
  For a diagonal matrix with entries \((2, 3, -1, 5)\), verify that the
  determinant is simply their product.
\item
  Use LU decomposition to compute the determinant of a \(3 \times 3\)
  matrix of your choice.
\item
  Challenge: Show that determinant computation by LU requires only
  \(O(n^3)\) operations, while cofactor expansion requires \(O(n!)\).
\end{enumerate}

Determinants are central, but in practice they are best approached with
systematic algorithms, where triangular forms and factorizations reveal
the answer quickly and reliably.

\subsubsection{Closing}\label{closing-5}

\begin{verbatim}
Flatness or fullness,
determinants quietly weigh
depth in every move.
\end{verbatim}

\section{Chapter 7. Eigenvalues, eigenvectors, and
dynamics}\label{chapter-7.-eigenvalues-eigenvectors-and-dynamics-1}

\subsubsection{Opening}\label{opening-5}

\begin{verbatim}
Stillness in motion,
directions that never fade,
time reveals its core.
\end{verbatim}

\subsection{61. Eigenvalues and
Eigenvectors}\label{eigenvalues-and-eigenvectors}

Among all the concepts in linear algebra, few are as central and
powerful as eigenvalues and eigenvectors. They reveal the hidden ``axes
of action'' of a linear transformation-directions in space where the
transformation behaves in the simplest possible way. Instead of mixing
and rotating everything, an eigenvector is left unchanged in direction,
scaled only by its corresponding eigenvalue.

\subsubsection{The Core Idea}\label{the-core-idea-1}

Let \(A\) be an \(n \times n\) matrix. A nonzero vector
\(v \in \mathbb{R}^n\) is called an eigenvector of \(A\) if

\[
Av = \lambda v,
\]

for some scalar \(\lambda \in \mathbb{R}\) (or \(\mathbb{C}\)). The
scalar \(\lambda\) is the eigenvalue corresponding to \(v\).

\begin{itemize}
\tightlist
\item
  Eigenvector: A special direction that is preserved by the
  transformation.
\item
  Eigenvalue: The factor by which the eigenvector is stretched or
  compressed.
\end{itemize}

If \(\lambda > 1\), the eigenvector is stretched. If
\(0 < \lambda < 1\), it is compressed. If \(\lambda < 0\), it is flipped
in direction and scaled. If \(\lambda = 0\), the vector is flattened to
zero.

\subsubsection{Why They Matter}\label{why-they-matter-1}

Eigenvalues and eigenvectors describe the intrinsic structure of a
transformation:

\begin{itemize}
\tightlist
\item
  They give preferred directions in which the action of the matrix is
  simplest.
\item
  They summarize long-term behavior of repeated applications (e.g.,
  powers of \(A\)).
\item
  They connect algebra, geometry, and applications in physics, data
  science, and engineering.
\end{itemize}

\subsubsection{Example: A Simple 2D
Case}\label{example-a-simple-2d-case}

Let

\[
A = \begin{bmatrix} 2 & 0 \\ 0 & 3 \end{bmatrix}.
\]

\begin{itemize}
\item
  Applying \(A\) to \((1,0)\):

  \[
  A \begin{bmatrix} 1 \\ 0 \end{bmatrix} = \begin{bmatrix} 2 \\ 0 \end{bmatrix} = 2 \begin{bmatrix} 1 \\ 0 \end{bmatrix}.
  \]

  So \((1,0)\) is an eigenvector with eigenvalue \(2\).
\item
  Applying \(A\) to \((0,1)\):

  \[
  A \begin{bmatrix} 0 \\ 1 \end{bmatrix} = \begin{bmatrix} 0 \\ 3 \end{bmatrix} = 3 \begin{bmatrix} 0 \\ 1 \end{bmatrix}.
  \]

  So \((0,1)\) is an eigenvector with eigenvalue \(3\).
\end{itemize}

Here the eigenvectors align with the coordinate axes, and the
eigenvalues are the diagonal entries.

\subsubsection{General Case: The Eigenvalue
Equation}\label{general-case-the-eigenvalue-equation}

To find eigenvalues, we solve

\[
Av = \lambda v \quad \Leftrightarrow \quad (A - \lambda I)v = 0.
\]

For nontrivial \(v\), the matrix \((A - \lambda I)\) must be singular:

\[
\det(A - \lambda I) = 0.
\]

This determinant expands to the characteristic polynomial, whose roots
are the eigenvalues. Eigenvectors come from solving the corresponding
null spaces.

\subsubsection{Geometric
Interpretation}\label{geometric-interpretation-14}

\begin{itemize}
\tightlist
\item
  Eigenvectors are invariant directions. When you apply \(A\), the
  vector may stretch or flip, but it does not rotate off its line.
\item
  Eigenvalues are scaling factors. They describe how much stretching,
  shrinking, or flipping happens along that invariant direction.
\end{itemize}

For example:

\begin{itemize}
\tightlist
\item
  In 2D, an eigenvector might be a line through the origin where the
  transformation acts as a stretch.
\item
  In 3D, planes of shear often have eigenvectors along axes of
  invariance.
\end{itemize}

\subsubsection{Dynamics and Repeated
Applications}\label{dynamics-and-repeated-applications}

One reason eigenvalues are so important is that they describe repeated
transformations:

\[
A^k v = \lambda^k v.
\]

If you apply \(A\) repeatedly to an eigenvector, the result is
predictable: just multiply by \(\lambda^k\). This explains stability in
dynamical systems, growth in population models, and convergence in
Markov chains.

\begin{itemize}
\tightlist
\item
  If \(|\lambda| < 1\), repeated applications shrink the vector to zero.
\item
  If \(|\lambda| > 1\), the vector grows without bound.
\item
  If \(\lambda = 1\), the vector stays the same length (though direction
  may flip if \(\lambda=-1\)).
\end{itemize}

\subsubsection{Everyday Analogies}\label{everyday-analogies-57}

\begin{itemize}
\tightlist
\item
  Echo in a room: Certain tones (eigenvalues) resonate because the room
  geometry preserves them.
\item
  Business growth: An eigenvector could represent a stable investment
  direction, with eigenvalue as the growth multiplier.
\item
  Genetics: Eigenvectors of population models describe stable
  distributions of traits, eigenvalues describe growth rates.
\item
  Traffic flow: Certain paths remain proportionally consistent over
  time; eigenvalues determine speed of growth or decay.
\end{itemize}

\subsubsection{Applications}\label{applications-23}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Physics: Vibrations of molecules, quantum energy levels, and resonance
  all rely on eigenvalues/eigenvectors.
\item
  Data Science: Principal Component Analysis (PCA) finds eigenvectors of
  covariance matrices to detect key directions of variance.
\item
  Markov Chains: Steady-state probabilities correspond to eigenvectors
  with eigenvalue 1.
\item
  Differential Equations: Eigenvalues simplify systems of linear ODEs.
\item
  Computer Graphics: Transformations like rotations and scalings can be
  analyzed with eigen-decompositions.
\end{enumerate}

\subsubsection{Why It Matters}\label{why-it-matters-57}

\begin{itemize}
\tightlist
\item
  Eigenvalues and eigenvectors reduce complex transformations to their
  simplest components.
\item
  They unify algebra (roots of characteristic polynomials), geometry
  (invariant directions), and applications (stability, resonance,
  variance).
\item
  They are the foundation for diagonalization, SVD, and spectral
  analysis, which dominate modern applied mathematics.
\end{itemize}

\subsubsection{Try It Yourself}\label{try-it-yourself-60}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute the eigenvalues and eigenvectors of
  \(\begin{bmatrix} 4 & 2 \\ 1 & 3 \end{bmatrix}\).
\item
  For \(A = \begin{bmatrix} 0 & 1 \\ -1 & 0 \end{bmatrix}\), find its
  eigenvalues. (Hint: they are complex.)
\item
  Take a random 2×2 matrix and check if its eigenvectors align with
  coordinate axes.
\item
  Challenge: Prove that eigenvectors corresponding to distinct
  eigenvalues are linearly independent.
\end{enumerate}

Eigenvalues and eigenvectors are the ``fingerprints'' of a matrix: they
capture the essential behavior of a transformation, guiding us to
understand stability, dynamics, and structure across countless
disciplines.

\subsection{62. The Characteristic
Polynomial}\label{the-characteristic-polynomial}

To uncover the eigenvalues of a matrix, we use a central tool: the
characteristic polynomial. This polynomial encodes the relationship
between a matrix and its eigenvalues. The roots of the polynomial are
precisely the eigenvalues, making it the algebraic gateway to spectral
analysis.

\subsubsection{Definition}\label{definition-3}

For a square matrix \(A \in \mathbb{R}^{n \times n}\), the
characteristic polynomial is defined as

\[
p_A(\lambda) = \det(A - \lambda I).
\]

\begin{itemize}
\tightlist
\item
  \(I\) is the identity matrix of the same size as \(A\).
\item
  The polynomial \(p_A(\lambda)\) has degree \(n\).
\item
  The eigenvalues of \(A\) are exactly the roots of \(p_A(\lambda)\).
\end{itemize}

\subsubsection{Why This Works}\label{why-this-works-1}

The eigenvalue equation is

\[
Av = \lambda v \quad \iff \quad (A - \lambda I)v = 0.
\]

For nontrivial \(v\), the matrix \(A - \lambda I\) must be singular:

\[
\det(A - \lambda I) = 0.
\]

Thus, eigenvalues are precisely the scalars \(\lambda\) for which the
determinant vanishes.

\subsubsection{Example: 2×2 Case}\label{example-22-case-3}

Let

\[
A = \begin{bmatrix} 4 & 2 \\ 1 & 3 \end{bmatrix}.
\]

Compute:

\[
p_A(\lambda) = \det \begin{bmatrix} 4-\lambda & 2 \\ 1 & 3-\lambda \end{bmatrix}.
\]

Expanding:

\[
p_A(\lambda) = (4-\lambda)(3-\lambda) - 2.
\]

\[
= \lambda^2 - 7\lambda + 10.
\]

The roots are \(\lambda = 5\) and \(\lambda = 2\). These are the
eigenvalues of \(A\).

\subsubsection{Example: 3×3 Case}\label{example-33-case-3}

For

\[
B = \begin{bmatrix} 2 & 0 & 0 \\ 0 & 3 & 4 \\ 0 & 4 & 9 \end{bmatrix},
\]

\[
p_B(\lambda) = \det \begin{bmatrix} 2-\lambda & 0 & 0 \\ 0 & 3-\lambda & 4 \\ 0 & 4 & 9-\lambda \end{bmatrix}.
\]

Expand:

\[
p_B(\lambda) = (2-\lambda)\big[(3-\lambda)(9-\lambda) - 16\big].
\]

\[
= (2-\lambda)(\lambda^2 - 12\lambda + 11).
\]

Roots: \(\lambda = 2, 1, 11\).

\subsubsection{Properties of the Characteristic
Polynomial}\label{properties-of-the-characteristic-polynomial}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Degree: Always degree \(n\).
\item
  Leading term: \((-1)^n \lambda^n\).
\item
  Constant term: \(\det(A)\).
\item
  Coefficient of \(\lambda^{n-1}\): \(-\text{tr}(A)\), where
  \(\text{tr}(A)\) is the trace (sum of diagonal entries).
\end{enumerate}

So:

\[
p_A(\lambda) = (-1)^n \lambda^n + (\text{tr}(A))(-1)^{n-1}\lambda^{n-1} + \cdots + \det(A).
\]

This ties together trace, determinant, and eigenvalues in one
polynomial.

\subsubsection{Geometric Meaning}\label{geometric-meaning-10}

\begin{itemize}
\tightlist
\item
  The roots of the characteristic polynomial tell us scaling factors
  along invariant directions.
\item
  In 2D: the polynomial encodes area scaling (\(\det(A)\)) and total
  stretching (\(\text{tr}(A)\)).
\item
  In higher dimensions: it condenses the complexity of \(A\) into a
  single equation whose solutions reveal the spectrum.
\end{itemize}

\subsubsection{Everyday Analogies}\label{everyday-analogies-58}

\begin{itemize}
\tightlist
\item
  Fingerprint: The characteristic polynomial uniquely identifies the
  eigenvalues of a matrix, much like a fingerprint identifies a person.
\item
  Recipe proportions: Just as ratios determine taste, the coefficients
  of the polynomial encode how trace and determinant control
  eigenvalues.
\item
  Financial portfolio: The polynomial summarizes growth rates
  (eigenvalues) into one compact formula.
\end{itemize}

\subsubsection{Applications}\label{applications-24}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Eigenvalue computation: Foundation for diagonalization and spectral
  theory.
\item
  Control theory: Stability of systems depends on eigenvalues (roots of
  the characteristic polynomial).
\item
  Differential equations: Characteristic polynomials describe natural
  frequencies and modes of oscillation.
\item
  Graph theory: The characteristic polynomial of an adjacency matrix
  encodes structural properties of the graph.
\item
  Quantum mechanics: Energy levels of quantum systems come from solving
  characteristic polynomials of operators.
\end{enumerate}

\subsubsection{Why It Matters}\label{why-it-matters-58}

\begin{itemize}
\tightlist
\item
  Provides a systematic, algebraic way to find eigenvalues.
\item
  Connects trace and determinant to deeper spectral properties.
\item
  Bridges linear algebra, polynomial theory, and geometry.
\item
  Forms the foundation for modern computational methods like QR
  iteration.
\end{itemize}

\subsubsection{Try It Yourself}\label{try-it-yourself-61}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute the characteristic polynomial of
  \(\begin{bmatrix} 1 & 1 \\ 0 & 2 \end{bmatrix}\). Find its
  eigenvalues.
\item
  Verify that the product of eigenvalues equals the determinant.
\item
  Verify that the sum of eigenvalues equals the trace.
\item
  Challenge: Prove that \(p_{AB}(\lambda) = p_{BA}(\lambda)\) for any
  \(A, B\) of the same size.
\end{enumerate}

The characteristic polynomial distills a matrix into a single algebraic
object whose roots reveal the essential dynamics of the transformation.

\subsection{63. Algebraic vs.~Geometric
Multiplicity}\label{algebraic-vs.-geometric-multiplicity}

When studying eigenvalues, it's not enough to just find the roots of the
characteristic polynomial. Each eigenvalue can appear multiple times,
and this ``multiplicity'' can be understood in two distinct but related
ways: algebraic multiplicity (how many times it appears as a root) and
geometric multiplicity (the dimension of its eigenspace). These two
multiplicities capture both the algebraic and geometric richness of
eigenvalues.

\subsubsection{Algebraic Multiplicity}\label{algebraic-multiplicity}

The algebraic multiplicity (AM) of an eigenvalue \(\lambda\) is the
number of times it appears as a root of the characteristic polynomial
\(p_A(\lambda)\).

\begin{itemize}
\tightlist
\item
  If \((\lambda - \lambda_0)^k\) divides \(p_A(\lambda)\), then the
  algebraic multiplicity of \(\lambda_0\) is \(k\).
\item
  The sum of all algebraic multiplicities equals the size of the matrix
  (\(n\)).
\end{itemize}

Example: If

\[
p_A(\lambda) = (\lambda-2)^3(\lambda+1)^2,
\]

then eigenvalue \(\lambda=2\) has AM = 3, and \(\lambda=-1\) has AM = 2.

\subsubsection{Geometric Multiplicity}\label{geometric-multiplicity}

The geometric multiplicity (GM) of an eigenvalue \(\lambda\) is the
dimension of the eigenspace corresponding to \(\lambda\):

\[
\text{GM}(\lambda) = \dim(\ker(A - \lambda I)).
\]

\begin{itemize}
\item
  This counts how many linearly independent eigenvectors correspond to
  \(\lambda\).
\item
  Always satisfies:

  \[
  1 \leq \text{GM}(\lambda) \leq \text{AM}(\lambda).
  \]
\end{itemize}

Example: If

\[
A = \begin{bmatrix} 2 & 1 \\ 0 & 2 \end{bmatrix},
\]

then \(p_A(\lambda) = (\lambda-2)^2\).

\begin{itemize}
\item
  AM of \(\lambda=2\) is 2.
\item
  Solve \((A-2I)v=0\):

  \[
  \begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix} v = 0 \quad \Rightarrow \quad v = \begin{bmatrix} 1 \\ 0 \end{bmatrix}.
  \]

  Only 1 independent eigenvector.
\item
  GM of \(\lambda=2\) is 1.
\end{itemize}

\subsubsection{Relationship Between the
Two}\label{relationship-between-the-two}

\begin{itemize}
\tightlist
\item
  Always: \(\text{GM}(\lambda) \leq \text{AM}(\lambda)\).
\item
  If they are equal for all eigenvalues, the matrix is diagonalizable.
\item
  If GM \textless{} AM for some eigenvalue, the matrix is defective,
  meaning it cannot be diagonalized, though it may still have a Jordan
  canonical form.
\end{itemize}

\subsubsection{Geometric Meaning}\label{geometric-meaning-11}

\begin{itemize}
\tightlist
\item
  AM measures how strongly the eigenvalue is ``encoded'' in the
  polynomial.
\item
  GM measures how much geometric freedom the eigenvalue's eigenspace
  provides.
\item
  If AM \textgreater{} GM, the eigenvalue ``wants'' more independent
  directions than the space allows.
\end{itemize}

Think of AM as the \emph{theoretical demand} for eigenvectors, and GM as
the \emph{actual supply}.

\subsubsection{Example: Diagonalizable
vs.~Defective}\label{example-diagonalizable-vs.-defective}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Diagonalizable case:

  \[
  B = \begin{bmatrix} 2 & 0 \\ 0 & 2 \end{bmatrix}.
  \]

  \begin{itemize}
  \tightlist
  \item
    \(p_B(\lambda) = (\lambda-2)^2\).
  \item
    AM = 2 for eigenvalue 2.
  \item
    GM = 2, since the eigenspace is all of \(\mathbb{R}^2\).
  \item
    Enough eigenvectors to diagonalize.
  \end{itemize}
\item
  Defective case: The earlier example

  \[
  A = \begin{bmatrix} 2 & 1 \\ 0 & 2 \end{bmatrix}
  \]

  had AM = 2, GM = 1.

  \begin{itemize}
  \tightlist
  \item
    Not enough eigenvectors.
  \item
    Cannot be diagonalized.
  \end{itemize}
\end{enumerate}

\subsubsection{Everyday Analogies}\label{everyday-analogies-59}

\begin{itemize}
\tightlist
\item
  Seats vs.~people: Algebraic multiplicity is the number of seats
  reserved for an eigenvalue; geometric multiplicity is how many actual
  people show up. If fewer people arrive, the system is defective.
\item
  Promises vs.~reality: AM is the theoretical promise given by the
  polynomial; GM is the reality of independent directions.
\item
  Musical notes: AM is how many times a note is written in the score; GM
  is how many distinct instruments actually play it.
\end{itemize}

\subsubsection{Applications}\label{applications-25}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Diagonalization: Only possible when GM = AM for all eigenvalues.
\item
  Jordan form: Defective matrices require Jordan blocks, governed by the
  gap between AM and GM.
\item
  Differential equations: The solution form depends on multiplicity;
  repeated eigenvalues with fewer eigenvectors require generalized
  solutions.
\item
  Stability analysis: Multiplicities reveal degeneracies in dynamical
  systems.
\item
  Quantum mechanics: Degeneracy of eigenvalues (AM vs.~GM) encodes
  physical symmetry.
\end{enumerate}

\subsubsection{Why It Matters}\label{why-it-matters-59}

\begin{itemize}
\tightlist
\item
  Multiplicities separate algebraic roots from geometric structure.
\item
  They decide whether diagonalization is possible.
\item
  They reveal hidden constraints in systems with repeated eigenvalues.
\item
  They form the basis for advanced concepts like Jordan canonical form
  and generalized eigenvectors.
\end{itemize}

\subsubsection{Try It Yourself}\label{try-it-yourself-62}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Find AM and GM for \(\begin{bmatrix} 3 & 1 \\ 0 & 3 \end{bmatrix}\).
\item
  Find AM and GM for \(\begin{bmatrix} 3 & 0 \\ 0 & 3 \end{bmatrix}\).
  Compare with the first case.
\item
  Show that AM always equals the multiplicity of a root of the
  characteristic polynomial.
\item
  Challenge: Prove that for any eigenvalue, GM ≥ 1.
\end{enumerate}

Algebraic and geometric multiplicity together tell the full story: the
algebra tells us how many times an eigenvalue appears, while the
geometry tells us how much room it really occupies in the vector space.

\subsection{64. Diagonalization}\label{diagonalization}

Diagonalization is one of the most powerful ideas in linear algebra. It
takes a complicated matrix and, when possible, rewrites it in a simple
form where its action is completely transparent. A diagonal matrix is
easy to understand: it just stretches or compresses each coordinate axis
by a fixed factor. If we can transform a matrix into a diagonal one,
many calculations-like computing powers or exponentials-become almost
trivial.

\subsubsection{The Core Concept}\label{the-core-concept}

A square matrix \(A \in \mathbb{R}^{n \times n}\) is diagonalizable if
there exists an invertible matrix \(P\) and a diagonal matrix \(D\) such
that

\[
A = P D P^{-1}.
\]

\begin{itemize}
\tightlist
\item
  The diagonal entries of \(D\) are the eigenvalues of \(A\).
\item
  The columns of \(P\) are the corresponding eigenvectors.
\end{itemize}

In words: \(A\) can be ``rewritten'' in a coordinate system made of its
eigenvectors, where its action reduces to simple scaling along
independent directions.

\subsubsection{Why Diagonalization
Matters}\label{why-diagonalization-matters}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Simplifies Computations:

  \begin{itemize}
  \item
    Computing powers:

    \[
    A^k = P D^k P^{-1}, \quad D^k \text{ is trivial to compute}.
    \]
  \item
    Matrix exponential:

    \[
    e^A = P e^D P^{-1}.
    \]

    Critical in solving differential equations.
  \end{itemize}
\item
  Clarifies Dynamics:

  \begin{itemize}
  \tightlist
  \item
    Long-term behavior of iterative processes depends directly on
    eigenvalues.
  \item
    Stable vs.~unstable systems can be read off from \(D\).
  \end{itemize}
\item
  Reveals Structure:

  \begin{itemize}
  \tightlist
  \item
    Tells us whether the system can be understood through independent
    modes.
  \item
    Connects algebraic structure with geometry.
  \end{itemize}
\end{enumerate}

\subsubsection{Conditions for
Diagonalization}\label{conditions-for-diagonalization}

A matrix \(A\) is diagonalizable if and only if it has enough linearly
independent eigenvectors to form a basis for \(\mathbb{R}^n\).

\begin{itemize}
\tightlist
\item
  Equivalently: For each eigenvalue, geometric multiplicity = algebraic
  multiplicity.
\item
  Distinct eigenvalues guarantee diagonalizability, since their
  eigenvectors are linearly independent.
\end{itemize}

\subsubsection{Example: Diagonalizable
Case}\label{example-diagonalizable-case}

Let

\[
A = \begin{bmatrix} 4 & 0 \\ 1 & 3 \end{bmatrix}.
\]

\begin{itemize}
\item
  Characteristic polynomial:

  \[
  p_A(\lambda) = (4-\lambda)(3-\lambda).
  \]

  Eigenvalues: \(\lambda_1=4, \lambda_2=3\).
\item
  Eigenvectors:

  \begin{itemize}
  \tightlist
  \item
    For \(\lambda=4\): \((1,1)^T\).
  \item
    For \(\lambda=3\): \((0,1)^T\).
  \end{itemize}
\item
  Build \(P = \begin{bmatrix} 1 & 0 \\ 1 & 1 \end{bmatrix}\),
  \(D = \begin{bmatrix} 4 & 0 \\ 0 & 3 \end{bmatrix}\).
\item
  Then \(A = P D P^{-1}\).
\end{itemize}

Now, computing \(A^{10}\) is easy: just compute \(D^{10}\) and
conjugate.

\subsubsection{Example: Defective (Non-Diagonalizable)
Case}\label{example-defective-non-diagonalizable-case}

\[
B = \begin{bmatrix} 2 & 1 \\ 0 & 2 \end{bmatrix}.
\]

\begin{itemize}
\tightlist
\item
  Characteristic polynomial: \((\lambda - 2)^2\).
\item
  AM of eigenvalue 2 is 2, but GM = 1 (only one eigenvector).
\item
  Not diagonalizable. Needs Jordan form instead.
\end{itemize}

\subsubsection{Geometric Meaning}\label{geometric-meaning-12}

Diagonalization means we can rotate into a basis of eigenvectors where
the transformation acts simply: scale each axis by its eigenvalue.

\begin{itemize}
\tightlist
\item
  Think of a room where the floor stretches more in one direction than
  another. In the right coordinate system (aligned with eigenvectors),
  the stretch is purely along axes.
\item
  Without diagonalization, stretching mixes directions and is harder to
  describe.
\end{itemize}

\subsubsection{Everyday Analogies}\label{everyday-analogies-60}

\begin{itemize}
\tightlist
\item
  Musical notes: A chord can be decomposed into independent notes.
  Diagonalization isolates each ``note'' of a transformation.
\item
  Recipe ingredients: A dish may look complex, but diagonalization
  breaks it into pure ingredients.
\item
  Business growth: A company might expand differently in separate
  divisions. In the right basis, each division grows independently, with
  its own multiplier.
\end{itemize}

\subsubsection{Applications}\label{applications-26}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Differential Equations: Solving systems of linear ODEs relies on
  diagonalization or Jordan form.
\item
  Markov Chains: Transition matrices are analyzed through
  diagonalization to study steady states.
\item
  Quantum Mechanics: Operators are diagonalized to reveal measurable
  states.
\item
  PCA (Principal Component Analysis): A covariance matrix is
  diagonalized to extract independent variance directions.
\item
  Computer Graphics: Diagonalization simplifies rotation-scaling
  transformations.
\end{enumerate}

\subsubsection{Why It Matters}\label{why-it-matters-60}

Diagonalization transforms complexity into simplicity. It exposes the
fundamental action of a matrix: scaling along preferred axes. Without
it, understanding or computing repeated transformations would be
intractable.

\subsubsection{Try It Yourself}\label{try-it-yourself-63}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Diagonalize

  \[
  C = \begin{bmatrix} 1 & 1 \\ 0 & 2 \end{bmatrix}.
  \]

  Compute \(C^5\) using \(P D^5 P^{-1}\).
\item
  Show why

  \[
  \begin{bmatrix} 2 & 1 \\ 0 & 2 \end{bmatrix}
  \]

  cannot be diagonalized.
\item
  Challenge: Prove that any symmetric real matrix is diagonalizable with
  an orthogonal basis.
\end{enumerate}

Diagonalization is like finding the natural ``language'' of a matrix:
once we listen in its native basis, everything becomes clear, elegant,
and simple.

\subsection{65. Powers of a Matrix}\label{powers-of-a-matrix}

Once we know about diagonalization, one of its most powerful
consequences is the ability to compute powers of a matrix efficiently.
Normally, multiplying a matrix by itself repeatedly is expensive and
messy. But if a matrix can be diagonalized, its powers become almost
trivial to calculate. This is crucial in understanding long-term
behavior of dynamical systems, Markov chains, and iterative algorithms.

\subsubsection{The General Principle}\label{the-general-principle}

If a matrix \(A\) is diagonalizable, then

\[
A = P D P^{-1},
\]

where \(D\) is diagonal and \(P\) is invertible.

Then for any positive integer \(k\):

\[
A^k = (P D P^{-1})^k = P D^k P^{-1}.
\]

Because \(P^{-1}P = I\), the middle terms cancel out in the product.

\begin{itemize}
\tightlist
\item
  Computing \(D^k\) is simple: just raise each diagonal entry to the
  \(k\)-th power.
\item
  Thus, eigenvalues control the growth or decay of powers of the matrix.
\end{itemize}

\subsubsection{Example: A Simple Diagonal
Case}\label{example-a-simple-diagonal-case}

Let

\[
D = \begin{bmatrix} 2 & 0 \\ 0 & 3 \end{bmatrix}.
\]

Then

\[
D^k = \begin{bmatrix} 2^k & 0 \\ 0 & 3^k \end{bmatrix}.
\]

Each eigenvalue is raised independently to the \(k\)-th power.

\subsubsection{Example: Using
Diagonalization}\label{example-using-diagonalization}

Consider

\[
A = \begin{bmatrix} 4 & 0 \\ 1 & 3 \end{bmatrix}.
\]

From before, we know it diagonalizes as

\[
A = P D P^{-1}, \quad D = \begin{bmatrix} 4 & 0 \\ 0 & 3 \end{bmatrix}.
\]

So,

\[
A^k = P \begin{bmatrix} 4^k & 0 \\ 0 & 3^k \end{bmatrix} P^{-1}.
\]

Instead of multiplying \(A\) by itself \(k\) times, we just exponentiate
the eigenvalues.

\subsubsection{Long-Term Behavior}\label{long-term-behavior}

Eigenvalues reveal exactly what happens as \(k \to \infty\).

\begin{itemize}
\tightlist
\item
  If all eigenvalues satisfy \(|\lambda| < 1\), then \(A^k \to 0\).
\item
  If some eigenvalues have \(|\lambda| > 1\), then \(A^k\) diverges
  along those eigenvector directions.
\item
  If \(|\lambda| = 1\), the behavior depends on the specific structure:
  it may oscillate, stabilize, or remain bounded.
\end{itemize}

This explains stability in recursive systems and iterative algorithms.

\subsubsection{Special Case: Markov
Chains}\label{special-case-markov-chains}

In probability, the transition matrix of a Markov chain has eigenvalues
less than or equal to 1.

\begin{itemize}
\tightlist
\item
  The largest eigenvalue is always \(\lambda = 1\).
\item
  As powers of the transition matrix grow, the chain converges to the
  eigenvector associated with \(\lambda = 1\), representing the
  stationary distribution.
\end{itemize}

Thus, \(A^k\) describes the long-run behavior of the chain.

\subsubsection{Non-Diagonalizable
Matrices}\label{non-diagonalizable-matrices}

If a matrix is not diagonalizable, things become more complicated. Such
matrices require the Jordan canonical form, where blocks can lead to
terms like \(k \lambda^{k-1}\).

Example:

\[
B = \begin{bmatrix} 2 & 1 \\ 0 & 2 \end{bmatrix}.
\]

Then

\[
B^k = \begin{bmatrix} 2^k & k 2^{k-1} \\ 0 & 2^k \end{bmatrix}.
\]

The presence of the off-diagonal entry introduces linear growth in
\(k\), in addition to exponential scaling.

\subsubsection{Geometric Meaning}\label{geometric-meaning-13}

\begin{itemize}
\tightlist
\item
  Powers of \(A\) correspond to repeated application of the linear
  transformation.
\item
  Eigenvalues dictate whether directions expand, shrink, or remain
  steady.
\item
  The eigenvectors mark the axes along which the repeated action is
  simplest to describe.
\end{itemize}

Think of stretching a rubber sheet: after each stretch, the sheet aligns
more and more strongly with the dominant eigenvector.

\subsubsection{Everyday Analogies}\label{everyday-analogies-61}

\begin{itemize}
\tightlist
\item
  Interest rates: If you repeatedly apply interest, the growth
  multiplies exponentially, just like powers of eigenvalues.
\item
  Echoes: Each echo in a room is weaker (if eigenvalues \textless{} 1)
  or stronger (if eigenvalues \textgreater{} 1).
\item
  Business growth: Repeated investment growth is governed by the largest
  eigenvalue, which dominates in the long run.
\end{itemize}

\subsubsection{Applications}\label{applications-27}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Dynamical Systems: Population models, economic growth, and iterative
  algorithms all rely on powers of a matrix.
\item
  Markov Chains: Powers reveal equilibrium behavior and mixing rates.
\item
  Differential Equations: Discrete-time models use matrix powers to
  describe state evolution.
\item
  Computer Graphics: Repeated transformations can be analyzed via
  eigenvalues.
\item
  Machine Learning: Convergence of iterative solvers (like gradient
  descent with linear updates) depends on spectral radius.
\end{enumerate}

\subsubsection{Why It Matters}\label{why-it-matters-61}

Matrix powers are the foundation of stability analysis, asymptotic
behavior, and convergence. Diagonalization turns this from a brute-force
multiplication into a deep, structured understanding.

\subsubsection{Try It Yourself}\label{try-it-yourself-64}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute \(A^5\) for \(\begin{bmatrix} 2 & 0 \\ 0 & 3 \end{bmatrix}\).
\item
  For \(\begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix}\), compute \(A^k\).
  What happens as \(k \to \infty\)?
\item
  Explore what happens to \(A^k\) when the largest eigenvalue has
  absolute value \textless{} 1, = 1, and \textgreater{} 1.
\item
  Challenge: Show that if a diagonalizable matrix has eigenvalues
  \(|\lambda_i| < 1\), then \(\lim_{k \to \infty} A^k = 0\).
\end{enumerate}

Powers of a matrix reveal the story of repetition: how a transformation
evolves when applied again and again. They connect linear algebra to
time, growth, and stability in every system that unfolds step by step.

\subsection{66. Real vs.~Complex
Spectra}\label{real-vs.-complex-spectra}

Not all eigenvalues are real numbers. Even when working with real
matrices, eigenvalues can emerge as complex numbers. Understanding when
eigenvalues are real, when they are complex, and what this means
geometrically is critical for grasping the full behavior of linear
transformations.

\subsubsection{Eigenvalues Over the Complex
Numbers}\label{eigenvalues-over-the-complex-numbers}

Every square matrix \(A \in \mathbb{R}^{n \times n}\) has at least one
eigenvalue in the complex numbers. This is guaranteed by the Fundamental
Theorem of Algebra, which says every polynomial (like the characteristic
polynomial) has roots in \(\mathbb{C}\).

\begin{itemize}
\tightlist
\item
  If \(p_A(\lambda)\) has only real roots, all eigenvalues are real.
\item
  If \(p_A(\lambda)\) has quadratic factors with no real roots, then
  eigenvalues appear as complex conjugate pairs.
\end{itemize}

\subsubsection{Why Complex Numbers
Appear}\label{why-complex-numbers-appear}

Consider a 2D rotation matrix:

\[
R_\theta = \begin{bmatrix} \cos\theta & -\sin\theta \\ \sin\theta & \cos\theta \end{bmatrix}.
\]

The characteristic polynomial is

\[
p(\lambda) = \lambda^2 - 2\cos\theta \lambda + 1.
\]

The eigenvalues are

\[
\lambda = \cos\theta \pm i \sin\theta = e^{\pm i\theta}.
\]

\begin{itemize}
\tightlist
\item
  Unless \(\theta = 0, \pi\), these eigenvalues are not real.
\item
  Geometrically, this makes sense: pure rotation has no invariant real
  direction. Instead, the eigenvalues are complex numbers of unit
  magnitude, encoding the rotation angle.
\end{itemize}

\subsubsection{Real vs.~Complex
Scenarios}\label{real-vs.-complex-scenarios}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Symmetric Real Matrices:

  \begin{itemize}
  \tightlist
  \item
    All eigenvalues are real.
  \item
    Eigenvectors form an orthogonal basis.
  \item
    Example: \(\begin{bmatrix} 2 & 1 \\ 1 & 2 \end{bmatrix}\) has
    eigenvalues \(3, 1\).
  \end{itemize}
\item
  General Real Matrices:

  \begin{itemize}
  \tightlist
  \item
    Eigenvalues may be complex.
  \item
    If complex, they always come in conjugate pairs: if
    \(\lambda = a+bi\), then \(\overline{\lambda} = a-bi\) is also an
    eigenvalue.
  \end{itemize}
\item
  Skew-Symmetric Matrices:

  \begin{itemize}
  \tightlist
  \item
    Purely imaginary eigenvalues.
  \item
    Example: \(\begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix}\) has
    eigenvalues \(\pm i\).
  \end{itemize}
\end{enumerate}

\subsubsection{Geometric Meaning of Complex
Eigenvalues}\label{geometric-meaning-of-complex-eigenvalues}

\begin{itemize}
\tightlist
\item
  If eigenvalues are real, the transformation scales along real
  directions.
\item
  If eigenvalues are complex, the transformation involves a combination
  of rotation and scaling.
\end{itemize}

For \(\lambda = re^{i\theta}\):

\begin{itemize}
\tightlist
\item
  \(r = |\lambda|\) controls expansion or contraction.
\item
  \(\theta\) controls rotation.
\end{itemize}

So a complex eigenvalue represents a spiral: stretching or shrinking
while rotating.

\subsubsection{Example: Spiral Dynamics}\label{example-spiral-dynamics}

Matrix

\[
A = \begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix}
\]

rotates vectors by 90°.

\begin{itemize}
\tightlist
\item
  Eigenvalues: \(\pm i\).
\item
  Magnitude = 1, angle = \(\pi/2\).
\item
  Interpretation: every step is a rotation of 90°, with no scaling.
\end{itemize}

If we change to

\[
B = \begin{bmatrix} 0.8 & -0.6 \\ 0.6 & 0.8 \end{bmatrix},
\]

the eigenvalues are complex with modulus \textless{} 1.

\begin{itemize}
\tightlist
\item
  Interpretation: rotation combined with shrinking → spiraling toward
  the origin.
\end{itemize}

\subsubsection{Everyday Analogies}\label{everyday-analogies-62}

\begin{itemize}
\tightlist
\item
  Clock hands: Rotation without stretching is like clock hands
  moving-direction changes continuously but length stays the same.
\item
  Spiral staircase: Each step forward involves both rising and rotating,
  just like scaling and rotation in complex eigenvalues.
\item
  Musical pitch shifts: A note can rise in pitch (rotation) while fading
  in volume (scaling).
\end{itemize}

\subsubsection{Applications}\label{applications-28}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Differential Equations: Complex eigenvalues produce oscillatory
  solutions with sine and cosine terms.
\item
  Physics: Vibrations and wave phenomena rely on complex eigenvalues to
  model periodic behavior.
\item
  Control Systems: Stability requires checking magnitudes of eigenvalues
  in the complex plane.
\item
  Computer Graphics: Rotations and spiral motions are naturally
  described by complex spectra.
\item
  Signal Processing: Fourier transforms rely on complex eigenstructures
  of convolution operators.
\end{enumerate}

\subsubsection{Why It Matters}\label{why-it-matters-62}

\begin{itemize}
\tightlist
\item
  Real eigenvalues describe pure stretching or compression.
\item
  Complex eigenvalues describe combined rotation and scaling.
\item
  Together, they provide a complete picture of matrix behavior in both
  real and complex spaces.
\item
  Without considering complex eigenvalues, we miss entire classes of
  transformations, like rotation and oscillation.
\end{itemize}

\subsubsection{Try It Yourself}\label{try-it-yourself-65}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Find eigenvalues of \(\begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix}\).
  Interpret geometrically.
\item
  For rotation by 45°, find eigenvalues of
  \(\begin{bmatrix} \cos\frac{\pi}{4} & -\sin\frac{\pi}{4} \\ \sin\frac{\pi}{4} & \cos\frac{\pi}{4} \end{bmatrix}\).
  Show that they are \(e^{\pm i\pi/4}\).
\item
  Check eigenvalues of
  \(\begin{bmatrix} 2 & -5 \\ 1 & -2 \end{bmatrix}\). Are they real or
  complex?
\item
  Challenge: Prove that real polynomials of odd degree always have at
  least one real root. Connect this to eigenvalues of odd-dimensional
  real matrices.
\end{enumerate}

Complex spectra extend our understanding of linear algebra into the full
richness of oscillations, rotations, and spirals, where numbers alone
are not enough-geometry and complex analysis merge to reveal the truth.

\subsection{67. Defective Matrices and Jordan Form (a
Glimpse)}\label{defective-matrices-and-jordan-form-a-glimpse}

Not every matrix can be simplified all the way into a diagonal form.
Some matrices, while having repeated eigenvalues, do not have enough
independent eigenvectors to span the entire space. These are called
defective matrices. Understanding them requires introducing the Jordan
canonical form, a generalization of diagonalization that handles these
tricky cases.

\subsubsection{Defective Matrices}\label{defective-matrices}

A square matrix \(A \in \mathbb{R}^{n \times n}\) is called defective
if:

\begin{itemize}
\tightlist
\item
  It has an eigenvalue \(\lambda\) with algebraic multiplicity (AM)
  strictly larger than its geometric multiplicity (GM).
\item
  Equivalently, \(A\) does not have enough linearly independent
  eigenvectors to form a full basis of \(\mathbb{R}^n\).
\end{itemize}

Example:

\[
A = \begin{bmatrix} 2 & 1 \\ 0 & 2 \end{bmatrix}.
\]

\begin{itemize}
\item
  Characteristic polynomial: \((\lambda - 2)^2\), so AM = 2.
\item
  Solving \((A - 2I)v = 0\):

  \[
  \begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix}v = 0 \quad \Rightarrow \quad v = \begin{bmatrix} 1 \\ 0 \end{bmatrix}.
  \]

  Only one independent eigenvector → GM = 1.
\item
  Since GM \textless{} AM, this matrix is defective.
\end{itemize}

Defective matrices cannot be diagonalized.

\subsubsection{Why Defective Matrices
Exist}\label{why-defective-matrices-exist}

Diagonalization requires one independent eigenvector per eigenvalue
copy. But sometimes the matrix ``collapses'' those directions together,
producing fewer eigenvectors than expected.

\begin{itemize}
\tightlist
\item
  Think of it like having multiple musical notes written in the score
  (AM), but fewer instruments available to play them (GM).
\item
  The matrix ``wants'' more independent directions, but the geometry of
  its null spaces prevents that.
\end{itemize}

\subsubsection{Jordan Canonical Form
(Intuition)}\label{jordan-canonical-form-intuition}

While defective matrices cannot be diagonalized, they can still be put
into a nearly diagonal form called the Jordan canonical form (JCF):

\[
J = P^{-1} A P,
\]

where \(J\) consists of Jordan blocks:

\[
J_k(\lambda) = \begin{bmatrix} 
\lambda & 1 & 0 & \cdots & 0 \\
0 & \lambda & 1 & \cdots & 0 \\
0 & 0 & \lambda & \cdots & 0 \\
\vdots & \vdots & \vdots & \ddots & 1 \\
0 & 0 & 0 & \cdots & \lambda
\end{bmatrix}.
\]

Each block corresponds to one eigenvalue \(\lambda\), with 1s on the
superdiagonal indicating the lack of independent eigenvectors.

\begin{itemize}
\tightlist
\item
  If every block is \(1 \times 1\), the matrix is diagonalizable.
\item
  If larger blocks appear, the matrix is defective.
\end{itemize}

\subsubsection{Example: Jordan Block of Size
2}\label{example-jordan-block-of-size-2}

The earlier defective example

\[
A = \begin{bmatrix} 2 & 1 \\ 0 & 2 \end{bmatrix}
\]

has Jordan form

\[
J = \begin{bmatrix} 2 & 1 \\ 0 & 2 \end{bmatrix}.
\]

Notice it is already in Jordan form: one block of size 2 for eigenvalue
2.

\subsubsection{Powers of Jordan Blocks}\label{powers-of-jordan-blocks}

A key property is how powers behave. For

\[
J = \begin{bmatrix} \lambda & 1 \\ 0 & \lambda \end{bmatrix},
\]

\[
J^k = \begin{bmatrix} \lambda^k & k\lambda^{k-1} \\ 0 & \lambda^k \end{bmatrix}.
\]

\begin{itemize}
\tightlist
\item
  Unlike diagonal matrices, extra polynomial terms in \(k\) appear.
\item
  This explains why defective matrices produce behavior like growth
  proportional to \(k \lambda^{k-1}\).
\end{itemize}

\subsubsection{Geometric Meaning}\label{geometric-meaning-14}

\begin{itemize}
\tightlist
\item
  Eigenvectors describe invariant lines.
\item
  When there aren't enough eigenvectors, Jordan form encodes chains of
  generalized eigenvectors.
\item
  Each chain captures how the matrix transforms vectors slightly off the
  invariant line, nudging them along directions linked together by the
  Jordan block.
\end{itemize}

So while a diagonalizable matrix decomposes space into neat independent
directions, a defective matrix entangles some directions together,
forcing them into chains.

\subsubsection{Everyday Analogies}\label{everyday-analogies-63}

\begin{itemize}
\tightlist
\item
  Orchestra analogy: A diagonalizable matrix has one instrument per
  note. A defective matrix has fewer instruments, so some notes must be
  ``shared'' in harmonies, represented by chains.
\item
  River flow: Instead of independent straight channels, some currents
  merge and pull each other, causing dependencies.
\item
  Office hierarchy: Instead of each manager having their own team
  (independent eigenvectors), some teams overlap, producing chains of
  influence.
\end{itemize}

\subsubsection{Applications}\label{applications-29}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Differential Equations: Jordan blocks determine the appearance of
  extra polynomial factors (like \(t e^{\lambda t}\)) in solutions.
\item
  Markov Chains: Non-diagonalizable transition matrices produce slower
  convergence to steady states.
\item
  Numerical Analysis: Algorithms may fail or slow down if the system
  matrix is defective.
\item
  Control Theory: Stability depends not just on eigenvalues, but on
  whether the matrix is diagonalizable.
\item
  Quantum Mechanics: Degenerate eigenvalues require Jordan analysis to
  fully describe states.
\end{enumerate}

\subsubsection{Why It Matters}\label{why-it-matters-63}

\begin{itemize}
\tightlist
\item
  Diagonalization is not always possible, and defective matrices are the
  exceptions.
\item
  Jordan form is the universal fallback: every square matrix has one,
  and it generalizes diagonalization.
\item
  It introduces generalized eigenvectors, which extend the reach of
  spectral theory.
\end{itemize}

\subsubsection{Try It Yourself}\label{try-it-yourself-66}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Verify that \(\begin{bmatrix} 3 & 1 \\ 0 & 3 \end{bmatrix}\) is
  defective. Find its Jordan form.
\item
  Show that for a Jordan block of size 3,

  \[
  J^k = \lambda^k I + k \lambda^{k-1} N + \frac{k(k-1)}{2}\lambda^{k-2} N^2,
  \]

  where \(N\) is the nilpotent part (matrix with 1s above diagonal).
\item
  Compare the behavior of \(A^k\) for a diagonalizable vs.~a defective
  matrix with the same eigenvalues.
\item
  Challenge: Prove that every square matrix has a Jordan form over the
  complex numbers.
\end{enumerate}

Defective matrices and Jordan form show us that even when eigenvectors
are ``insufficient,'' we can still impose structure, capturing how
linear transformations behave in their most fundamental building blocks.

\subsection{68. Stability and Spectral
Radius}\label{stability-and-spectral-radius}

When a matrix is applied repeatedly-through iteration, recursion, or
dynamical systems-its long-term behavior is governed not by individual
entries, but by its eigenvalues. The key measure here is the spectral
radius, which tells us whether repeated applications lead to
convergence, oscillation, or divergence.

\subsubsection{The Spectral Radius}\label{the-spectral-radius}

The spectral radius of a matrix \(A\) is defined as

\[
\rho(A) = \max \{ |\lambda| : \lambda \text{ is an eigenvalue of } A \}.
\]

\begin{itemize}
\tightlist
\item
  It is the largest absolute value among all eigenvalues.
\item
  If \(|\lambda| > 1\), the eigenvalue leads to exponential growth along
  its eigenvector.
\item
  If \(|\lambda| < 1\), it leads to exponential decay.
\item
  If \(|\lambda| = 1\), behavior depends on whether the eigenvalue is
  simple or defective.
\end{itemize}

\subsubsection{Stability in Iterative
Systems}\label{stability-in-iterative-systems}

Consider a recursive process:

\[
x_{k+1} = A x_k.
\]

\begin{itemize}
\tightlist
\item
  If \(\rho(A) < 1\), then \(A^k \to 0\) as \(k \to \infty\). All
  trajectories converge to the origin.
\item
  If \(\rho(A) > 1\), then \(A^k\) grows without bound along the
  dominant eigenvector.
\item
  If \(\rho(A) = 1\), trajectories neither vanish nor diverge but may
  oscillate or stagnate.
\end{itemize}

\subsubsection{Example: Convergence with Small Spectral
Radius}\label{example-convergence-with-small-spectral-radius}

\[
A = \begin{bmatrix} 0.5 & 0 \\ 0 & 0.8 \end{bmatrix}.
\]

\begin{itemize}
\tightlist
\item
  Eigenvalues: \(0.5, 0.8\).
\item
  \(\rho(A) = 0.8 < 1\).
\item
  Powers \(A^k\) shrink vectors to zero → stable system.
\end{itemize}

\subsubsection{Example: Divergence with Large Spectral
Radius}\label{example-divergence-with-large-spectral-radius}

\[
B = \begin{bmatrix} 2 & 0 \\ 0 & 0.5 \end{bmatrix}.
\]

\begin{itemize}
\tightlist
\item
  Eigenvalues: \(2, 0.5\).
\item
  \(\rho(B) = 2 > 1\).
\item
  Powers \(B^k\) explode along the eigenvector \((1,0)\).
\end{itemize}

\subsubsection{Example: Oscillation with Complex
Eigenvalues}\label{example-oscillation-with-complex-eigenvalues}

\[
C = \begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix}.
\]

\begin{itemize}
\tightlist
\item
  Eigenvalues: \(\pm i\), both with modulus 1.
\item
  \(\rho(C) = 1\).
\item
  System is neutrally stable: vectors rotate forever without shrinking
  or growing.
\end{itemize}

\subsubsection{Beyond Simple Stability: Defective
Cases}\label{beyond-simple-stability-defective-cases}

If a matrix has eigenvalues with \(|\lambda|=1\) and is defective, extra
polynomial terms in \(k\) appear in \(A^k\), leading to slow divergence
even though \(\rho(A)=1\).

Example:

\[
D = \begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix}.
\]

\begin{itemize}
\item
  Eigenvalue: \(\lambda=1\) (AM=2, GM=1).
\item
  \(\rho(D)=1\).
\item
  Powers grow linearly with \(k\):

  \[
  D^k = \begin{bmatrix} 1 & k \\ 0 & 1 \end{bmatrix}.
  \]
\item
  System is unstable, despite spectral radius equal to 1.
\end{itemize}

\subsubsection{Geometric Meaning}\label{geometric-meaning-15}

The spectral radius measures the dominant mode of a transformation:

\begin{itemize}
\tightlist
\item
  Imagine stretching and rotating a rubber sheet. After many
  repetitions, the sheet aligns with the direction corresponding to the
  largest eigenvalue.
\item
  If the stretching is less than 1, everything shrinks.
\item
  If greater than 1, everything expands.
\item
  If exactly 1, the system is balanced on the edge of stability.
\end{itemize}

\subsubsection{Everyday Analogies}\label{everyday-analogies-64}

\begin{itemize}
\tightlist
\item
  Population dynamics: If the reproduction factor (largest eigenvalue)
  is below 1, a species dies out; above 1, it grows; at 1, it balances.
\item
  Bank interest: Interest rate \textless{} 1 shrinks your balance;
  \textgreater{} 1 grows it; = 1 keeps it steady.
\item
  Echoes in a hall: Each echo fades if \(\rho<1\), persists if
  \(\rho=1\), and grows chaotic if \(\rho>1\).
\end{itemize}

\subsubsection{Applications}\label{applications-30}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Numerical Methods: Convergence of iterative solvers (e.g., Jacobi,
  Gauss--Seidel) depends on spectral radius \textless{} 1.
\item
  Markov Chains: Long-term distributions exist if the largest eigenvalue
  = 1 and others \textless{} 1 in magnitude.
\item
  Control Theory: System stability is judged by eigenvalues inside the
  unit circle (\(|\lambda| < 1\)).
\item
  Economics: Input-output models remain bounded only if spectral radius
  \textless{} 1.
\item
  Epidemiology: Basic reproduction number \(R_0\) is essentially the
  spectral radius of a next-generation matrix.
\end{enumerate}

\subsubsection{Why It Matters}\label{why-it-matters-64}

\begin{itemize}
\tightlist
\item
  The spectral radius condenses the entire spectrum of a matrix into a
  single stability criterion.
\item
  It predicts the fate of iterative processes, from financial growth to
  disease spread.
\item
  It draws a sharp boundary between decay, balance, and explosion in
  linear systems.
\end{itemize}

\subsubsection{Try It Yourself}\label{try-it-yourself-67}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Compute the spectral radius of
  \(\begin{bmatrix} 0.6 & 0.3 \\ 0.1 & 0.8 \end{bmatrix}\). Does the
  system converge?
\item
  Show that for any matrix norm \(\|\cdot\|\),

  \[
  \rho(A) \leq \|A\|.
  \]

  (Hint: use Gelfand's formula.)
\item
  For \(\begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix}\), explain why it
  diverges even though \(\rho=1\).
\item
  Challenge: Prove Gelfand's formula:

  \[
  \rho(A) = \lim_{k\to\infty} \|A^k\|^{1/k}.
  \]
\end{enumerate}

The spectral radius is the compass of linear dynamics: it points to
stability, oscillation, or divergence, guiding us across disciplines
wherever repeated transformations shape the future.

\subsection{69. Markov Chains and Steady
States}\label{markov-chains-and-steady-states}

Markov chains are one of the most direct and beautiful applications of
eigenvalues in probability and statistics. They describe systems that
evolve step by step, where the next state depends only on the current
one, not on the past. The mathematics of steady states-the long-term
behavior of such chains-rests firmly on eigenvalues and eigenvectors of
the transition matrix.

\subsubsection{Transition Matrices}\label{transition-matrices}

A Markov chain is defined by a transition matrix
\(P \in \mathbb{R}^{n \times n}\) with the following properties:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  All entries are nonnegative: \(p_{ij} \geq 0\).
\item
  Each row sums to 1: \(\sum_j p_{ij} = 1\).
\end{enumerate}

If the chain is in state \(i\) at time \(k\), then \(p_{ij}\) is the
probability of moving to state \(j\) at time \(k+1\).

\subsubsection{Evolution of States}\label{evolution-of-states}

If the probability distribution at time \(k\) is a row vector
\(\pi^{(k)}\), then

\[
\pi^{(k+1)} = \pi^{(k)} P.
\]

After \(k\) steps:

\[
\pi^{(k)} = \pi^{(0)} P^k.
\]

So understanding the long-term behavior requires analyzing \(P^k\).

\subsubsection{Eigenvalue Structure of Transition
Matrices}\label{eigenvalue-structure-of-transition-matrices}

\begin{itemize}
\item
  Every transition matrix \(P\) has eigenvalue \(\lambda = 1\).
\item
  All other eigenvalues satisfy \(|\lambda| \leq 1\).
\item
  If the chain is irreducible (all states communicate) and aperiodic (no
  cyclic locking), then:

  \begin{itemize}
  \tightlist
  \item
    \(\lambda=1\) is a simple eigenvalue (AM=GM=1).
  \item
    All other eigenvalues have magnitude strictly less than 1.
  \end{itemize}
\end{itemize}

This ensures convergence to a unique steady state.

\subsubsection{Steady States as
Eigenvectors}\label{steady-states-as-eigenvectors}

A steady state distribution \(\pi\) satisfies:

\[
\pi = \pi P.
\]

This is equivalent to:

\[
\pi^T \text{ is a right eigenvector of } P^T \text{ with eigenvalue } 1.
\]

\begin{itemize}
\tightlist
\item
  The steady state vector lies in the eigenspace of eigenvalue 1.
\item
  Since probabilities must sum to 1, normalization gives a unique steady
  state.
\end{itemize}

\subsubsection{Example: A 2-State Markov
Chain}\label{example-a-2-state-markov-chain}

\[
P = \begin{bmatrix} 0.7 & 0.3 \\ 0.4 & 0.6 \end{bmatrix}.
\]

\begin{itemize}
\item
  Eigenvalues: solve \(\det(P-\lambda I) = 0\).

  \[
  \lambda_1 = 1, \quad \lambda_2 = 0.3.
  \]
\item
  The steady state is found from \(\pi = \pi P\):

  \[
  \pi = \bigg(\frac{4}{7}, \frac{3}{7}\bigg).
  \]
\item
  As \(k \to \infty\), any initial distribution \(\pi^{(0)}\) converges
  to this steady state.
\end{itemize}

\subsubsection{Example: Random Walk on a
Graph}\label{example-random-walk-on-a-graph}

Take a simple graph: 3 nodes in a line, where each node passes to
neighbors equally.

Transition matrix:

\[
P = \begin{bmatrix} 
0 & 1 & 0 \\ 
0.5 & 0 & 0.5 \\ 
0 & 1 & 0 
\end{bmatrix}.
\]

\begin{itemize}
\tightlist
\item
  Eigenvalues: \(\{1, 0, -1\}\).
\item
  The steady state corresponds to eigenvalue 1.
\item
  After many steps, the distribution converges to \((0.25, 0.5, 0.25)\).
\end{itemize}

\subsubsection{Geometric Meaning}\label{geometric-meaning-16}

\begin{itemize}
\tightlist
\item
  Eigenvalue 1: the fixed ``direction'' of probabilities that does not
  change under transitions.
\item
  Eigenvalues \textless{} 1 in magnitude: transient modes that vanish as
  \(k \to \infty\).
\item
  The dominant eigenvector (steady state) is like the ``center of
  gravity'' of the system.
\end{itemize}

So powers of \(P\) filter out all but the eigenvector of eigenvalue 1.

\subsubsection{Everyday Analogies}\label{everyday-analogies-65}

\begin{itemize}
\tightlist
\item
  Board games: After many moves, the distribution of where players end
  up (like Monopoly squares) stabilizes, regardless of start.
\item
  Web surfing: The Google PageRank algorithm is based on eigenvalue 1 of
  a huge transition matrix.
\item
  Shuffling cards: With enough shuffles, the probability distribution
  becomes uniform-a steady state.
\item
  Weather models: Even if today's weather matters, in the long run the
  system converges to a stable climate distribution.
\end{itemize}

\subsubsection{Applications}\label{applications-31}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Google PageRank: Steady state eigenvectors rank webpages.
\item
  Economics: Input-output models evolve like Markov chains.
\item
  Epidemiology: Spread of diseases can be modeled as Markov processes.
\item
  Machine Learning: Hidden Markov models (HMMs) underpin speech
  recognition and bioinformatics.
\item
  Queuing Theory: Customer arrivals and service evolve according to
  Markov dynamics.
\end{enumerate}

\subsubsection{Why It Matters}\label{why-it-matters-65}

\begin{itemize}
\tightlist
\item
  The concept of steady states shows how randomness can lead to
  predictability.
\item
  Eigenvalues explain why convergence happens, and at what rate.
\item
  The link between linear algebra and probability provides one of the
  clearest real-world uses of eigenvectors.
\end{itemize}

\subsubsection{Try It Yourself}\label{try-it-yourself-68}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  For

  \[
  P = \begin{bmatrix} 0.9 & 0.1 \\ 0.5 & 0.5 \end{bmatrix},
  \]

  compute its eigenvalues and steady state.
\item
  Show that for any transition matrix, the largest eigenvalue is always
  1.
\item
  Prove that if a chain is irreducible and aperiodic, the steady state
  is unique.
\item
  Challenge: Construct a 3-state transition matrix with a cycle
  (periodic) and show why it doesn't converge to a steady distribution
  until perturbed.
\end{enumerate}

Markov chains and steady states are the meeting point of probability and
linear algebra: randomness, when multiplied many times, is tamed by the
calm persistence of eigenvalue 1.

\subsection{70. Linear Differential
Systems}\label{linear-differential-systems}

Many natural and engineered processes evolve continuously over time.
When these processes can be expressed as linear relationships, they lead
to systems of linear differential equations. The analysis of such
systems relies almost entirely on eigenvalues and eigenvectors, which
determine the behavior of solutions: whether they oscillate, decay,
grow, or stabilize.

\subsubsection{The General Setup}\label{the-general-setup}

Consider a system of first-order linear differential equations:

\[
\frac{d}{dt}x(t) = A x(t),
\]

where:

\begin{itemize}
\tightlist
\item
  \(x(t) \in \mathbb{R}^n\) is the state vector at time \(t\).
\item
  \(A \in \mathbb{R}^{n \times n}\) is a constant coefficient matrix.
\end{itemize}

The task is to solve for \(x(t)\), given an initial state \(x(0)\).

\subsubsection{The Matrix Exponential}\label{the-matrix-exponential}

The formal solution is:

\[
x(t) = e^{At} x(0),
\]

where \(e^{At}\) is the matrix exponential defined as:

\[
e^{At} = I + At + \frac{(At)^2}{2!} + \frac{(At)^3}{3!} + \cdots.
\]

But how do we compute \(e^{At}\) in practice? The answer comes from
diagonalization and Jordan form.

\subsubsection{Case 1: Diagonalizable
Matrices}\label{case-1-diagonalizable-matrices}

If \(A\) is diagonalizable:

\[
A = P D P^{-1}, \quad D = \text{diag}(\lambda_1, \ldots, \lambda_n).
\]

Then:

\[
e^{At} = P e^{Dt} P^{-1}, \quad e^{Dt} = \text{diag}(e^{\lambda_1 t}, \ldots, e^{\lambda_n t}).
\]

Thus the solution is:

\[
x(t) = P \begin{bmatrix} e^{\lambda_1 t} & & \\ & \ddots & \\ & & e^{\lambda_n t} \end{bmatrix} P^{-1} x(0).
\]

Each eigenvalue \(\lambda_i\) dictates the time behavior along its
eigenvector direction.

\subsubsection{Case 2: Non-Diagonalizable
Matrices}\label{case-2-non-diagonalizable-matrices}

If \(A\) is defective, use its Jordan form \(J = P^{-1}AP\):

\[
e^{At} = P e^{Jt} P^{-1}.
\]

For a Jordan block of size 2:

\[
J = \begin{bmatrix} \lambda & 1 \\ 0 & \lambda \end{bmatrix}, \quad
e^{Jt} = e^{\lambda t} \begin{bmatrix} 1 & t \\ 0 & 1 \end{bmatrix}.
\]

Polynomial terms in \(t\) appear, multiplying the exponential part. This
explains why repeated eigenvalues with insufficient eigenvectors yield
solutions with extra polynomial factors.

\subsubsection{Real vs.~Complex
Eigenvalues}\label{real-vs.-complex-eigenvalues}

\begin{itemize}
\item
  Real eigenvalues: solutions grow or decay exponentially along
  eigenvector directions.

  \begin{itemize}
  \tightlist
  \item
    If \(\lambda < 0\): exponential decay → stability.
  \item
    If \(\lambda > 0\): exponential growth → instability.
  \end{itemize}
\item
  Complex eigenvalues: \(\lambda = a \pm bi\). Solutions involve
  oscillations:

  \[
  e^{(a+bi)t} = e^{at}(\cos(bt) + i \sin(bt)).
  \]

  \begin{itemize}
  \tightlist
  \item
    If \(a < 0\): decaying oscillations.
  \item
    If \(a > 0\): growing oscillations.
  \item
    If \(a = 0\): pure oscillations, neutrally stable.
  \end{itemize}
\end{itemize}

\subsubsection{Example 1: Real
Eigenvalues}\label{example-1-real-eigenvalues}

\[
A = \begin{bmatrix} -2 & 0 \\ 0 & -3 \end{bmatrix}.
\]

Eigenvalues: \(-2, -3\). Solution:

\[
x(t) = \begin{bmatrix} c_1 e^{-2t} \\ c_2 e^{-3t} \end{bmatrix}.
\]

Both terms decay → stable equilibrium at the origin.

\subsubsection{Example 2: Complex
Eigenvalues}\label{example-2-complex-eigenvalues}

\[
A = \begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix}.
\]

Eigenvalues: \(\pm i\). Solution:

\[
x(t) = c_1 \begin{bmatrix} \cos t \\ \sin t \end{bmatrix} + c_2 \begin{bmatrix} -\sin t \\ \cos t \end{bmatrix}.
\]

Pure oscillation → circular motion around the origin.

\subsubsection{Example 3: Mixed
Stability}\label{example-3-mixed-stability}

\[
A = \begin{bmatrix} 1 & 0 \\ 0 & -2 \end{bmatrix}.
\]

Eigenvalues: \(1, -2\). Solution:

\[
x(t) = \begin{bmatrix} c_1 e^t \\ c_2 e^{-2t} \end{bmatrix}.
\]

One direction grows, one decays → unstable overall, since divergence in
one direction dominates.

\subsubsection{Geometric Meaning}\label{geometric-meaning-17}

\begin{itemize}
\tightlist
\item
  The eigenvectors form the ``axes of flow'' of the system.
\item
  The eigenvalues determine whether the flow along those axes spirals,
  grows, or shrinks.
\item
  The phase portrait of the system-trajectories in the plane-is shaped
  by this interplay.
\end{itemize}

For example:

\begin{itemize}
\tightlist
\item
  Negative eigenvalues → trajectories funnel into the origin.
\item
  Positive eigenvalues → trajectories repel outward.
\item
  Complex eigenvalues → spirals or circles.
\end{itemize}

\subsubsection{Everyday Analogies}\label{everyday-analogies-66}

\begin{itemize}
\tightlist
\item
  Population dynamics: Growth rates correspond to eigenvalues. Negative
  rates → extinction; positive rates → explosion.
\item
  Engineering vibrations: Eigenvalues determine resonance frequencies
  and damping.
\item
  Finance: Interest rates with oscillatory components (complex
  eigenvalues) describe cyclical economies.
\item
  Climate models: Stability of equilibria (e.g., greenhouse gas balance)
  comes from sign of eigenvalues.
\end{itemize}

\subsubsection{Applications}\label{applications-32}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Control theory: Stability analysis of systems requires eigenvalue
  placement in the left-half plane.
\item
  Physics: Vibrations, quantum oscillations, and decay processes all
  follow eigenvalue rules.
\item
  Biology: Population models evolve according to linear differential
  equations.
\item
  Economics: Linear models of markets converge or diverge depending on
  eigenvalues.
\item
  Neuroscience: Neural firing dynamics can be modeled as linear ODE
  systems.
\end{enumerate}

\subsubsection{Why It Matters}\label{why-it-matters-66}

\begin{itemize}
\tightlist
\item
  Linear differential systems bridge linear algebra with real-world
  dynamics.
\item
  Eigenvalues determine not just numbers, but behaviors over time:
  growth, decay, oscillation, or equilibrium.
\item
  They provide the foundation for analyzing nonlinear systems, which are
  often studied by linearizing around equilibrium points.
\end{itemize}

\subsubsection{Try It Yourself}\label{try-it-yourself-69}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Solve
  \(\frac{dx}{dt} = \begin{bmatrix} -1 & 2 \\ -2 & -1 \end{bmatrix}x\).
  Interpret the solution.
\item
  For \(A = \begin{bmatrix} 0 & -2 \\ 2 & 0 \end{bmatrix}\), compute
  eigenvalues and describe the motion.
\item
  Verify that \(e^{At} = P e^{Dt} P^{-1}\) works when \(A\) is
  diagonalizable.
\item
  Challenge: Show that if all eigenvalues of \(A\) have negative real
  parts, then \(\lim_{t \to \infty} x(t) = 0\) for any initial
  condition.
\end{enumerate}

Linear differential systems show how eigenvalues control the flow of
time itself in models. They explain why some processes die out, others
oscillate, and others grow without bound-providing the mathematical
skeleton behind countless real-world phenomena.

\subsubsection{Closing}\label{closing-6}

\begin{verbatim}
Spectra guide the flow,
growth and decay intertwining,
future sings through roots.
\end{verbatim}

\section{Chapter 8. Orthogonality, least squars, and
QR}\label{chapter-8.-orthogonality-least-squars-and-qr}

\subsubsection{Opening}\label{opening-6}

\begin{verbatim}
Perpendiculars,
meeting without crossing paths,
balance in silence.
\end{verbatim}

\subsection{71. Inner Products Beyond Dot
Product}\label{inner-products-beyond-dot-product}

The dot product is the first inner product most students encounter. In
\(\mathbb{R}^n\), it is defined as

\[
\langle x, y \rangle = x \cdot y = \sum_{i=1}^n x_i y_i,
\]

and it provides a way to measure length, angle, and orthogonality. But
the dot product is just one special case of a much broader concept.
Inner products generalize the dot product, extending its geometric
intuition to more abstract vector spaces.

\subsubsection{Definition of an Inner
Product}\label{definition-of-an-inner-product}

An inner product on a real vector space \(V\) is a function

\[
\langle \cdot, \cdot \rangle : V \times V \to \mathbb{R}
\]

that satisfies the following axioms for all \(x,y,z \in V\) and scalar
\(\alpha \in \mathbb{R}\):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Positivity: \(\langle x, x \rangle \geq 0\), and
  \(\langle x, x \rangle = 0 \iff x=0\).
\item
  Symmetry: \(\langle x, y \rangle = \langle y, x \rangle\).
\item
  Linearity in the first argument:
  \(\langle \alpha x + y, z \rangle = \alpha \langle x, z \rangle + \langle y, z \rangle\).
\end{enumerate}

In complex vector spaces, the symmetry condition changes to conjugate
symmetry: \(\langle x, y \rangle = \overline{\langle y, x \rangle}\).

\subsubsection{Norms and Angles from Inner
Products}\label{norms-and-angles-from-inner-products}

Once an inner product is defined, we immediately get:

\begin{itemize}
\item
  Norm (length): \(\|x\| = \sqrt{\langle x, x \rangle}\).
\item
  Distance: \(d(x,y) = \|x-y\|\).
\item
  Angle between vectors:
  \(\cos \theta = \frac{\langle x, y \rangle}{\|x\|\|y\|}\).
\end{itemize}

Thus, inner products generalize the familiar geometry of
\(\mathbb{R}^n\) to broader contexts.

\subsubsection{Examples Beyond the Dot
Product}\label{examples-beyond-the-dot-product}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Weighted Inner Product (in \(\mathbb{R}^n\)):

  \[
  \langle x, y \rangle_W = x^T W y,
  \]

  where \(W\) is a symmetric positive definite matrix.

  \begin{itemize}
  \tightlist
  \item
    Here, lengths and angles depend on the weights encoded in \(W\).
  \item
    Useful when some dimensions are more important than others (e.g.,
    weighted least squares).
  \end{itemize}
\item
  Function Spaces (continuous inner product): On \(V = C[a,b]\), the
  space of continuous functions on \([a,b]\):

  \[
  \langle f, g \rangle = \int_a^b f(t) g(t) \, dt.
  \]

  \begin{itemize}
  \tightlist
  \item
    Length: \(\|f\| = \sqrt{\int_a^b f(t)^2 dt}\).
  \item
    Orthogonality: \(f\) and \(g\) are orthogonal if their integral
    product is zero.
  \item
    This inner product underpins Fourier series.
  \end{itemize}
\item
  Complex Inner Product (in \(\mathbb{C}^n\)):

  \[
  \langle x, y \rangle = \sum_{i=1}^n x_i \overline{y_i}.
  \]

  \begin{itemize}
  \tightlist
  \item
    Conjugation ensures positivity.
  \item
    Critical for quantum mechanics, where states are vectors in complex
    Hilbert spaces.
  \end{itemize}
\item
  Polynomial Spaces: For polynomials on \([-1,1]\):

  \[
  \langle p, q \rangle = \int_{-1}^1 p(x) q(x) \, dx.
  \]

  \begin{itemize}
  \tightlist
  \item
    Leads to orthogonal polynomials (Legendre, Chebyshev), fundamental
    in approximation theory.
  \end{itemize}
\end{enumerate}

\subsubsection{Geometric
Interpretation}\label{geometric-interpretation-15}

\begin{itemize}
\tightlist
\item
  Inner products reshape geometry. Instead of measuring lengths and
  angles with the Euclidean metric, we measure them with the metric
  induced by the chosen inner product.
\item
  Different inner products create different geometries on the same
  vector space.
\end{itemize}

Example: A weighted inner product distorts circles into ellipses,
changing which vectors count as ``orthogonal.''

\subsubsection{Everyday Analogies}\label{everyday-analogies-67}

\begin{itemize}
\tightlist
\item
  Weighted voting: In an election, some votes count more; in a weighted
  inner product, some dimensions of a vector count more.
\item
  Sound and music: The inner product of two signals measures how much
  one resonates with the other.
\item
  Search engines: Inner products between word-frequency vectors measure
  document similarity. Different weighting schemes (like TF-IDF)
  correspond to different inner products.
\end{itemize}

\subsubsection{Applications}\label{applications-33}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Signal Processing: Correlation between signals is an inner product.
  Orthogonality means two signals carry independent information.
\item
  Fourier Analysis: Fourier coefficients come from inner products with
  sine and cosine functions.
\item
  Machine Learning: Kernel methods generalize inner products to
  infinite-dimensional spaces.
\item
  Quantum Mechanics: Probabilities are squared magnitudes of complex
  inner products.
\item
  Optimization: Weighted least squares problems use weighted inner
  products.
\end{enumerate}

\subsubsection{Why It Matters}\label{why-it-matters-67}

\begin{itemize}
\tightlist
\item
  Inner products generalize geometry to new contexts: weighted spaces,
  functions, polynomials, quantum states.
\item
  They provide the foundation for defining orthogonality, projections,
  and orthonormal bases in spaces far beyond \(\mathbb{R}^n\).
\item
  They unify ideas across pure mathematics, physics, engineering, and
  computer science.
\end{itemize}

\subsubsection{Try It Yourself}\label{try-it-yourself-70}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Show that the weighted inner product
  \(\langle x, y \rangle_W = x^T W y\) satisfies the inner product
  axioms if \(W\) is positive definite.
\item
  Compute \(\langle f, g \rangle = \int_0^\pi \sin(t)\cos(t)\, dt\). Are
  \(f=\sin\) and \(g=\cos\) orthogonal?
\item
  In \(\mathbb{C}^2\), verify that \(\langle (1,i), (i,1) \rangle = 0\).
  What does this mean geometrically?
\item
  Challenge: Prove that every inner product induces a norm, and that
  different inner products can lead to different geometries on the same
  space.
\end{enumerate}

The dot product is just the beginning. Inner products provide the
language to extend geometry into weighted spaces, continuous functions,
and infinite dimensions-transforming how we measure similarity,
distance, and structure across mathematics and science.

\subsection{72. Orthogonality and Orthonormal
Bases}\label{orthogonality-and-orthonormal-bases}

Orthogonality is one of the most powerful ideas in linear algebra. It
generalizes the familiar concept of perpendicularity in Euclidean space
to abstract vector spaces equipped with an inner product. When
orthogonality is combined with normalization (making vectors have unit
length), we obtain orthonormal bases, which simplify computations,
clarify geometry, and underpin many algorithms.

\subsubsection{Orthogonality}\label{orthogonality-1}

Two vectors \(x, y \in V\) are orthogonal if

\[
\langle x, y \rangle = 0.
\]

\begin{itemize}
\tightlist
\item
  In \(\mathbb{R}^2\) or \(\mathbb{R}^3\), this means the vectors are
  perpendicular.
\item
  In function spaces, it means the integral of their product is zero.
\item
  In signal processing, it means the signals are independent and
  non-overlapping.
\end{itemize}

Orthogonality captures the idea of ``no overlap'' or ``independence''
under the geometry of the inner product.

\subsubsection{Properties of Orthogonal
Vectors}\label{properties-of-orthogonal-vectors}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  If \(x \perp y\), then \(\|x+y\|^2 = \|x\|^2 + \|y\|^2\) (Pythagoras'
  theorem generalized).
\item
  Orthogonality is symmetric: if \(x \perp y\), then \(y \perp x\).
\item
  Any set of mutually orthogonal nonzero vectors is automatically
  linearly independent.
\end{enumerate}

This last property is critical: orthogonality guarantees independence.

\subsubsection{Orthonormal Sets}\label{orthonormal-sets}

An orthonormal set is a collection of vectors \(\{u_1, \dots, u_k\}\)
such that

\[
\langle u_i, u_j \rangle = \begin{cases} 
1 & \text{if } i=j, \\ 
0 & \text{if } i \neq j.  
\end{cases}
\]

\begin{itemize}
\tightlist
\item
  Each vector has unit length.
\item
  Distinct vectors are mutually orthogonal.
\end{itemize}

This structure makes computations with coordinates as simple as
possible.

\subsubsection{Orthonormal Bases}\label{orthonormal-bases}

A basis \(\{u_1, \dots, u_n\}\) for a vector space is orthonormal if it
is orthonormal as a set.

\begin{itemize}
\item
  Any vector \(x \in V\) can be written as

  \[
  x = \sum_{i=1}^n \langle x, u_i \rangle u_i.
  \]
\item
  The coefficients are just inner products, no need to solve systems of
  equations.
\end{itemize}

This is why orthonormal bases are the most convenient: they make
representation and projection effortless.

\subsubsection{Examples}\label{examples-3}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Standard Basis in \(\mathbb{R}^n\): \(\{e_1, e_2, \dots, e_n\}\),
  where \(e_i\) has 1 in the \(i\)-th coordinate and 0 elsewhere.

  \begin{itemize}
  \tightlist
  \item
    Orthonormal under the standard dot product.
  \end{itemize}
\item
  Fourier Basis: Functions \(\{\sin(nx), \cos(nx)\}\) on \([0,2\pi]\)
  are orthogonal under the inner product
  \(\langle f,g\rangle = \int_0^{2\pi} f(x)g(x)dx\).

  \begin{itemize}
  \tightlist
  \item
    This basis decomposes signals into pure frequencies.
  \end{itemize}
\item
  Polynomial Basis: Legendre polynomials \(P_n(x)\) are orthogonal on
  \([-1,1]\) with respect to
  \(\langle f,g\rangle = \int_{-1}^1 f(x)g(x)\,dx\).
\end{enumerate}

\subsubsection{Geometric Meaning}\label{geometric-meaning-18}

Orthogonality splits space into independent ``directions.''

\begin{itemize}
\tightlist
\item
  Orthonormal bases are like perfectly aligned coordinate axes.
\item
  Any vector decomposes uniquely as a sum of independent contributions
  along these axes.
\item
  Distances and angles are preserved, making the geometry transparent.
\end{itemize}

\subsubsection{Everyday Analogies}\label{everyday-analogies-68}

\begin{itemize}
\tightlist
\item
  Sound frequencies: Each note in a chord can be separated if the notes
  are orthogonal (independent frequencies).
\item
  Data analysis: Orthogonal axes correspond to independent features.
\item
  Sports: Offense and defense in a game are orthogonal roles-different
  but both necessary.
\item
  Finance: Orthogonal portfolios have no correlation-risk in one does
  not affect the other.
\end{itemize}

\subsubsection{Applications}\label{applications-34}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Signal Processing: Decompose signals into orthogonal frequency
  components.
\item
  Machine Learning: Principal components form an orthonormal basis
  capturing variance directions.
\item
  Numerical Methods: Orthonormal bases improve numerical stability.
\item
  Quantum Mechanics: States are orthogonal if they represent mutually
  exclusive outcomes.
\item
  Computer Graphics: Rotations are represented by orthogonal matrices
  with orthonormal columns.
\end{enumerate}

\subsubsection{Why It Matters}\label{why-it-matters-68}

\begin{itemize}
\tightlist
\item
  Orthogonality provides independence; orthonormality provides
  normalization.
\item
  Together they make computations, decompositions, and projections clean
  and efficient.
\item
  They underlie Fourier analysis, principal component analysis, and
  countless modern algorithms.
\end{itemize}

\subsubsection{Try It Yourself}\label{try-it-yourself-71}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Show that \(\{(1,0,0), (0,1,0), (0,0,1)\}\) is an orthonormal basis of
  \(\mathbb{R}^3\).
\item
  Check whether \(\{(1,1,0), (1,-1,0), (0,0,1)\}\) is orthonormal under
  the dot product. If not, normalize it.
\item
  Compute the coefficients of \(x=(3,4)\) in the basis
  \(\{(1,0), (0,1)\}\) and in the rotated orthonormal basis
  \(\{(1/\sqrt{2}, 1/\sqrt{2}), (-1/\sqrt{2}, 1/\sqrt{2})\}\).
\item
  Challenge: Prove that in any finite-dimensional inner product space,
  an orthonormal basis always exists (hint: Gram--Schmidt).
\end{enumerate}

Orthogonality and orthonormal bases are the backbone of linear algebra:
they transform messy problems into elegant decompositions, giving us the
cleanest possible language for describing vectors, signals, and data.

\subsection{73. Gram--Schmidt Process}\label{gramschmidt-process}

The Gram--Schmidt process is a systematic method for turning any
linearly independent set of vectors into an orthonormal basis. This
process is one of the most elegant bridges between algebra and geometry:
it takes arbitrary vectors and makes them mutually perpendicular, while
preserving the span.

\subsubsection{The Problem It Solves}\label{the-problem-it-solves}

Given a set of linearly independent vectors \(\{v_1, v_2, \dots, v_n\}\)
in an inner product space:

\begin{itemize}
\tightlist
\item
  They span some subspace \(W\).
\item
  But they are not necessarily orthogonal or normalized.
\end{itemize}

Goal: Construct an orthonormal basis \(\{u_1, u_2, \dots, u_n\}\) for
\(W\).

\subsubsection{The Gram--Schmidt
Algorithm}\label{the-gramschmidt-algorithm}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Start with the first vector:

  \[
  u_1 = \frac{v_1}{\|v_1\|}.
  \]
\item
  For the second vector, subtract the projection onto \(u_1\):

  \[
  w_2 = v_2 - \langle v_2, u_1 \rangle u_1, \quad u_2 = \frac{w_2}{\|w_2\|}.
  \]
\item
  For the third vector, subtract projections onto both \(u_1\) and
  \(u_2\):

  \[
  w_3 = v_3 - \langle v_3, u_1 \rangle u_1 - \langle v_3, u_2 \rangle u_2, \quad u_3 = \frac{w_3}{\|w_3\|}.
  \]
\item
  Continue inductively:

  \[
  w_k = v_k - \sum_{j=1}^{k-1} \langle v_k, u_j \rangle u_j, \quad u_k = \frac{w_k}{\|w_k\|}.
  \]
\end{enumerate}

At each step, \(w_k\) is made orthogonal to all previous \(u_j\), and
then normalized to form \(u_k\).

\subsubsection{\texorpdfstring{Example in
\(\mathbb{R}^2\)}{Example in \textbackslash mathbb\{R\}\^{}2}}\label{example-in-mathbbr2-3}

Start with \(v_1 = (1,1)\), \(v_2 = (1,0)\).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Normalize first vector:

  \[
  u_1 = \frac{(1,1)}{\sqrt{2}} = \left(\tfrac{1}{\sqrt{2}}, \tfrac{1}{\sqrt{2}}\right).
  \]
\item
  Subtract projection of \(v_2\) on \(u_1\):

  \[
  w_2 = (1,0) - \left(\tfrac{1}{\sqrt{2}}\cdot1 + \tfrac{1}{\sqrt{2}}\cdot0\right)\left(\tfrac{1}{\sqrt{2}}, \tfrac{1}{\sqrt{2}}\right).
  \]

  \[
  = (1,0) - \tfrac{1}{\sqrt{2}}\left(\tfrac{1}{\sqrt{2}}, \tfrac{1}{\sqrt{2}}\right).
  \]

  \[
  = (1,0) - (0.5,0.5) = (0.5,-0.5).
  \]
\item
  Normalize:

  \[
  u_2 = \frac{(0.5,-0.5)}{\sqrt{0.5^2+(-0.5)^2}} = \frac{(0.5,-0.5)}{\sqrt{0.5}} = \left(\tfrac{1}{\sqrt{2}}, -\tfrac{1}{\sqrt{2}}\right).
  \]
\end{enumerate}

Final orthonormal basis:

\[
u_1 = \left(\tfrac{1}{\sqrt{2}}, \tfrac{1}{\sqrt{2}}\right), \quad u_2 = \left(\tfrac{1}{\sqrt{2}}, -\tfrac{1}{\sqrt{2}}\right).
\]

\subsubsection{Geometric Intuition}\label{geometric-intuition-5}

\begin{itemize}
\tightlist
\item
  Each step removes ``overlap'' with previously chosen directions.
\item
  Think of it as building new perpendicular coordinate axes inside the
  span of the original vectors.
\item
  The result is like rotating and scaling the original set into a
  perfectly orthogonal system.
\end{itemize}

\subsubsection{Numerical Stability}\label{numerical-stability}

\begin{itemize}
\tightlist
\item
  Classical Gram--Schmidt can suffer from round-off errors in computer
  calculations.
\item
  A numerically stable alternative is Modified Gram--Schmidt (MGS),
  which reorders the projection steps to reduce loss of orthogonality.
\item
  In practice, QR factorization algorithms often implement MGS or
  Householder reflections.
\end{itemize}

\subsubsection{Everyday Analogies}\label{everyday-analogies-69}

\begin{itemize}
\tightlist
\item
  Teamwork: If two teammates overlap in their roles, Gram--Schmidt
  reallocates effort until each works independently.
\item
  Noise filtering: Orthogonal components separate useful signals from
  redundancy.
\item
  Cooking recipe: Adjusting ingredients so that each adds a unique
  flavor, without duplicating what's already present.
\end{itemize}

\subsubsection{Applications}\label{applications-35}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  QR Factorization: Gram--Schmidt provides the foundation: \(A = QR\),
  where \(Q\) is orthogonal and \(R\) is upper triangular.
\item
  Data Compression: Orthonormal bases from Gram--Schmidt lead to
  efficient representations.
\item
  Signal Processing: Ensures independent frequency or wave components.
\item
  Machine Learning: Used in orthogonalization of features and
  dimensionality reduction.
\item
  Physics: Orthogonal states in quantum mechanics can be constructed
  from arbitrary states using Gram--Schmidt.
\end{enumerate}

\subsubsection{Why It Matters}\label{why-it-matters-69}

\begin{itemize}
\tightlist
\item
  Gram--Schmidt guarantees that any independent set can be reshaped into
  an orthonormal basis.
\item
  It underlies computational methods like QR decomposition, least
  squares, and numerical PDE solvers.
\item
  It makes projections, coordinates, and orthogonality explicit and
  manageable.
\end{itemize}

\subsubsection{Try It Yourself}\label{try-it-yourself-72}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Apply Gram--Schmidt to \((1,0,1)\), \((1,1,0)\), \((0,1,1)\) in
  \(\mathbb{R}^3\). Verify orthonormality.
\item
  Show that the span of the orthonormal basis equals the span of the
  original vectors.
\item
  Use Gram--Schmidt to find an orthonormal basis for polynomials
  \(\{1,x,x^2\}\) on \([-1,1]\) with inner product
  \(\langle f,g\rangle = \int_{-1}^1 f(x)g(x)\,dx\).
\item
  Challenge: Prove that Gram--Schmidt always works for linearly
  independent sets, but fails if the set is dependent.
\end{enumerate}

The Gram--Schmidt process is the algorithmic heart of orthogonality: it
takes the messy and redundant and reshapes it into clean, perpendicular
building blocks for the spaces we study.

\subsection{74. Projections onto
Subspaces}\label{projections-onto-subspaces}

Projections are a natural extension of orthogonality: they describe how
to ``drop'' a vector onto a subspace in the most natural way, minimizing
the distance. Understanding projections is crucial for solving least
squares problems, decomposing vectors, and interpreting data in terms of
simpler, lower-dimensional structures.

\subsubsection{Projection onto a Vector}\label{projection-onto-a-vector}

Start with the simplest case: projecting a vector \(x\) onto a nonzero
vector \(u\).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The projection is the component of \(x\) that lies in the direction of
  \(u\).
\item
  Formula:

  \[
  \text{proj}_u(x) = \frac{\langle x, u \rangle}{\langle u, u \rangle} u.
  \]

  If \(u\) is normalized (\(\|u\|=1\)), this simplifies to

  \[
  \text{proj}_u(x) = \langle x, u \rangle u.
  \]
\end{enumerate}

Geometrically, this is the foot of the perpendicular from \(x\) onto the
line spanned by \(u\).

\subsubsection{Projection onto an Orthonormal
Basis}\label{projection-onto-an-orthonormal-basis}

Suppose we have an orthonormal basis \(\{u_1, u_2, \dots, u_k\}\) for a
subspace \(W\). Then the projection of \(x\) onto \(W\) is:

\[
\text{proj}_W(x) = \sum_{i=1}^k \langle x, u_i \rangle u_i.
\]

This formula is powerful:

\begin{itemize}
\tightlist
\item
  Each coefficient \(\langle x, u_i \rangle\) captures how much of \(x\)
  aligns with \(u_i\).
\item
  The sum reconstructs the best approximation of \(x\) inside \(W\).
\end{itemize}

\subsubsection{Projection Matrix}\label{projection-matrix}

When working in coordinates, projections can be represented by matrices.

\begin{itemize}
\item
  If \(U\) is the \(n \times k\) matrix with orthonormal columns
  \(\{u_1, \dots, u_k\}\), then

  \[
  P = UU^T
  \]

  is the projection matrix onto \(W\).
\end{itemize}

Properties of \(P\):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Idempotence: \(P^2 = P\).
\item
  Symmetry: \(P^T = P\).
\item
  Best approximation: For any \(x\), \(\|x - Px\|\) is minimized.
\end{enumerate}

\subsubsection{Projection and Orthogonal
Complements}\label{projection-and-orthogonal-complements}

If \(W\) is a subspace of \(V\), then every vector \(x \in V\) can be
decomposed uniquely as

\[
x = \text{proj}_W(x) + \text{proj}_{W^\perp}(x),
\]

where \(W^\perp\) is the orthogonal complement of \(W\).

This decomposition is the orthogonal decomposition theorem. It says:
space splits cleanly into ``in'' and ``out of'' components relative to a
subspace.

\subsubsection{\texorpdfstring{Example in
\(\mathbb{R}^2\)}{Example in \textbackslash mathbb\{R\}\^{}2}}\label{example-in-mathbbr2-4}

Let \(u = (2,1)\), and project \(x = (3,4)\) onto span\(\{u\}\).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Compute inner product:
  \(\langle x,u\rangle = 3\cdot 2 + 4\cdot 1 = 10\).
\item
  Compute norm squared: \(\langle u,u\rangle = 2^2 + 1^2 = 5\).
\item
  Projection:

  \[
  \text{proj}_u(x) = \frac{10}{5}(2,1) = 2(2,1) = (4,2).
  \]
\item
  Orthogonal error:

  \[
  x - \text{proj}_u(x) = (3,4) - (4,2) = (-1,2).
  \]
\end{enumerate}

Notice: \((4,2)\) lies on the line through \(u\), and the error vector
\((-1,2)\) is orthogonal to \(u\).

\subsubsection{Everyday Analogies}\label{everyday-analogies-70}

\begin{itemize}
\tightlist
\item
  Casting shadows: A projection is like shining a light; the shadow is
  the projection onto the floor or wall.
\item
  Work in physics: The projection of a force onto a direction gives the
  effective force doing work in that direction.
\item
  Budget allocation: Projecting expenses onto categories shows how much
  belongs to each category.
\item
  Simplified models: Projecting data onto a lower-dimensional space
  gives the closest ``simplified'' version.
\end{itemize}

\subsubsection{Applications}\label{applications-36}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Least Squares Regression: The regression line is the projection of
  data onto the subspace spanned by predictor variables.
\item
  Dimensionality Reduction: Principal Component Analysis (PCA) projects
  data onto the subspace of top eigenvectors.
\item
  Computer Graphics: 3D objects are projected onto 2D screens.
\item
  Numerical Methods: Projections solve equations approximately when
  exact solutions don't exist.
\item
  Physics: Work and energy are computed via projections of forces and
  velocities.
\end{enumerate}

\subsubsection{Why It Matters}\label{why-it-matters-70}

\begin{itemize}
\tightlist
\item
  Projections are the essence of approximation: they give the ``best
  possible'' version of a vector inside a chosen subspace.
\item
  They formalize independence: the error vector is always orthogonal to
  the subspace.
\item
  They provide geometric intuition for statistics, machine learning, and
  numerical computation.
\end{itemize}

\subsubsection{Try It Yourself}\label{try-it-yourself-73}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute the projection of \(x = (2,3,4)\) onto \(u = (1,1,1)\).
\item
  Verify that the residual \(x - \text{proj}_u(x)\) is orthogonal to
  \(u\).
\item
  Write the projection matrix for the subspace spanned by
  \(\{(1,0,0),(0,1,0)\}\) in \(\mathbb{R}^3\).
\item
  Challenge: Prove that projection matrices are idempotent and
  symmetric.
\end{enumerate}

Projections turn vector spaces into cleanly split components: what lies
``inside'' a subspace and what lies ``outside.'' This idea, simple yet
profound, runs through geometry, data analysis, and physics alike.

\subsection{75. Orthogonal Decomposition
Theorem}\label{orthogonal-decomposition-theorem}

One of the cornerstones of linear algebra is the orthogonal
decomposition theorem, which states that every vector in an inner
product space can be uniquely split into two parts: one lying inside a
subspace and the other lying in its orthogonal complement. This gives us
a clear way to organize information, separate influences, and simplify
computations.

\subsubsection{Statement of the
Theorem}\label{statement-of-the-theorem-1}

Let \(V\) be an inner product space and \(W\) a subspace of \(V\). Then
for every vector \(x \in V\), there exist unique vectors \(w \in W\) and
\(z \in W^\perp\) such that

\[
x = w + z.
\]

Here:

\begin{itemize}
\tightlist
\item
  \(w = \text{proj}_W(x)\), the projection of \(x\) onto \(W\).
\item
  \(z = x - \text{proj}_W(x)\), the orthogonal component.
\end{itemize}

This decomposition is unique: no other pair of vectors from \(W\) and
\(W^\perp\) adds up to \(x\).

\subsubsection{\texorpdfstring{Example in
\(\mathbb{R}^2\)}{Example in \textbackslash mathbb\{R\}\^{}2}}\label{example-in-mathbbr2-5}

Take \(W\) to be the line spanned by \(u = (1,2)\). For \(x = (4,1)\):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Projection:

  \[
  \text{proj}_u(x) = \frac{\langle x,u \rangle}{\langle u,u \rangle} u.
  \]

  Compute: \(\langle x,u\rangle = 4\cdot 1 + 1\cdot 2 = 6\), and
  \(\langle u,u\rangle = 1^2+2^2=5\). So

  \[
  \text{proj}_u(x) = \frac{6}{5}(1,2) = \left(\tfrac{6}{5}, \tfrac{12}{5}\right).
  \]
\item
  Orthogonal component:

  \[
  z = x - \text{proj}_u(x) = (4,1) - \left(\tfrac{6}{5}, \tfrac{12}{5}\right) = \left(\tfrac{14}{5}, -\tfrac{7}{5}\right).
  \]
\item
  Verify:
  \(\langle u, z\rangle = 1\cdot \tfrac{14}{5} + 2\cdot (-\tfrac{7}{5}) = 0\).
  Thus, \(z \in W^\perp\).
\end{enumerate}

So we have

\[
x = \underbrace{\left(\tfrac{6}{5}, \tfrac{12}{5}\right)}_{\in W} + \underbrace{\left(\tfrac{14}{5}, -\tfrac{7}{5}\right)}_{\in W^\perp}.
\]

\subsubsection{Geometric Meaning}\label{geometric-meaning-19}

\begin{itemize}
\tightlist
\item
  The decomposition splits \(x\) into its ``in-subspace'' part and its
  ``out-of-subspace'' part.
\item
  \(w\) is the closest point in \(W\) to \(x\).
\item
  \(z\) is the leftover ``error,'' always perpendicular to \(W\).
\end{itemize}

Geometrically, the shortest path from \(x\) to a subspace is always
orthogonal.

\subsubsection{Orthogonal Complements}\label{orthogonal-complements}

\begin{itemize}
\item
  The orthogonal complement \(W^\perp\) contains all vectors orthogonal
  to every vector in \(W\).
\item
  Dimensional relationship:

  \[
  \dim(W) + \dim(W^\perp) = \dim(V).
  \]
\item
  Together, \(W\) and \(W^\perp\) partition the space \(V\).
\end{itemize}

\subsubsection{Projection Matrices and
Decomposition}\label{projection-matrices-and-decomposition}

If \(P\) is the projection matrix onto \(W\):

\[
x = Px + (I-P)x,
\]

where \(Px \in W\) and \((I-P)x \in W^\perp\).

This formulation is used constantly in numerical linear algebra.

\subsubsection{Everyday Analogies}\label{everyday-analogies-71}

\begin{itemize}
\tightlist
\item
  Work-life balance: Splitting your time into ``work'' and ``non-work,''
  where the two categories do not overlap.
\item
  Noise filtering: A signal is split into the meaningful part
  (projection onto a signal space) and pure noise (orthogonal
  component).
\item
  Budgeting: Expenses can be uniquely divided into ``planned''
  (projection) and ``unexpected'' (orthogonal) categories.
\item
  Debate: Arguments can be split into ``relevant to the topic'' and
  ``off-topic'' parts, independent of each other.
\end{itemize}

\subsubsection{Applications}\label{applications-37}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Least Squares Approximation: The best-fit solution is the projection;
  the error lies in the orthogonal complement.
\item
  Fourier Analysis: Any signal decomposes into a sum of components along
  orthogonal basis functions plus residuals.
\item
  Statistics: Regression decomposes data into explained variance (in the
  subspace of predictors) and residual variance (orthogonal).
\item
  Engineering: Splitting forces into parallel and perpendicular
  components relative to a surface.
\item
  Computer Graphics: Decomposing movement into screen-plane projection
  and depth (orthogonal direction).
\end{enumerate}

\subsubsection{Why It Matters}\label{why-it-matters-71}

\begin{itemize}
\tightlist
\item
  Orthogonal decomposition gives clarity: every vector splits into
  ``relevant'' and ``irrelevant'' parts relative to a chosen subspace.
\item
  It provides the foundation for least squares, regression, and signal
  approximation.
\item
  It ensures uniqueness, stability, and interpretability in vector
  computations.
\end{itemize}

\subsubsection{Try It Yourself}\label{try-it-yourself-74}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  In \(\mathbb{R}^3\), decompose \(x = (1,2,3)\) into components in
  span\((1,0,0)\) and its orthogonal complement.
\item
  Show that if \(W\) is spanned by \((1,1,0)\) and \((0,1,1)\), then any
  vector in \(\mathbb{R}^3\) can be uniquely split into \(W\) and
  \(W^\perp\).
\item
  Write down the projection matrix \(P\) for
  \(W = \text{span}\{(1,0,0),(0,1,0)\}\) in \(\mathbb{R}^3\). Verify
  that \(I-P\) projects onto \(W^\perp\).
\item
  Challenge: Prove the orthogonal decomposition theorem using projection
  matrices and the fact that \(P^2 = P\).
\end{enumerate}

The orthogonal decomposition theorem guarantees that every vector finds
its closest approximation in a chosen subspace and a perfectly
perpendicular remainder-an elegant structure that makes analysis and
computation possible in countless domains.

\subsection{76. Orthogonal Projections and Least
Squares}\label{orthogonal-projections-and-least-squares}

One of the deepest connections in linear algebra is between orthogonal
projections and the least squares method. When equations don't have an
exact solution, least squares finds the ``best approximate'' one. The
theory behind it is entirely geometric: the best solution is the
projection of a vector onto a subspace.

\subsubsection{The Setup: Overdetermined
Systems}\label{the-setup-overdetermined-systems}

Consider a system of equations \(Ax = b\), where

\begin{itemize}
\tightlist
\item
  \(A\) is an \(m \times n\) matrix with \(m > n\) (more equations than
  unknowns).
\item
  \(b \in \mathbb{R}^m\) may not lie in the column space of \(A\).
\end{itemize}

This means:

\begin{itemize}
\tightlist
\item
  There may be no exact solution.
\item
  Instead, we want \(x\) that makes \(Ax\) as close as possible to
  \(b\).
\end{itemize}

\subsubsection{Least Squares Problem}\label{least-squares-problem}

The least squares solution minimizes the error:

\[
\min_x \|Ax - b\|^2.
\]

Here:

\begin{itemize}
\tightlist
\item
  \(Ax\) is the projection of \(b\) onto the column space of \(A\).
\item
  The error vector \(b - Ax\) is orthogonal to the column space.
\end{itemize}

This is exactly the orthogonal decomposition theorem applied to \(b\).

\subsubsection{Derivation of Normal
Equations}\label{derivation-of-normal-equations}

We want \(r = b - Ax\) to be orthogonal to every column of \(A\):

\[
A^T (b - Ax) = 0.
\]

Rearranging:

\[
A^T A x = A^T b.
\]

This system is called the normal equations. Its solution \(x\) gives the
least squares approximation.

\subsubsection{Projection Matrix in Least
Squares}\label{projection-matrix-in-least-squares}

The projection of \(b\) onto \(\text{Col}(A)\) is

\[
\hat{b} = A(A^T A)^{-1} A^T b,
\]

assuming \(A^T A\) is invertible.

Here,

\begin{itemize}
\tightlist
\item
  \(P = A(A^T A)^{-1} A^T\) is the projection matrix onto the column
  space of \(A\).
\item
  The fitted vector is \(\hat{b} = Pb\).
\item
  The residual \(r = b - \hat{b}\) lies in the orthogonal complement of
  \(\text{Col}(A)\).
\end{itemize}

\subsubsection{Example}\label{example-2}

Suppose \(A = \begin{bmatrix}1 \\ 2 \\ 3\end{bmatrix}\),
\(b = \begin{bmatrix}2 \\ 2 \\ 4\end{bmatrix}\).

\begin{itemize}
\item
  Column space of \(A\): span of \((1,2,3)\).
\item
  Projection formula:

  \[
  \hat{b} = \frac{\langle b, A \rangle}{\langle A, A \rangle} A.
  \]
\item
  Compute: \(\langle b,A\rangle = 2\cdot1+2\cdot2+4\cdot3 = 18\).
  \(\langle A,A\rangle = 1^2+2^2+3^2=14\).
\item
  Projection:

  \[
  \hat{b} = \frac{18}{14}(1,2,3) = \left(\tfrac{9}{7}, \tfrac{18}{7}, \tfrac{27}{7}\right).
  \]
\item
  Residual:

  \[
  r = b - \hat{b} = \left(\tfrac{5}{7}, -\tfrac{4}{7}, \tfrac{1}{7}\right).
  \]
\end{itemize}

Check: \(\langle r,A\rangle = 0\), so it's orthogonal.

\subsubsection{Geometric Meaning}\label{geometric-meaning-20}

\begin{itemize}
\tightlist
\item
  The least squares solution is the point in \(\text{Col}(A)\) closest
  to \(b\).
\item
  The error vector is orthogonal to the subspace.
\item
  This is like dropping a perpendicular from \(b\) to the subspace
  \(\text{Col}(A)\).
\end{itemize}

\subsubsection{Everyday Analogies}\label{everyday-analogies-72}

\begin{itemize}
\tightlist
\item
  Fitting a line to data: The line won't pass through every point, but
  it minimizes the sum of squared vertical distances.
\item
  Negotiation: You can't satisfy every demand, but you settle in the
  ``closest possible'' compromise.
\item
  Budget cuts: You can't match every expense exactly, but you minimize
  the overall deviation.
\item
  Image compression: You can't store every detail, but you keep the
  closest possible low-dimensional version.
\end{itemize}

\subsubsection{Applications}\label{applications-38}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Statistics: Linear regression uses least squares to fit models to
  data.
\item
  Engineering: Curve fitting, system identification, and calibration.
\item
  Computer Graphics: Best-fit transformations (e.g., Procrustes
  analysis).
\item
  Machine Learning: Optimization of linear models (before moving to
  nonlinear methods).
\item
  Numerical Methods: Solving inconsistent systems of equations.
\end{enumerate}

\subsubsection{Why It Matters}\label{why-it-matters-72}

\begin{itemize}
\tightlist
\item
  Orthogonal projections explain why least squares gives the best
  approximation.
\item
  They reveal the geometry behind regression: data is projected onto the
  model space.
\item
  They connect linear algebra with statistics, optimization, and applied
  sciences.
\end{itemize}

\subsubsection{Try It Yourself}\label{try-it-yourself-75}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Solve \(\min_x \|Ax-b\|\) for
  \(A = \begin{bmatrix}1 & 1 \\ 1 & 2 \\ 1 & 3\end{bmatrix}\),
  \(b=(1,2,2)^T\). Interpret the result.
\item
  Derive the projection matrix \(P\) for this system.
\item
  Show that the residual is orthogonal to each column of \(A\).
\item
  Challenge: Prove that among all possible approximations \(Ax\), the
  least squares solution is unique if and only if \(A^T A\) is
  invertible.
\end{enumerate}

Orthogonal projections turn the messy, unsolvable world of
overdetermined equations into one of best possible approximations. Least
squares is not just an algebraic trick-it is the geometric essence of
``closeness'' in higher-dimensional spaces.

\subsection{77. QR Decomposition}\label{qr-decomposition}

QR decomposition is a factorization of a matrix into an orthogonal part
and a triangular part. It grows directly out of orthogonality and the
Gram--Schmidt process, and it plays a central role in numerical linear
algebra, providing a stable and efficient way to solve systems, compute
least squares solutions, and analyze matrices.

\subsubsection{Definition}\label{definition-4}

For a real \(m \times n\) matrix \(A\) with linearly independent
columns:

\[
A = QR,
\]

where:

\begin{itemize}
\tightlist
\item
  \(Q\) is an \(m \times n\) matrix with orthonormal columns
  (\(Q^T Q = I\)).
\item
  \(R\) is an \(n \times n\) upper triangular matrix.
\end{itemize}

This decomposition is unique if we require \(R\) to have positive
diagonal entries.

\subsubsection{Connection to
Gram--Schmidt}\label{connection-to-gramschmidt}

The Gram--Schmidt process applied to the columns of \(A\) produces the
orthonormal columns of \(Q\). The coefficients used during the
orthogonalization steps naturally form the entries of \(R\).

\begin{itemize}
\tightlist
\item
  Each column of \(A\) is expressed as a combination of the orthonormal
  columns of \(Q\).
\item
  The coefficients of this expression populate the triangular matrix
  \(R\).
\end{itemize}

\subsubsection{Example}\label{example-3}

Let

\[
A = \begin{bmatrix} 1 & 1 \\ 1 & 0 \\ 0 & 1 \end{bmatrix}.
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Apply Gram--Schmidt to the columns:

  \begin{itemize}
  \item
    \(v_1 = (1,1,0)^T\), normalize:

    \[
    u_1 = \frac{1}{\sqrt{2}}(1,1,0)^T.
    \]
  \item
    Subtract projection from \(v_2=(1,0,1)^T\):

    \[
    w_2 = v_2 - \langle v_2,u_1\rangle u_1.
    \]

    Compute
    \(\langle v_2,u_1\rangle = \tfrac{1}{\sqrt{2}}(1+0+0)=\tfrac{1}{\sqrt{2}}\).
    So

    \[
    w_2 = (1,0,1)^T - \tfrac{1}{\sqrt{2}}(1,1,0)^T = \left(\tfrac{1}{2}, -\tfrac{1}{2}, 1\right)^T.
    \]

    Normalize:

    \[
    u_2 = \frac{1}{\sqrt{1.5}} \left(\tfrac{1}{2}, -\tfrac{1}{2}, 1\right)^T.
    \]
  \end{itemize}
\item
  Construct \(Q = [u_1, u_2]\).
\item
  Compute \(R = Q^T A\).
\end{enumerate}

The result is \(A = QR\), with \(Q\) orthonormal and \(R\) triangular.

\subsubsection{Geometric Meaning}\label{geometric-meaning-21}

\begin{itemize}
\tightlist
\item
  \(Q\) represents an orthogonal change of basis-rotations and
  reflections that preserve length and angle.
\item
  \(R\) encodes scaling and shear in the new orthonormal coordinate
  system.
\item
  Together, they show how \(A\) transforms space: first rotate into a
  clean basis, then apply triangular distortion.
\end{itemize}

\subsubsection{Everyday Analogies}\label{everyday-analogies-73}

\begin{itemize}
\tightlist
\item
  Changing perspective: QR decomposition is like rotating your view so
  the structure of a problem becomes simpler, then measuring distortion
  in this aligned frame.
\item
  Team roles: \(Q\) represents independent, orthogonal roles, while
  \(R\) shows how much each role contributes to the final outcome.
\item
  Navigation: \(Q\) gives perfectly aligned compass directions, while
  \(R\) records how far you move along each.
\end{itemize}

\subsubsection{Applications}\label{applications-39}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Least Squares: Instead of solving \(A^T A x = A^T b\), we use \(QR\):

  \[
  Ax = b \quad \Rightarrow \quad QRx = b.
  \]

  Multiply by \(Q^T\):

  \[
  Rx = Q^T b.
  \]

  Since \(R\) is triangular, solving for \(x\) is efficient and
  numerically stable.
\item
  Eigenvalue Algorithms: The QR algorithm iteratively applies QR
  factorizations to approximate eigenvalues.
\item
  Numerical Stability: Orthogonal transformations minimize numerical
  errors compared to solving normal equations.
\item
  Machine Learning: Many algorithms (e.g., linear regression, PCA) use
  QR decomposition for efficiency and stability.
\item
  Computer Graphics: Orthogonal factors preserve shapes; triangular
  factors simplify transformations.
\end{enumerate}

\subsubsection{Why It Matters}\label{why-it-matters-73}

\begin{itemize}
\tightlist
\item
  QR decomposition bridges theory (Gram--Schmidt orthogonalization) and
  computation (matrix factorization).
\item
  It avoids pitfalls of normal equations, improving numerical stability.
\item
  It underpins algorithms across statistics, engineering, and computer
  science.
\end{itemize}

\subsubsection{Try It Yourself}\label{try-it-yourself-76}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Compute the QR decomposition of

  \[
  A = \begin{bmatrix}1 & 2 \\ 2 & 3 \\ 4 & 5\end{bmatrix}.
  \]
\item
  Verify that \(Q^T Q = I\) and \(R\) is upper triangular.
\item
  Use QR to solve the least squares problem \(Ax \approx b\) with
  \(b=(1,1,1)^T\).
\item
  Challenge: Show that if \(A\) is square and orthogonal, then \(R=I\)
  and \(Q=A\).
\end{enumerate}

QR decomposition turns the messy process of solving least squares into a
clean, geometric procedure-rotating into a better coordinate system
before solving. It is one of the most powerful tools in the linear
algebra toolkit.

\subsection{78. Orthogonal Matrices}\label{orthogonal-matrices}

Orthogonal matrices are square matrices whose rows and columns form an
orthonormal set. They are the algebraic counterpart of rigid motions in
geometry: transformations that preserve lengths, angles, and orientation
(except for reflections).

\subsubsection{Definition}\label{definition-5}

A square matrix \(Q \in \mathbb{R}^{n \times n}\) is orthogonal if

\[
Q^T Q = QQ^T = I.
\]

This means:

\begin{itemize}
\tightlist
\item
  The columns of \(Q\) are orthonormal.
\item
  The rows of \(Q\) are also orthonormal.
\end{itemize}

\subsubsection{Properties}\label{properties}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Inverse Equals Transpose:

  \[
  Q^{-1} = Q^T.
  \]

  This makes orthogonal matrices especially easy to invert.
\item
  Preservation of Norms: For any vector \(x\),

  \[
  \|Qx\| = \|x\|.
  \]

  Orthogonal transformations never stretch or shrink vectors.
\item
  Preservation of Inner Products:

  \[
  \langle Qx, Qy \rangle = \langle x, y \rangle.
  \]

  Angles are preserved.
\item
  Determinant: \(\det(Q) = \pm 1\).

  \begin{itemize}
  \tightlist
  \item
    If \(\det(Q) = 1\), \(Q\) is a rotation.
  \item
    If \(\det(Q) = -1\), \(Q\) is a reflection combined with rotation.
  \end{itemize}
\end{enumerate}

\subsubsection{Examples}\label{examples-4}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  2D Rotation Matrix:

  \[
  Q = \begin{bmatrix} \cos \theta & -\sin \theta \\ \sin \theta & \cos \theta \end{bmatrix}.
  \]

  Rotates vectors by angle \(\theta\).
\item
  2D Reflection Matrix: Reflection across the \(x\)-axis:

  \[
  Q = \begin{bmatrix} 1 & 0 \\ 0 & -1 \end{bmatrix}.
  \]
\item
  Permutation Matrices: Swapping coordinates is orthogonal because it
  preserves lengths. Example in 3D:

  \[
  Q = \begin{bmatrix}0 & 1 & 0 \\ 1 & 0 & 0 \\ 0 & 0 & 1\end{bmatrix}.
  \]
\end{enumerate}

\subsubsection{Geometric Meaning}\label{geometric-meaning-22}

Orthogonal matrices represent isometries: transformations that preserve
the shape of objects.

\begin{itemize}
\tightlist
\item
  They can rotate, reflect, or permute axes.
\item
  They never distort lengths or angles.
\end{itemize}

This is why in computer graphics, orthogonal matrices model pure
rotations and reflections without scaling.

\subsubsection{Everyday Analogies}\label{everyday-analogies-74}

\begin{itemize}
\tightlist
\item
  Photography: Rotating a camera keeps the picture content intact-it
  only changes orientation.
\item
  Maps: A compass rotation preserves distances between cities.
\item
  Dance moves: Rotating or flipping a formation keeps relative spacing
  unchanged.
\end{itemize}

\subsubsection{Applications}\label{applications-40}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Computer Graphics: Rotations of 3D models use orthogonal matrices to
  avoid distortion.
\item
  Numerical Linear Algebra: Orthogonal transformations are numerically
  stable, widely used in QR factorization and eigenvalue algorithms.
\item
  Data Compression: Orthogonal transforms like the Fourier and cosine
  transforms preserve energy.
\item
  Signal Processing: Orthogonal filters separate signals into
  independent components.
\item
  Physics: Orthogonal matrices describe rotations in rigid body
  dynamics.
\end{enumerate}

\subsubsection{Why It Matters}\label{why-it-matters-74}

\begin{itemize}
\tightlist
\item
  Orthogonal matrices are the building blocks of stable algorithms.
\item
  They describe symmetry, structure, and invariance in physical and
  computational systems.
\item
  They serve as the simplest and most powerful class of transformations
  that preserve geometry exactly.
\end{itemize}

\subsubsection{Try It Yourself}\label{try-it-yourself-77}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Verify that

  \[
  Q = \begin{bmatrix}0 & -1 \\ 1 & 0\end{bmatrix}
  \]

  is orthogonal. What geometric transformation does it represent?
\item
  Prove that the determinant of an orthogonal matrix must be \(\pm 1\).
\item
  Show that multiplying two orthogonal matrices gives another orthogonal
  matrix.
\item
  Challenge: Prove that eigenvalues of orthogonal matrices lie on the
  complex unit circle (i.e., \(|\lambda|=1\)).
\end{enumerate}

Orthogonal matrices capture the essence of symmetry: transformations
that preserve structure exactly. They lie at the heart of geometry,
physics, and computation.

\subsection{79. Fourier Viewpoint}\label{fourier-viewpoint}

The Fourier viewpoint is one of the most profound connections in linear
algebra: the idea that complex signals, data, or functions can be
decomposed into sums of simpler, orthogonal waves. Instead of describing
information in its raw form (time, space, or coordinates), we express it
in terms of frequencies. This perspective reshapes how we analyze,
compress, and understand information across mathematics, physics, and
engineering.

\subsubsection{Fourier Series: The Basic
Idea}\label{fourier-series-the-basic-idea}

Suppose we have a periodic function \(f(x)\) defined on \([-\pi, \pi]\).
The Fourier series expresses \(f(x)\) as:

\[
f(x) = a_0 + \sum_{n=1}^\infty \left( a_n \cos(nx) + b_n \sin(nx) \right).
\]

\begin{itemize}
\item
  The coefficients \(a_n, b_n\) are found using inner products with sine
  and cosine functions.
\item
  Each sine and cosine is orthogonal to the others under the inner
  product

  \[
  \langle f, g \rangle = \int_{-\pi}^\pi f(x) g(x) \, dx.
  \]
\end{itemize}

Thus, Fourier series is nothing more than expanding a function in an
orthonormal basis of trigonometric functions.

\subsubsection{Fourier Transform: From Time to
Frequency}\label{fourier-transform-from-time-to-frequency}

For non-periodic signals, the Fourier transform generalizes this
expansion. For a function \(f(t)\),

\[
\hat{f}(\omega) = \int_{-\infty}^\infty f(t) e^{-i \omega t} dt
\]

transforms it into frequency space. The inverse transform reconstructs
\(f(t)\) from its frequencies.

This is again an inner product viewpoint: the exponential functions
\(e^{i \omega t}\) act as orthogonal basis functions on \(\mathbb{R}\).

\subsubsection{Orthogonality of Waves}\label{orthogonality-of-waves}

The trigonometric functions \(\{\cos(nx), \sin(nx)\}\) and the complex
exponentials \(\{e^{i\omega t}\}\) form orthogonal families.

\begin{itemize}
\tightlist
\item
  Two different sine waves have zero inner product over a full period.
\item
  Likewise, exponentials with different frequencies are orthogonal.
\end{itemize}

This is exactly like orthogonal vectors in \(\mathbb{R}^n\), except here
the space is infinite-dimensional.

\subsubsection{Discrete Fourier Transform
(DFT)}\label{discrete-fourier-transform-dft}

In computational settings, we don't work with infinite integrals but
with finite data. The DFT expresses an \(n\)-dimensional vector
\(x = (x_0, \dots, x_{n-1})\) as a linear combination of orthogonal
complex exponentials:

\[
X_k = \sum_{j=0}^{n-1} x_j e^{-2\pi i jk / n}, \quad k=0,\dots,n-1.
\]

This is simply a change of basis: from the standard basis (time domain)
to the Fourier basis (frequency domain).

The Fast Fourier Transform (FFT) computes this in \(O(n \log n)\) time,
making Fourier analysis practical at scale.

\subsubsection{Geometric Meaning}\label{geometric-meaning-23}

\begin{itemize}
\tightlist
\item
  In the time domain, data is expressed as a sequence of raw values.
\item
  In the frequency domain, data is expressed as amplitudes of orthogonal
  waves.
\item
  The Fourier viewpoint is just a rotation into a new orthogonal
  coordinate system, exactly like diagonalizing a matrix or changing
  basis.
\end{itemize}

\subsubsection{Everyday Analogies}\label{everyday-analogies-75}

\begin{itemize}
\tightlist
\item
  Music: Any chord is a mixture of pure notes. The Fourier viewpoint
  extracts the individual notes (frequencies) from the sound.
\item
  Light: A beam of light is a mixture of colors (frequencies of waves).
  A prism ``performs'' a Fourier decomposition.
\item
  Finance: A price signal is decomposed into long-term trends (low
  frequencies) and short-term fluctuations (high frequencies).
\item
  Cooking: A dish is made from distinct ingredients-Fourier analysis
  tells you what proportions of each ingredient are in the mix.
\end{itemize}

\subsubsection{Applications}\label{applications-41}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Signal Processing: Filtering unwanted noise corresponds to removing
  high-frequency components.
\item
  Image Compression: JPEG uses Fourier-like transforms (cosine
  transforms) to compress images.
\item
  Data Analysis: Identifying cycles and periodic patterns in time
  series.
\item
  Physics: Quantum states are represented in both position and momentum
  bases, linked by Fourier transform.
\item
  Partial Differential Equations: Solutions are simplified by moving to
  frequency space, where derivatives become multipliers.
\end{enumerate}

\subsubsection{Why It Matters}\label{why-it-matters-75}

\begin{itemize}
\tightlist
\item
  Fourier methods turn difficult problems into simpler ones: convolution
  becomes multiplication, differentiation becomes scaling.
\item
  They provide a universal language for analyzing periodicity,
  oscillation, and wave phenomena.
\item
  They are linear algebra at heart: orthogonal expansions in special
  bases.
\end{itemize}

\subsubsection{Try It Yourself}\label{try-it-yourself-78}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute the Fourier series coefficients for \(f(x) = x\) on
  \([-\pi,\pi]\).
\item
  For the sequence \((1,0,0,0)\), compute the 4-point DFT and interpret
  the result.
\item
  Show that \(\int_{-\pi}^\pi \sin(mx)\cos(nx) dx = 0\).
\item
  Challenge: Prove that the Fourier basis
  \(\{e^{i2\pi k t}\}_{k=0}^{n-1}\) is orthonormal in \(\mathbb{C}^n\).
\end{enumerate}

The Fourier viewpoint reveals that every signal, no matter how complex,
can be seen as a combination of simple, orthogonal waves. It is a
perfect marriage of geometry, algebra, and analysis, and one of the most
important ideas in modern mathematics.

\subsection{80. Polynomial and Multifeature Least
Squares}\label{polynomial-and-multifeature-least-squares}

Least squares problems become especially powerful when extended to
fitting polynomials or handling multiple features at once. Instead of a
single straight line through data, we can fit curves of higher degree or
surfaces in higher dimensions. These generalizations lie at the heart of
regression, data analysis, and scientific modeling.

\subsubsection{From Line to Polynomial}\label{from-line-to-polynomial}

The simplest least squares model is a straight line:

\[
y \approx \beta_0 + \beta_1 x.
\]

But many relationships are nonlinear. Polynomial least squares
generalizes the model to:

\[
y \approx \beta_0 + \beta_1 x + \beta_2 x^2 + \dots + \beta_d x^d.
\]

Here, each power of \(x\) is treated as a new feature. The problem
reduces to ordinary least squares on the design matrix:

\[
A = \begin{bmatrix}
1 & x_1 & x_1^2 & \dots & x_1^d \\
1 & x_2 & x_2^2 & \dots & x_2^d \\
\vdots & \vdots & \vdots & & \vdots \\
1 & x_n & x_n^2 & \dots & x_n^d
\end{bmatrix},
\quad
\beta = \begin{bmatrix}\beta_0 \\ \beta_1 \\ \vdots \\ \beta_d\end{bmatrix}.
\]

The least squares solution minimizes \(\|A\beta - y\|\).

\subsubsection{Multiple Features}\label{multiple-features}

When data involves several predictors, we extend the model to:

\[
y \approx \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_p x_p.
\]

In matrix form:

\[
y \approx A\beta,
\]

where \(A\) is the design matrix with columns corresponding to features
(including a column of ones for the intercept).

The least squares solution is still given by the normal equations:

\[
\hat{\beta} = (A^T A)^{-1} A^T y,
\]

or more stably by QR or SVD factorizations.

\subsubsection{Example: Polynomial Fit}\label{example-polynomial-fit}

Suppose we have data points \((1,1), (2,2.2), (3,2.9), (4,4.1)\).
Fitting a quadratic model
\(y \approx \beta_0 + \beta_1 x + \beta_2 x^2\):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Construct design matrix:

  \[
  A = \begin{bmatrix}
  1 & 1 & 1 \\
  1 & 2 & 4 \\
  1 & 3 & 9 \\
  1 & 4 & 16
  \end{bmatrix}.
  \]
\item
  Solve least squares problem \(\min \|A\beta - y\|\).
\item
  The result gives coefficients \(\beta_0, \beta_1, \beta_2\) that best
  approximate the curve.
\end{enumerate}

The same process works for higher-degree polynomials or multiple
features.

\subsubsection{Geometric Meaning}\label{geometric-meaning-24}

\begin{itemize}
\tightlist
\item
  In polynomial least squares, the feature space expands: instead of
  points on a line, data lives in a higher-dimensional feature space
  \((1, x, x^2, \dots, x^d)\).
\item
  In multifeature least squares, the column space of \(A\) spans all
  possible linear combinations of features.
\item
  The least squares solution projects the observed output vector \(y\)
  onto this subspace.
\end{itemize}

Thus, whether polynomial or multifeature, the geometry is the same:
projection onto the model space.

\subsubsection{Everyday Analogies}\label{everyday-analogies-76}

\begin{itemize}
\tightlist
\item
  Forecasting growth: Linear models capture trends, but quadratic or
  cubic fits capture acceleration and curvature.
\item
  House prices: Price depends on multiple features (size, location,
  number of rooms). Multifeature least squares fits them simultaneously.
\item
  Sports performance: A player's score may depend on training time,
  rest, and nutrition-multiple interacting predictors.
\item
  Physics experiments: Data often follows quadratic or higher-order laws
  (projectile motion, energy vs.~displacement).
\end{itemize}

\subsubsection{Practical Challenges}\label{practical-challenges}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Overfitting: Higher-degree polynomials fit noise, not just signal.
\item
  Multicollinearity: Features may be correlated, making \(A^T A\) nearly
  singular.
\item
  Scaling: Features with different magnitudes should be normalized.
\item
  Regularization: Adding penalty terms (ridge or LASSO) stabilizes the
  solution.
\end{enumerate}

\subsubsection{Applications}\label{applications-42}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Regression in Statistics: Extending linear regression to handle
  multiple predictors or polynomial terms.
\item
  Machine Learning: Basis expansion for feature engineering (before
  neural nets, this was the standard).
\item
  Engineering: Curve fitting for calibration, modeling, and prediction.
\item
  Economics: Forecasting models with many variables (inflation, interest
  rates, spending).
\item
  Physics and Chemistry: Polynomial regression to model experimental
  data.
\end{enumerate}

\subsubsection{Why It Matters}\label{why-it-matters-76}

\begin{itemize}
\tightlist
\item
  Polynomial least squares captures curvature in data.
\item
  Multifeature least squares allows multiple predictors to explain
  outcomes.
\item
  Both generalizations turn linear algebra into a practical modeling
  tool across science and society.
\end{itemize}

\subsubsection{Try It Yourself}\label{try-it-yourself-79}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Fit a quadratic curve through points \((0,1), (1,2), (2,5), (3,10)\).
  Compare to a straight-line fit.
\item
  Construct a multifeature design matrix for predicting exam scores
  based on hours studied, sleep, and prior grades.
\item
  Show that polynomial regression is just linear regression on
  transformed features.
\item
  Challenge: Derive the bias--variance tradeoff in polynomial least
  squares-why higher degrees increase variance.
\end{enumerate}

Polynomial and multifeature least squares extend the reach of linear
algebra from straight lines to complex patterns, giving us a universal
framework for modeling relationships in data.

\subsubsection{Closing}\label{closing-7}

\begin{verbatim}
Closest lines are drawn,
errors fall away to rest,
angles guard the truth.
\end{verbatim}

\section{Chapter 9. SVD, PCA, and
conditioning}\label{chapter-9.-svd-pca-and-conditioning-1}

\subsection{Opening}\label{opening-7}

\begin{verbatim}
Closest lines are drawn,
errors fall away to rest,
angles guard the truth.
\end{verbatim}

\subsection{81. Singular Values and SVD}\label{singular-values-and-svd}

The Singular Value Decomposition (SVD) is one of the most powerful tools
in linear algebra. It generalizes eigen-decomposition, works for all
rectangular matrices (not just square ones), and provides deep insights
into geometry, computation, and data analysis. At its core, the SVD
tells us that every matrix can be factored into three pieces:
rotations/reflections, scaling, and rotations/reflections again.

\subsubsection{Definition of SVD}\label{definition-of-svd}

For any real \(m \times n\) matrix \(A\), the SVD is:

\[
A = U \Sigma V^T,
\]

where:

\begin{itemize}
\tightlist
\item
  \(U\) is an \(m \times m\) orthogonal matrix (columns = left singular
  vectors).
\item
  \(\Sigma\) is an \(m \times n\) diagonal matrix with nonnegative
  entries \(\sigma_1 \geq \sigma_2 \geq \dots \geq 0\) (singular
  values).
\item
  \(V\) is an \(n \times n\) orthogonal matrix (columns = right singular
  vectors).
\end{itemize}

Even if \(A\) is rectangular or not diagonalizable, this factorization
always exists.

\subsubsection{Geometric Meaning}\label{geometric-meaning-25}

The SVD describes how \(A\) transforms space:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  First rotation/reflection: Multiply by \(V^T\) to rotate or reflect
  coordinates into the right singular vector basis.
\item
  Scaling: Multiply by \(\Sigma\), stretching/shrinking each axis by a
  singular value.
\item
  Second rotation/reflection: Multiply by \(U\) to reorient into the
  output space.
\end{enumerate}

Thus, \(A\) acts as a rotation, followed by scaling, followed by another
rotation.

\subsubsection{Singular Values}\label{singular-values}

\begin{itemize}
\tightlist
\item
  The singular values \(\sigma_i\) are the square roots of the
  eigenvalues of \(A^T A\).
\item
  They measure how much \(A\) stretches space in particular directions.
\item
  The largest singular value \(\sigma_1\) is the operator norm of \(A\):
  the maximum stretch factor.
\item
  If some singular values are zero, they correspond to directions
  collapsed by \(A\).
\end{itemize}

\subsubsection{\texorpdfstring{Example in
\(\mathbb{R}^2\)}{Example in \textbackslash mathbb\{R\}\^{}2}}\label{example-in-mathbbr2-6}

Let

\[
A = \begin{bmatrix} 3 & 1 \\ 0 & 2 \end{bmatrix}.
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute \(A^T A = \begin{bmatrix} 9 & 3 \\ 3 & 5 \end{bmatrix}\).
\item
  Find its eigenvalues: \(\lambda_1, \lambda_2 = 10 \pm \sqrt{10}\).
\item
  Singular values: \(\sigma_i = \sqrt{\lambda_i}\).
\item
  The corresponding eigenvectors form the right singular vectors \(V\).
\item
  Left singular vectors \(U\) are obtained by \(U = AV/\Sigma\).
\end{enumerate}

This decomposition reveals how \(A\) reshapes circles into ellipses.

\subsubsection{Links to
Eigen-Decomposition}\label{links-to-eigen-decomposition}

\begin{itemize}
\tightlist
\item
  Eigen-decomposition works only for square, diagonalizable matrices.
\item
  SVD works for all matrices, square or rectangular, diagonalizable or
  not.
\item
  Instead of eigenvalues (which may be complex or negative), we get
  singular values (always real and nonnegative).
\item
  Eigenvectors can fail to exist in a full basis; singular vectors
  always form orthonormal bases.
\end{itemize}

\subsubsection{Applications}\label{applications-43}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Data Compression: Truncate small singular values to approximate
  matrices with fewer dimensions (used in JPEG).
\item
  Principal Component Analysis (PCA): SVD on centered data finds
  principal components, directions of maximum variance.
\item
  Least Squares Problems: SVD provides stable solutions, even for
  ill-conditioned or singular systems.
\item
  Noise Filtering: Discard small singular values to remove noise in
  signals and images.
\item
  Numerical Stability: SVD helps diagnose conditioning-how sensitive
  solutions are to input errors.
\end{enumerate}

\subsubsection{Everyday Analogies}\label{everyday-analogies-77}

\begin{itemize}
\tightlist
\item
  Music mixing: Any complex sound can be broken into independent
  ``tracks'' with different strengths.
\item
  Clothing store: A pile of clothes can be organized along principal
  directions (shirts, pants, jackets), with singular values measuring
  how much each dominates.
\item
  Maps: A geographic map distorts distances differently along different
  directions; singular values quantify those distortions.
\end{itemize}

\subsubsection{Why It Matters}\label{why-it-matters-77}

\begin{itemize}
\tightlist
\item
  SVD is the ``Swiss army knife'' of linear algebra: versatile, always
  applicable, and rich in interpretation.
\item
  It provides geometric, algebraic, and computational clarity.
\item
  It is indispensable for modern applications in machine learning,
  statistics, engineering, and physics.
\end{itemize}

\subsubsection{Try It Yourself}\label{try-it-yourself-80}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Compute the SVD of

  \[
  A = \begin{bmatrix}1 & 0 \\ 0 & 2 \\ 0 & 0\end{bmatrix}.
  \]

  Interpret the scaling and rotations.
\item
  Show that for any vector \(x\), \(\|Ax\| \leq \sigma_1 \|x\|\).
\item
  Use SVD to approximate the matrix

  \[
  \begin{bmatrix}1 & 1 & 1 \\ 1 & 1 & 1 \\ 1 & 1 & 1\end{bmatrix}
  \]

  with rank 1.
\item
  Challenge: Prove that the Frobenius norm of \(A\) is the square root
  of the sum of squares of its singular values.
\end{enumerate}

The singular value decomposition is universal: every matrix can be
dissected into rotations and scalings, revealing its structure and
enabling powerful techniques across mathematics and applied sciences.

\subsection{82. Geometry of SVD}\label{geometry-of-svd}

The Singular Value Decomposition (SVD) is not just an algebraic
factorization-it has a precise geometric meaning. It explains exactly
how any linear transformation reshapes space: stretching, rotating,
compressing, and possibly collapsing dimensions. Understanding this
geometry turns SVD from a formal tool into an intuitive picture of what
matrices do.

\subsubsection{Transformation of the Unit
Sphere}\label{transformation-of-the-unit-sphere}

Take the unit sphere (or circle, in 2D) in the input space. When we
apply a matrix \(A\):

\begin{itemize}
\tightlist
\item
  The sphere is transformed into an ellipsoid.
\item
  The axes of this ellipsoid correspond to the right singular vectors
  \(v_i\).
\item
  The lengths of the axes are the singular values \(\sigma_i\).
\item
  The directions of the axes in the output space are the left singular
  vectors \(u_i\).
\end{itemize}

Thus, SVD tells us:

\[
A v_i = \sigma_i u_i.
\]

Every matrix maps orthogonal basis directions into orthogonal ellipsoid
axes, scaled by singular values.

\subsubsection{Step-by-Step Geometry}\label{step-by-step-geometry}

The decomposition \(A = U \Sigma V^T\) can be read geometrically:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Rotate/reflect by \(V^T\): Align input coordinates with the
  ``principal directions'' of \(A\).
\item
  Scale by \(\Sigma\): Stretch or compress each axis by its singular
  value. Some singular values may be zero, flattening dimensions.
\item
  Rotate/reflect by \(U\): Reorient the scaled axes into the output
  space.
\end{enumerate}

This process is universal: no matter how irregular a matrix seems, it
always reshapes space by rotation → scaling → rotation.

\subsubsection{2D Example}\label{d-example}

Take

\[
A = \begin{bmatrix}3 & 1 \\ 0 & 2\end{bmatrix}.
\]

\begin{itemize}
\tightlist
\item
  A circle in \(\mathbb{R}^2\) is mapped into an ellipse.
\item
  The ellipse's major and minor axes align with the right singular
  vectors of \(A\).
\item
  Their lengths equal the singular values.
\item
  The ellipse itself is then oriented in the output plane according to
  the left singular vectors.
\end{itemize}

This makes SVD the perfect tool for visualizing how \(A\) ``distorts''
geometry.

\subsubsection{Stretching and Rank}\label{stretching-and-rank}

\begin{itemize}
\tightlist
\item
  If all singular values are positive, the ellipsoid has full dimension
  (no collapse).
\item
  If some singular values are zero, \(A\) flattens the sphere along
  certain directions, lowering the rank.
\item
  The rank of \(A\) equals the number of nonzero singular values.
\end{itemize}

Thus, rank-deficient matrices literally squash space into lower
dimensions.

\subsubsection{Distance and Energy
Preservation}\label{distance-and-energy-preservation}

\begin{itemize}
\tightlist
\item
  The largest singular value \(\sigma_1\) is how much \(A\) can stretch
  a vector.
\item
  The smallest nonzero singular value \(\sigma_r\) (where
  \(r = \text{rank}(A)\)) measures how much the matrix compresses.
\item
  The condition number \(\kappa(A) = \sigma_1 / \sigma_r\) measures
  distortion: small values mean nearly spherical stretching, large
  values mean extreme elongation.
\end{itemize}

\subsubsection{Everyday Analogies}\label{everyday-analogies-78}

\begin{itemize}
\tightlist
\item
  Elastic band: Stretching a circular band into an ellipse, with major
  and minor axes as singular values.
\item
  Maps: A globe projected onto flat paper distorts distances differently
  along latitude and longitude-like singular values stretching some
  directions more than others.
\item
  Clothing: Pulling fabric in one direction stretches it more along that
  axis while leaving other directions less affected.
\end{itemize}

\subsubsection{Applications of the
Geometry}\label{applications-of-the-geometry}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Data Compression: Keeping only the largest singular values keeps the
  ``major axes'' of variation.
\item
  PCA: Data is analyzed along orthogonal axes of greatest variance
  (singular vectors).
\item
  Numerical Analysis: Geometry of SVD shows why ill-conditioned systems
  amplify errors-because some directions are squashed almost flat.
\item
  Signal Processing: Elliptical distortions correspond to filtering out
  certain frequency components.
\item
  Machine Learning: Dimensionality reduction is essentially projecting
  data onto the largest singular directions.
\end{enumerate}

\subsubsection{Why It Matters}\label{why-it-matters-78}

\begin{itemize}
\tightlist
\item
  SVD transforms algebraic equations into geometric pictures.
\item
  It reveals exactly how matrices warp space, offering intuition behind
  abstract operations.
\item
  By interpreting ellipses, singular values, and orthogonal vectors, we
  gain visual clarity for problems in data, physics, and computation.
\end{itemize}

\subsubsection{Try It Yourself}\label{try-it-yourself-81}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Draw the unit circle in \(\mathbb{R}^2\), apply the matrix

  \[
  A = \begin{bmatrix}2 & 0 \\ 1 & 3\end{bmatrix},
  \]

  and sketch the resulting ellipse. Identify its axes and lengths.
\item
  Verify numerically that \(Av_i = \sigma_i u_i\) for computed singular
  vectors and singular values.
\item
  For a rank-1 matrix, sketch how the unit circle collapses to a line
  segment.
\item
  Challenge: Prove that the set of vectors with maximum stretch under
  \(A\) are precisely the first right singular vectors.
\end{enumerate}

The geometry of SVD gives us a universal lens: every linear
transformation is a controlled distortion of space, built from
orthogonal rotations and directional scalings.

\subsection{83. Relation to
Eigen-Decompositions}\label{relation-to-eigen-decompositions}

The Singular Value Decomposition (SVD) is often introduced as something
entirely new, but it is deeply tied to eigen-decomposition. In fact,
singular values and singular vectors emerge from the eigen-decomposition
of certain symmetric matrices constructed from \(A\). Understanding this
connection shows why SVD always exists, why singular values are
nonnegative, and how it generalizes eigen-analysis to all matrices, even
rectangular ones.

\subsubsection{Eigen-Decomposition
Recap}\label{eigen-decomposition-recap}

For a square matrix \(M \in \mathbb{R}^{n \times n}\), an
eigen-decomposition is:

\[
M = X \Lambda X^{-1},
\]

where \(\Lambda\) is a diagonal matrix of eigenvalues and the columns of
\(X\) are eigenvectors.

However:

\begin{itemize}
\tightlist
\item
  Not all matrices are diagonalizable.
\item
  Eigenvalues may be complex.
\item
  Rectangular matrices don't have eigenvalues at all.
\end{itemize}

This is where SVD provides a universal framework.

\subsubsection{\texorpdfstring{From \(A^T A\) to Singular
Values}{From A\^{}T A to Singular Values}}\label{from-at-a-to-singular-values}

For any \(m \times n\) matrix \(A\):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Consider the symmetric, positive semidefinite matrix
  \(A^T A \in \mathbb{R}^{n \times n}\).

  \begin{itemize}
  \tightlist
  \item
    Symmetry ensures all eigenvalues are real.
  \item
    Positive semidefiniteness ensures they are nonnegative.
  \end{itemize}
\item
  The eigenvalues of \(A^T A\) are squares of the singular values of
  \(A\):

  \[
  \lambda_i(A^T A) = \sigma_i^2.
  \]
\item
  The eigenvectors of \(A^T A\) are the right singular vectors \(v_i\).
\item
  Similarly, for \(AA^T\), eigenvalues are the same \(\sigma_i^2\), and
  eigenvectors are the left singular vectors \(u_i\).
\end{enumerate}

Thus:

\[
Av_i = \sigma_i u_i, \quad A^T u_i = \sigma_i v_i.
\]

This pair of relationships binds eigen-decomposition and SVD together.

\subsubsection{Why Eigen-Decomposition Is Not
Enough}\label{why-eigen-decomposition-is-not-enough}

\begin{itemize}
\tightlist
\item
  Eigen-decomposition requires a square matrix. SVD works for
  rectangular matrices.
\item
  Eigenvalues can be negative or complex; singular values are always
  real and nonnegative.
\item
  Eigenvectors may not exist as a complete basis; singular vectors
  always form orthonormal bases.
\end{itemize}

In short, SVD provides the robustness that eigen-decomposition lacks.

\subsubsection{Example}\label{example-4}

Let

\[
A = \begin{bmatrix}3 & 0 \\ 4 & 0 \\ 0 & 5\end{bmatrix}.
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Compute \(A^T A = \begin{bmatrix}25 & 0 \\ 0 & 25\end{bmatrix}\).

  \begin{itemize}
  \tightlist
  \item
    Eigenvalues: \(25, 25\).
  \item
    Singular values: \(\sigma_1 = \sigma_2 = 5\).
  \end{itemize}
\item
  Right singular vectors are eigenvectors of \(A^T A\). Here, they form
  the standard basis.
\item
  Left singular vectors come from \(Av_i / \sigma_i\).
\end{enumerate}

So the geometry of SVD is fully encoded in eigen-analysis of \(A^T A\)
and \(AA^T\).

\subsubsection{Geometric Picture}\label{geometric-picture-1}

\begin{itemize}
\tightlist
\item
  Eigenvectors of \(A^T A\) describe directions in input space where
  \(A\) stretches without mixing directions.
\item
  Eigenvectors of \(AA^T\) describe the corresponding directions in
  output space.
\item
  Singular values tell us how much stretching occurs.
\end{itemize}

Thus, SVD is essentially eigen-decomposition in disguise-but applied to
the right symmetric companions.

\subsubsection{Everyday Analogies}\label{everyday-analogies-79}

\begin{itemize}
\tightlist
\item
  Echo in a room: Eigen-decomposition studies how vibrations behave
  within the room's geometry. SVD studies both the input (sound waves)
  and output (what you hear) by linking them with matching directions.
\item
  Teamwork analogy: Eigen-decomposition tells you about the strengths of
  a team internally. SVD tells you how those strengths show up when
  tasks are actually performed.
\item
  Projection screens: Eigen-decomposition shows the internal stability
  of the projector lens; SVD shows how the input image translates into a
  stretched or compressed projection on the wall.
\end{itemize}

\subsubsection{Applications of the
Connection}\label{applications-of-the-connection}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  PCA: Data covariance matrix \(X^T X\) uses eigen-decomposition, but
  PCA is implemented with SVD directly.
\item
  Numerical Methods: Algorithms for SVD rely on eigen-analysis of
  \(A^T A\).
\item
  Stability Analysis: The relationship ensures singular values are
  reliable measures of conditioning.
\item
  Signal Processing: Power in signals (variance) is explained by
  eigenvalues of covariance, which connect to singular values.
\item
  Machine Learning: Kernel PCA and related methods depend on this link
  to handle nonlinear features.
\end{enumerate}

\subsubsection{Why It Matters}\label{why-it-matters-79}

\begin{itemize}
\tightlist
\item
  SVD explains every matrix transformation in terms of orthogonal bases
  and scalings.
\item
  Its relationship with eigen-decomposition ensures that SVD is not an
  alien tool, but a generalization.
\item
  The eigenview shows why SVD is guaranteed to exist and why singular
  values are always real and nonnegative.
\end{itemize}

\subsubsection{Try It Yourself}\label{try-it-yourself-82}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Prove that if \(v\) is an eigenvector of \(A^T A\) with eigenvalue
  \(\lambda\), then \(Av\) is either zero or a left singular vector of
  \(A\) with singular value \(\sqrt{\lambda}\).
\item
  For the matrix

  \[
  A = \begin{bmatrix}1 & 2 \\ 2 & 1\end{bmatrix},
  \]

  compute both eigen-decomposition and SVD. Compare the results.
\item
  Show that \(A^T A\) and \(AA^T\) always share the same nonzero
  eigenvalues.
\item
  Challenge: Explain why an orthogonal diagonalization of \(A^T A\) is
  enough to guarantee existence of the full SVD of \(A\).
\end{enumerate}

The relationship between SVD and eigen-decomposition unifies two of
linear algebra's deepest ideas: every matrix transformation is built
from eigen-geometry, stretched into a form that always exists and always
makes sense.

\subsection{84. Low-Rank Approximation (Best Small
Models)}\label{low-rank-approximation-best-small-models}

A central idea in data analysis, scientific computing, and machine
learning is that many datasets or matrices are far more complicated in
raw form than they truly need to be. Much of the apparent complexity
hides redundancy, noise, or low-dimensional patterns. Low-rank
approximation is the process of compressing a large, complicated matrix
into a smaller, simpler version that preserves the most important
information. This concept, grounded in the Singular Value Decomposition
(SVD), lies at the heart of dimensionality reduction, recommender
systems, and modern AI.

\subsubsection{The General Problem}\label{the-general-problem}

Suppose we have a matrix \(A \in \mathbb{R}^{m \times n}\), perhaps
representing:

\begin{itemize}
\tightlist
\item
  An image, with rows as pixels and columns as color channels.
\item
  A ratings table, with rows as users and columns as movies.
\item
  A word embedding matrix, with rows as words and columns as features.
\end{itemize}

Often, \(A\) is very large but highly structured. The question is:

\emph{Can we find a smaller matrix \(B\) of rank \(k\) (where
\(k \ll \min(m, n)\)) that approximates \(A\) well?}

\subsubsection{Rank and Complexity}\label{rank-and-complexity}

The rank of a matrix is the number of independent directions it encodes.
High rank means complexity; low rank means redundancy.

\begin{itemize}
\tightlist
\item
  A rank-1 matrix can be written as an outer product of two vectors:
  \(uv^T\).
\item
  A rank-\(k\) matrix is a sum of \(k\) such outer products.
\item
  Limiting rank controls how much structure we allow the approximation
  to capture.
\end{itemize}

\subsubsection{The SVD Solution}\label{the-svd-solution}

The SVD provides a natural decomposition:

\[
A = U \Sigma V^T,
\]

where singular values
\(\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_r\) measure importance.

To approximate \(A\) with rank \(k\):

\[
A_k = U_k \Sigma_k V_k^T,
\]

where we keep only the top \(k\) singular values and vectors.

This is not just a heuristic: it is the Eckart--Young theorem:

\begin{quote}
Among all rank-\(k\) matrices, \(A_k\) minimizes the error \(\|A - B\|\)
(both in Frobenius and spectral norm).
\end{quote}

Thus, SVD provides the \emph{best possible} low-rank approximation.

\subsubsection{Geometric Intuition}\label{geometric-intuition-6}

\begin{itemize}
\tightlist
\item
  Each singular value \(\sigma_i\) measures how strongly \(A\) stretches
  in the direction of singular vector \(v_i\).
\item
  Keeping the top \(k\) singular values means keeping the most important
  stretches and ignoring weaker directions.
\item
  The approximation captures the ``essence'' of \(A\) while discarding
  small, noisy, or redundant effects.
\end{itemize}

\subsubsection{Examples}\label{examples-5}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Images A grayscale image can be stored as a matrix of pixel
  intensities. Using SVD, one can compress it by keeping only the
  largest singular values:
\end{enumerate}

\begin{itemize}
\tightlist
\item
  \(k = 10\): blurry but recognizable image.
\item
  \(k = 50\): much sharper, yet storage cost is far less than full.
\item
  \(k = 200\): nearly indistinguishable from the original.
\end{itemize}

This is practical image compression: fewer numbers, same perception.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\item
  Recommender Systems Consider a user--movie rating matrix. Although it
  may be huge, the true patterns (genre preferences, popularity trends)
  live in a low-dimensional subspace. A rank-\(k\) approximation
  captures these patterns, predicting missing ratings by filling in the
  structure.
\item
  Natural Language Processing (NLP) Word embeddings often arise from
  co-occurrence matrices. Low-rank approximation via SVD extracts
  semantic structure, enabling words like ``king,'' ``queen,'' and
  ``crown'' to cluster together.
\end{enumerate}

\subsubsection{Error and Trade-Offs}\label{error-and-trade-offs}

\begin{itemize}
\tightlist
\item
  Error decay: If singular values drop quickly, small \(k\) gives a
  great approximation. If they decay slowly, more terms are needed.
\item
  Energy preserved: The squared singular values \(\sigma_i^2\) represent
  variance captured. Keeping the first \(k\) terms preserves most of the
  ``energy.''
\item
  Balance: Too low rank = oversimplification (loss of structure). Too
  high rank = no compression.
\end{itemize}

\subsubsection{Practical Computation}\label{practical-computation}

For very large matrices, full SVD is expensive (\(O(mn^2)\) for
\(m \geq n\)). Alternatives include:

\begin{itemize}
\tightlist
\item
  Truncated SVD algorithms (Lanczos, randomized methods).
\item
  Iterative methods that compute only the top \(k\) singular values.
\item
  Incremental approaches that update low-rank models as new data
  arrives.
\end{itemize}

These are vital in modern data science, where datasets often have
millions of entries.

\subsubsection{Analogy}\label{analogy}

\begin{itemize}
\tightlist
\item
  Music playlist: Imagine a playlist with hundreds of songs, but most
  are variations on a few themes. A low-rank approximation is like
  keeping only the core melodies while discarding repetitive riffs.
\item
  Photograph compression: Keeping only the brightest and most important
  strokes of light, while ignoring faint and irrelevant details.
\item
  Book summary: Instead of the full text, you read the essential plot
  points. That's low-rank approximation.
\end{itemize}

\subsubsection{Why It Matters}\label{why-it-matters-80}

\begin{itemize}
\tightlist
\item
  Reveals hidden structure in high-dimensional data.
\item
  Reduces storage and computational cost.
\item
  Filters noise while preserving the signal.
\item
  Provides the foundation for PCA, recommender systems, and
  dimensionality reduction.
\end{itemize}

\subsubsection{Try It Yourself}\label{try-it-yourself-83}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Take a small \(5 \times 5\) random matrix. Compute its SVD. Construct
  the best rank-1 approximation. Compare to the original.
\item
  Download a grayscale image (e.g., \(256 \times 256\)). Reconstruct it
  with 10, 50, and 100 singular values. Visually compare.
\item
  Prove the Eckart--Young theorem for the spectral norm: why can no
  other rank-\(k\) approximation do better than truncated SVD?
\item
  For a dataset with many features, compute PCA and explain why it is
  equivalent to finding a low-rank approximation.
\end{enumerate}

Low-rank approximation shows how linear algebra captures the essence of
complexity: most of what matters lives in a small number of dimensions.
The art is in finding and using them effectively.

\subsection{85. Principal Component Analysis (Variance and
Directions)}\label{principal-component-analysis-variance-and-directions}

Principal Component Analysis (PCA) is one of the most widely used
techniques in statistics, data analysis, and machine learning. It
provides a method to reduce the dimensionality of a dataset while
retaining as much important information as possible. The central insight
is that data often varies more strongly in some directions than others,
and by focusing on those directions we can summarize the dataset with
fewer dimensions, less noise, and more interpretability.

\subsubsection{The Basic Question}\label{the-basic-question}

Suppose we have data points in high-dimensional space, say
\(x_1, x_2, \dots, x_m \in \mathbb{R}^n\). Each point might be:

\begin{itemize}
\tightlist
\item
  A face image flattened into thousands of pixels.
\item
  A customer's shopping history across hundreds of products.
\item
  A gene expression profile across thousands of genes.
\end{itemize}

Storing and working with all features directly is expensive, and many
features may be redundant or correlated. PCA asks:

\emph{Can we re-express this data in a smaller set of directions that
capture the most variability?}

\subsubsection{Variance as Information}\label{variance-as-information}

The guiding principle of PCA is variance.

\begin{itemize}
\tightlist
\item
  Variance measures how spread out the data is along a direction.
\item
  High variance directions capture meaningful structure (e.g., different
  facial expressions, major spending habits).
\item
  Low variance directions often correspond to noise or unimportant
  fluctuations.
\end{itemize}

Thus, PCA searches for the directions (called principal components)
along which the variance of the data is maximized.

\subsubsection{Centering and Covariance}\label{centering-and-covariance}

To begin, we center the data by subtracting the mean vector:

\[
X_c = X - \mathbf{1}\mu^T,
\]

where \(\mu\) is the average of all data points.

The covariance matrix is then:

\[
C = \frac{1}{m} X_c^T X_c.
\]

\begin{itemize}
\tightlist
\item
  The diagonal entries measure variance of each feature.
\item
  Off-diagonal entries measure how features vary together.
\end{itemize}

Finding principal components is equivalent to finding the eigenvectors
of this covariance matrix.

\subsubsection{The Eigenview}\label{the-eigenview}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The eigenvectors of \(C\) are the directions (principal components).
\item
  The corresponding eigenvalues tell us how much variance lies along
  each component.
\item
  Sorting eigenvalues from largest to smallest gives the most
  informative to least informative directions.
\end{enumerate}

If we keep the top \(k\) eigenvectors, we project data into a
\(k\)-dimensional subspace that preserves most variance.

\subsubsection{The SVD View}\label{the-svd-view}

Another perspective uses the Singular Value Decomposition (SVD):

\[
X_c = U \Sigma V^T.
\]

\begin{itemize}
\tightlist
\item
  Columns of \(V\) are the principal directions.
\item
  Singular values squared (\(\sigma_i^2\)) correspond to eigenvalues of
  the covariance matrix.
\item
  Projecting onto the first \(k\) columns of \(V\) gives the reduced
  representation.
\end{itemize}

This makes PCA and SVD essentially the same computation.

\subsubsection{A Simple Example}\label{a-simple-example-1}

Imagine we measure height and weight of 1000 people. Plotting them shows
a strong correlation: taller people are often heavier. The cloud of
points stretches along a diagonal.

\begin{itemize}
\tightlist
\item
  PCA's first component is this diagonal line: the direction of maximum
  variance.
\item
  The second component is perpendicular, capturing the much smaller
  differences (like people of equal height but slightly different
  weights).
\item
  Keeping only the first component reduces two features into one while
  retaining most of the information.
\end{itemize}

\subsubsection{Geometric Picture}\label{geometric-picture-2}

\begin{itemize}
\tightlist
\item
  PCA rotates the coordinate system so that axes align with directions
  of greatest variance.
\item
  Projecting onto the top \(k\) components flattens the data into a
  lower-dimensional space, like flattening a tilted pancake onto its
  broadest plane.
\end{itemize}

\subsubsection{Everyday Analogies}\label{everyday-analogies-80}

\begin{itemize}
\tightlist
\item
  Photography: PCA is like adjusting the angle of a camera to capture
  the most important part of a scene in one shot.
\item
  Summarizing a book: Instead of every page, you keep only the main
  themes.
\item
  Music: A symphony can be broken into a few dominant melodies; the rest
  are variations.
\end{itemize}

\subsubsection{Applications}\label{applications-44}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Data Compression: Reduce storage by keeping only leading components
  (e.g., compressing images).
\item
  Noise Reduction: Small-variance directions often correspond to
  measurement noise; discarding them yields cleaner data.
\item
  Visualization: Reducing data to 2D or 3D for scatterplots helps us see
  clusters and patterns.
\item
  Preprocessing in Machine Learning: Many models train faster and
  generalize better on PCA-transformed data.
\item
  Genomics and Biology: PCA finds major axes of variation across
  thousands of genes.
\item
  Finance: PCA summarizes correlated movements of stocks into a few
  principal ``factors.''
\end{enumerate}

\subsubsection{Trade-Offs and
Limitations}\label{trade-offs-and-limitations}

\begin{itemize}
\tightlist
\item
  Interpretability: Principal components are linear combinations of
  original features, sometimes hard to explain in plain terms.
\item
  Linearity: PCA only captures linear relationships; nonlinear methods
  (like kernel PCA, t-SNE, or UMAP) may be better for curved manifolds.
\item
  Scaling: Features must be normalized properly; otherwise, PCA might
  overemphasize units with large raw variance.
\item
  Global Method: PCA captures overall variance, not local structures
  (e.g., small clusters within the data).
\end{itemize}

\subsubsection{Mathematical Guarantees}\label{mathematical-guarantees}

PCA has an optimality guarantee:

\begin{itemize}
\tightlist
\item
  Among all \(k\)-dimensional linear subspaces, the PCA subspace
  minimizes the reconstruction error (squared Euclidean distance between
  data and its projection).
\item
  This is essentially the low-rank approximation theorem seen earlier,
  applied to covariance matrices.
\end{itemize}

\subsubsection{Why It Matters}\label{why-it-matters-81}

PCA shows how linear algebra transforms raw data into insight. By
focusing on variance, it provides a principled way to filter noise,
compress information, and reveal hidden patterns. It is simple,
computationally efficient, and foundational-almost every modern data
pipeline uses PCA, explicitly or implicitly.

\subsubsection{Try It Yourself}\label{try-it-yourself-84}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Take a dataset of two correlated features (like height and weight).
  Compute the covariance matrix, eigenvectors, and project onto the
  first component. Visualize before and after.
\item
  For a grayscale image stored as a matrix, flatten it into vectors and
  apply PCA. How many components are needed to reconstruct it with 90\%
  accuracy?
\item
  Use PCA on the famous Iris dataset (4 features). Plot the data in 2D
  using the first two components. Notice how species separate in this
  reduced space.
\item
  Prove that the first principal component is the unit vector \(v\) that
  maximizes \(\|X_c v\|^2\).
\end{enumerate}

PCA distills complexity into clarity: it tells us not just where the
data is, but where it \emph{really goes}.

\subsection{86. Pseudoinverse (Moore--Penrose) and Solving Ill-Posed
Systems}\label{pseudoinverse-moorepenrose-and-solving-ill-posed-systems}

In linear algebra, the inverse of a matrix is a powerful tool: if \(A\)
is invertible, then solving \(Ax = b\) is as simple as \(x = A^{-1}b\).
But what happens when \(A\) is not square, or not invertible? In
practice, this is the norm: many problems involve rectangular matrices
(more equations than unknowns, or more unknowns than equations), or
square matrices that are singular. The Moore--Penrose pseudoinverse,
usually denoted \(A^+\), generalizes the idea of an inverse to all
matrices, providing a systematic way to find solutions-or best
approximations-when ordinary inversion fails.

\subsubsection{Why Ordinary Inverses
Fail}\label{why-ordinary-inverses-fail}

\begin{itemize}
\tightlist
\item
  Non-square matrices: If \(A\) is \(m \times n\) with \(m \neq n\), no
  standard inverse exists.
\item
  Singular matrices: Even if \(A\) is square, if \(\det(A) = 0\), it has
  no inverse.
\item
  Ill-posed problems: In real-world data, exact solutions may not exist
  (inconsistent systems) or may not be unique (underdetermined systems).
\end{itemize}

Despite these obstacles, we still want a systematic way to solve or
approximate \(Ax = b\).

\subsubsection{Definition of the
Pseudoinverse}\label{definition-of-the-pseudoinverse}

The Moore--Penrose pseudoinverse \(A^+\) is defined as the unique matrix
that satisfies four properties:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(AA^+A = A\).
\item
  \(A^+AA^+ = A^+\).
\item
  \((AA^+)^T = AA^+\).
\item
  \((A^+A)^T = A^+A\).
\end{enumerate}

These conditions ensure \(A^+\) acts as an ``inverse'' in the broadest
consistent sense.

\subsubsection{Constructing the Pseudoinverse with
SVD}\label{constructing-the-pseudoinverse-with-svd}

Given the SVD of \(A\):

\[
A = U \Sigma V^T,
\]

where \(\Sigma\) is diagonal with singular values
\(\sigma_1, \dots, \sigma_r\), the pseudoinverse is:

\[
A^+ = V \Sigma^+ U^T,
\]

where \(\Sigma^+\) is formed by inverting nonzero singular values and
transposing the matrix. Specifically:

\begin{itemize}
\tightlist
\item
  If \(\sigma_i \neq 0\), replace it with \(1/\sigma_i\).
\item
  If \(\sigma_i = 0\), leave it as 0.
\end{itemize}

This definition works for all matrices, square or rectangular.

\subsubsection{\texorpdfstring{Solving Linear Systems with
\(A^+\)}{Solving Linear Systems with A\^{}+}}\label{solving-linear-systems-with-a}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Overdetermined systems (\(m > n\), more equations than unknowns):

  \begin{itemize}
  \item
    Often no exact solution exists.
  \item
    The pseudoinverse gives the least-squares solution:

    \[
    x = A^+ b,
    \]

    which minimizes \(\|Ax - b\|\).
  \end{itemize}
\item
  Underdetermined systems (\(m < n\), more unknowns than equations):

  \begin{itemize}
  \item
    Infinitely many solutions exist.
  \item
    The pseudoinverse chooses the solution with the smallest norm:

    \[
    x = A^+ b,
    \]

    which minimizes \(\|x\|\) among all solutions.
  \end{itemize}
\item
  Square but singular systems:

  \begin{itemize}
  \tightlist
  \item
    Some solutions exist, but not uniquely.
  \item
    The pseudoinverse again picks the least-norm solution.
  \end{itemize}
\end{enumerate}

\subsubsection{Example 1:
Overdetermined}\label{example-1-overdetermined}

Suppose we want to solve:

\[
\begin{bmatrix}1 & 1 \\ 1 & -1 \\ 1 & 0\end{bmatrix} x = \begin{bmatrix}2 \\ 0 \\ 1\end{bmatrix}.
\]

This \(3 \times 2\) system has no exact solution. Using the
pseudoinverse, we obtain the least-squares solution that best fits all
three equations simultaneously.

\subsubsection{Example 2:
Underdetermined}\label{example-2-underdetermined}

For

\[
\begin{bmatrix}1 & 0 & 0 \\ 0 & 1 & 0\end{bmatrix} x = \begin{bmatrix}3 \\ 4\end{bmatrix},
\]

the system has infinitely many solutions because \(x_3\) is free. The
pseudoinverse gives:

\[
x = \begin{bmatrix}3 \\ 4 \\ 0\end{bmatrix},
\]

choosing the solution with minimum norm.

\subsubsection{Geometric
Interpretation}\label{geometric-interpretation-16}

\begin{itemize}
\tightlist
\item
  The pseudoinverse acts like projecting onto subspaces.
\item
  For overdetermined systems, it projects \(b\) onto the column space of
  \(A\), then finds the closest \(x\).
\item
  For underdetermined systems, it picks the point in the solution space
  closest to the origin.
\end{itemize}

So \(A^+\) embodies the principle of ``best possible inverse'' under the
circumstances.

\subsubsection{Everyday Analogies}\label{everyday-analogies-81}

\begin{itemize}
\tightlist
\item
  GPS triangulation: If multiple satellites give inconsistent
  measurements, the pseudoinverse provides the location that minimizes
  total error.
\item
  Fitting clothes: If a shirt doesn't fit perfectly, you pick the size
  that is closest overall; that's least squares in action.
\item
  Balancing a scale: When too many constraints pull in different
  directions, the pseudoinverse finds the compromise that balances them
  best.
\end{itemize}

\subsubsection{Applications}\label{applications-45}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Least-Squares Regression: Solving \(\min_x \|Ax - b\|^2\) via \(A^+\).
\item
  Signal Processing: Reconstructing signals from incomplete or noisy
  data.
\item
  Control Theory: Designing inputs when exact control is impossible.
\item
  Machine Learning: Training models with non-invertible design matrices.
\item
  Statistics: Computing generalized inverses of covariance matrices.
\end{enumerate}

\subsubsection{Limitations}\label{limitations}

\begin{itemize}
\tightlist
\item
  Sensitive to very small singular values: numerical instability may
  occur.
\item
  Regularization (like ridge regression) is often preferred in noisy
  settings.
\item
  Computationally expensive for very large matrices, though truncated
  SVD can help.
\end{itemize}

\subsubsection{Why It Matters}\label{why-it-matters-82}

The pseudoinverse is a unifying idea: it handles inconsistent,
underdetermined, or singular problems with one formula. It ensures we
always have a principled answer, even when classical algebra says ``no
solution'' or ``infinitely many solutions.'' In real data analysis,
almost every problem is ill-posed to some degree, making the
pseudoinverse a practical cornerstone of modern applied linear algebra.

\subsubsection{Try It Yourself}\label{try-it-yourself-85}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute the pseudoinverse of a simple \(2 \times 2\) singular matrix
  by hand using SVD.
\item
  Solve both an overdetermined (\(3 \times 2\)) and underdetermined
  (\(2 \times 3\)) system using \(A^+\). Compare with intuitive
  expectations.
\item
  Explore what happens numerically when singular values are very small.
  Try truncating them-this connects to regularization.
\end{enumerate}

The Moore--Penrose pseudoinverse shows that even when linear systems are
``broken,'' linear algebra still provides a systematic way forward.

\subsection{87. Conditioning and Sensitivity (How Errors
Amplify)}\label{conditioning-and-sensitivity-how-errors-amplify}

Linear algebra is not only about exact solutions-it is also about how
\emph{stable} those solutions are when data is perturbed. In real-world
applications, every dataset contains noise: measurement errors in
physics experiments, rounding errors in financial computations, or
floating-point precision limits in numerical software. Conditioning is
the study of how sensitive the solution of a problem is to small changes
in input. A well-conditioned problem reacts gently to perturbations; an
ill-conditioned one amplifies errors dramatically.

\subsubsection{The Basic Idea}\label{the-basic-idea}

Suppose we solve the linear system:

\[
Ax = b.
\]

Now imagine we slightly change \(b\) to \(b + \delta b\). The new
solution is \(x + \delta x\).

\begin{itemize}
\tightlist
\item
  If \(\|\delta x\|\) is about the same size as \(\|\delta b\|\), the
  problem is well-conditioned.
\item
  If \(\|\delta x\|\) is much larger, the problem is ill-conditioned.
\end{itemize}

Conditioning measures this amplification factor.

\subsubsection{Condition Number}\label{condition-number}

The central tool is the condition number of a matrix \(A\):

\[
\kappa(A) = \|A\| \cdot \|A^{-1}\|,
\]

where \(\|\cdot\|\) is a matrix norm (often the 2-norm).

\begin{itemize}
\tightlist
\item
  If \(\kappa(A)\) is close to 1, the problem is well-conditioned.
\item
  If \(\kappa(A)\) is large (say, \(10^6\) or higher), the problem is
  ill-conditioned.
\end{itemize}

Interpretation:

\begin{itemize}
\tightlist
\item
  \(\kappa(A)\) estimates the maximum relative error in the solution
  compared to the relative error in the data.
\item
  In practical terms, every digit of accuracy in \(b\) may be lost in
  \(x\) if \(\kappa(A)\) is too large.
\end{itemize}

\subsubsection{Singular Values and
Conditioning}\label{singular-values-and-conditioning}

Condition number in 2-norm can be expressed using singular values:

\[
\kappa(A) = \frac{\sigma_{\max}}{\sigma_{\min}},
\]

where \(\sigma_{\max}\) and \(\sigma_{\min}\) are the largest and
smallest singular values of \(A\).

\begin{itemize}
\tightlist
\item
  If the smallest singular value is tiny compared to the largest, \(A\)
  nearly collapses some directions, making inversion unstable.
\item
  This explains why nearly singular matrices are so problematic in
  numerical computation.
\end{itemize}

\subsubsection{Example 1: A Stable
System}\label{example-1-a-stable-system}

\[
A = \begin{bmatrix}2 & 0 \\ 0 & 3\end{bmatrix}.
\]

Here, \(\sigma_{\max} = 3, \sigma_{\min} = 2\). So
\(\kappa(A) = 3/2 = 1.5\). Very well-conditioned: small changes in input
produce small changes in output.

\subsubsection{Example 2: An Ill-Conditioned
System}\label{example-2-an-ill-conditioned-system}

\[
A = \begin{bmatrix}1 & 1 \\ 1 & 1.0001\end{bmatrix}.
\]

The determinant is very small, so the system is nearly singular.

\begin{itemize}
\tightlist
\item
  One singular value is about 2.0.
\item
  The other is about 0.0001.
\item
  Condition number: \(\kappa(A) \approx 20000\).
\end{itemize}

This means even tiny changes in \(b\) can wildly change \(x\).

\subsubsection{Geometric Intuition}\label{geometric-intuition-7}

A matrix transforms a unit sphere into an ellipse.

\begin{itemize}
\tightlist
\item
  The longest axis of the ellipse = \(\sigma_{\max}\).
\item
  The shortest axis = \(\sigma_{\min}\).
\item
  The ratio \(\sigma_{\max} / \sigma_{\min}\) shows how stretched the
  transformation is.
\end{itemize}

If the ellipse is nearly flat, directions aligned with the short axis
almost vanish, and recovering them is highly unstable.

\subsubsection{Everyday Analogies}\label{everyday-analogies-82}

\begin{itemize}
\tightlist
\item
  Whispering in a noisy room: If the signal is much weaker than the
  background noise, any message is drowned out. That's an
  ill-conditioned system.
\item
  Balancing a broomstick on your finger: Small wobbles cause big
  movements-sensitive, unstable, ill-conditioned.
\item
  Maps: A distorted map stretches some regions (well-measured) and
  squashes others (poorly represented). That's condition number in
  geometry.
\end{itemize}

\subsubsection{Why Conditioning Matters in
Computation}\label{why-conditioning-matters-in-computation}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Numerical Precision: Computers store numbers with limited precision
  (floating-point). An ill-conditioned system magnifies rounding errors,
  leading to unreliable results.
\item
  Regression: In statistics, highly correlated features make the design
  matrix ill-conditioned, destabilizing coefficient estimates.
\item
  Machine Learning: Ill-conditioning leads to unstable training,
  exploding or vanishing gradients.
\item
  Engineering: Control systems based on ill-conditioned models may be
  hypersensitive to measurement errors.
\end{enumerate}

\subsubsection{Techniques for Handling
Ill-Conditioning}\label{techniques-for-handling-ill-conditioning}

\begin{itemize}
\tightlist
\item
  Regularization: Add a penalty term, like ridge regression
  (\(\lambda I\)), to stabilize inversion.
\item
  Truncated SVD: Ignore tiny singular values that only amplify noise.
\item
  Scaling and Preconditioning: Rescale data or multiply by a well-chosen
  matrix to improve conditioning.
\item
  Avoiding Explicit Inverses: Use factorizations (LU, QR, SVD) rather
  than computing \(A^{-1}\).
\end{itemize}

\subsubsection{Connection to Previous
Topics}\label{connection-to-previous-topics}

\begin{itemize}
\tightlist
\item
  Pseudoinverse: Ill-conditioning is visible when singular values
  approach zero, making \(A^+\) unstable.
\item
  Low-rank approximation: Truncating small singular values both
  compresses data and improves conditioning.
\item
  PCA: Discarding low-variance components is essentially a conditioning
  improvement step.
\end{itemize}

\subsubsection{Why It Matters}\label{why-it-matters-83}

Conditioning bridges abstract algebra and numerical reality. Linear
algebra promises solutions, but conditioning tells us whether those
solutions are trustworthy. Without it, one might misinterpret noise as
signal, or lose all accuracy in computations that look fine on paper.

\subsubsection{Try It Yourself}\label{try-it-yourself-86}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute the condition number of
  \(\begin{bmatrix}1 & 1 \\ 1 & 1.0001\end{bmatrix}\). Solve for \(x\)
  in \(Ax = b\) for several slightly different \(b\). Watch how
  solutions swing dramatically.
\item
  Take a dataset with nearly collinear features. Compute the condition
  number of its covariance matrix. Relate this to instability in
  regression coefficients.
\item
  Simulate numerical errors: Add random noise of size \(10^{-6}\) to an
  ill-conditioned system and observe solution errors.
\item
  Prove that \(\kappa(A) \geq 1\) always holds.
\end{enumerate}

Conditioning reveals the hidden fragility of problems. It warns us when
algebra says ``solution exists'' but computation whispers ``don't trust
it.''

\subsection{88. Matrix Norms and Singular Values (Measuring Size
Properly)}\label{matrix-norms-and-singular-values-measuring-size-properly}

In linear algebra, we often need to measure the ``size'' of a matrix.
For vectors, this is straightforward: the length (norm) tells us how big
the vector is. But for matrices, the question is more subtle: do we
measure size by entries, by how much the matrix stretches vectors, or by
some invariant property? Different contexts demand different answers,
and matrix norms-closely tied to singular values-provide the framework
for doing so.

\subsubsection{Why Measure the Size of a
Matrix?}\label{why-measure-the-size-of-a-matrix}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Stability: To know how much error a matrix might amplify.
\item
  Conditioning: The ratio of largest to smallest stretching.
\item
  Optimization: Many algorithms minimize some matrix norm.
\item
  Data analysis: Norms measure complexity or energy of data.
\end{enumerate}

Without norms, we cannot compare matrices, analyze sensitivity, or judge
approximation quality.

\subsubsection{Matrix Norms from Vector
Norms}\label{matrix-norms-from-vector-norms}

A natural way to define a matrix norm is to ask: \emph{How much does
this matrix stretch vectors?}

Formally, for a given vector norm \(\|\cdot\|\):

\[
\|A\| = \max_{x \neq 0} \frac{\|Ax\|}{\|x\|}.
\]

This is called the induced matrix norm.

\subsubsection{The 2-Norm and Singular
Values}\label{the-2-norm-and-singular-values}

When we use the Euclidean norm (\(\|x\|_2\)) for vectors, the induced
matrix norm becomes:

\[
\|A\|_2 = \sigma_{\max}(A),
\]

the largest singular value of \(A\).

\begin{itemize}
\tightlist
\item
  This means the 2-norm measures the \emph{maximum stretching factor}.
\item
  Geometrically: \(A\) maps the unit sphere into an ellipse; \(\|A\|_2\)
  is the length of the ellipse's longest axis.
\end{itemize}

This link makes singular values the natural language for matrix size.

\subsubsection{Other Common Norms}\label{other-common-norms}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Frobenius Norm
\end{enumerate}

\[
\|A\|_F = \sqrt{\sum_{i,j} |a_{ij}|^2}.
\]

\begin{itemize}
\item
  Equivalent to the Euclidean length of all entries stacked in one big
  vector.
\item
  Can also be expressed as:

  \[
  \|A\|_F^2 = \sum_i \sigma_i^2.
  \]
\item
  Often used in data science and machine learning because it is easy to
  compute and differentiable.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  1-Norm
\end{enumerate}

\[
\|A\|_1 = \max_j \sum_i |a_{ij}|,
\]

the maximum absolute column sum.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Infinity Norm
\end{enumerate}

\[
\|A\|_\infty = \max_i \sum_j |a_{ij}|,
\]

the maximum absolute row sum.

Both are computationally cheap, useful in numerical analysis.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Nuclear Norm (Trace Norm)
\end{enumerate}

\[
\|A\|_* = \sum_i \sigma_i,
\]

the sum of singular values.

\begin{itemize}
\tightlist
\item
  Important in low-rank approximation and machine learning (matrix
  completion, recommender systems).
\end{itemize}

\subsubsection{Singular Values as the Unifying
Thread}\label{singular-values-as-the-unifying-thread}

\begin{itemize}
\tightlist
\item
  Spectral norm (2-norm): maximum singular value.
\item
  Frobenius norm: root of the sum of squared singular values.
\item
  Nuclear norm: sum of singular values.
\end{itemize}

Thus, norms capture different ways of summarizing singular values:
maximum, sum, or energy.

\subsubsection{Example: Small Matrix}\label{example-small-matrix}

Take

\[
A = \begin{bmatrix}3 & 4 \\ 0 & 0\end{bmatrix}.
\]

\begin{itemize}
\tightlist
\item
  Singular values: \(\sigma_1 = 5, \sigma_2 = 0\).
\item
  \(\|A\|_2 = 5\).
\item
  \(\|A\|_F = \sqrt{3^2 + 4^2} = 5\).
\item
  \(\|A\|_* = 5\).
\end{itemize}

Here, different norms coincide, but generally they highlight different
aspects of the matrix.

\subsubsection{Geometric Intuition}\label{geometric-intuition-8}

\begin{itemize}
\tightlist
\item
  2-norm: ``How much can \(A\) stretch a vector?''
\item
  Frobenius norm: ``What is the overall energy in all entries?''
\item
  1-norm / ∞-norm: ``What is the heaviest column or row load?''
\item
  Nuclear norm: ``How much total stretching power does \(A\) have?''
\end{itemize}

Each is a lens, giving a different perspective.

\subsubsection{Everyday Analogies}\label{everyday-analogies-83}

\begin{itemize}
\item
  Car performance:

  \begin{itemize}
  \tightlist
  \item
    Top speed = spectral norm (max stretch).
  \item
    Average fuel usage = Frobenius norm (overall energy).
  \item
    Total mileage cost = nuclear norm (total contribution).
  \end{itemize}
\item
  Team output:

  \begin{itemize}
  \tightlist
  \item
    Strongest player = spectral norm.
  \item
    Combined effort = Frobenius norm.
  \item
    Collective capacity = nuclear norm.
  \end{itemize}
\end{itemize}

\subsubsection{Applications}\label{applications-46}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Numerical Stability: Condition number
  \(\kappa(A) = \sigma_{\max}/\sigma_{\min}\) uses the spectral norm.
\item
  Machine Learning: Nuclear norm is used for matrix completion (Netflix
  Prize).
\item
  Image Compression: Frobenius norm measures reconstruction error.
\item
  Control Theory: 1-norm and ∞-norm bound system responses.
\item
  Optimization: Norms serve as penalties or constraints, shaping
  solutions.
\end{enumerate}

\subsubsection{Why It Matters}\label{why-it-matters-84}

Matrix norms provide the language to compare, approximate, and control
matrices. Singular values ensure that this language is not arbitrary but
grounded in geometry. Together, they explain how matrices distort space,
how error grows, and how we can measure complexity.

\subsubsection{Try It Yourself}\label{try-it-yourself-87}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  For \(A = \begin{bmatrix}1 & 2 \\ 3 & 4\end{bmatrix}\), compute
  \(\|A\|_1\), \(\|A\|_\infty\), \(\|A\|_F\), and \(\|A\|_2\) (using SVD
  for the last). Compare.
\item
  Prove that \(\|A\|_F^2 = \sum \sigma_i^2\).
\item
  Show that \(\|A\|_2 \leq \|A\|_F \leq \|A\|_*\). Interpret
  geometrically.
\item
  Consider a rank-1 matrix \(uv^T\). What are its norms? Which are
  equal?
\end{enumerate}

Matrix norms and singular values are the measuring sticks of linear
algebra-they tell us not just how big a matrix is, but how it acts,
where it is stable, and when it is fragile.

\subsection{89. Regularization (Ridge/Tikhonov to Tame
Instability)}\label{regularization-ridgetikhonov-to-tame-instability}

When solving linear systems or regression problems, instability often
arises because the system is ill-conditioned: tiny errors in data lead
to huge swings in the solution. Regularization is the strategy of
\emph{adding stability} by deliberately modifying the problem,
sacrificing exactness for robustness. The two most common
approaches-ridge regression and Tikhonov regularization-embody this
principle.

\subsubsection{The Problem of
Instability}\label{the-problem-of-instability}

Consider the least-squares problem:

\[
\min_x \|Ax - b\|_2^2.
\]

If \(A\) has nearly dependent columns, or if \(\sigma_{\min}(A)\) is
very small, then:

\begin{itemize}
\tightlist
\item
  Solutions are unstable.
\item
  Coefficients \(x\) can explode in magnitude.
\item
  Predictions vary wildly with small changes in \(b\).
\end{itemize}

Regularization modifies the objective so that the solution prefers
stability over exactness.

\subsubsection{Ridge / Tikhonov
Regularization}\label{ridge-tikhonov-regularization}

The modified problem is:

\[
\min_x \big( \|Ax - b\|_2^2 + \lambda \|x\|_2^2 \big),
\]

where \(\lambda > 0\) is the regularization parameter.

\begin{itemize}
\tightlist
\item
  The first term enforces data fit.
\item
  The second term penalizes large coefficients, discouraging unstable
  solutions.
\end{itemize}

This is called ridge regression in statistics and Tikhonov
regularization in numerical analysis.

\subsubsection{The Closed-Form Solution}\label{the-closed-form-solution}

Expanding the objective and differentiating gives:

\[
x_\lambda = (A^T A + \lambda I)^{-1} A^T b.
\]

Key points:

\begin{itemize}
\tightlist
\item
  The added \(\lambda I\) makes the matrix invertible, even if \(A^T A\)
  is singular.
\item
  As \(\lambda \to 0\), the solution approaches the ordinary
  least-squares solution.
\item
  As \(\lambda \to \infty\), the solution shrinks toward 0.
\end{itemize}

\subsubsection{SVD View}\label{svd-view}

If \(A = U \Sigma V^T\), then the least-squares solution is:

\[
x = \sum_i \frac{u_i^T b}{\sigma_i} v_i.
\]

If \(\sigma_i\) is very small, the term \(\frac{1}{\sigma_i}\) causes
instability.

With regularization:

\[
x_\lambda = \sum_i \frac{\sigma_i}{\sigma_i^2 + \lambda} (u_i^T b) v_i.
\]

\begin{itemize}
\tightlist
\item
  Small singular values (unstable directions) are suppressed.
\item
  Large singular values (stable directions) are mostly preserved.
\end{itemize}

This explains why ridge regression stabilizes solutions: it damps
noise-amplifying directions.

\subsubsection{Geometric
Interpretation}\label{geometric-interpretation-17}

\begin{itemize}
\tightlist
\item
  The unregularized problem fits \(b\) exactly in the column space of
  \(A\).
\item
  Regularization tilts the solution toward the origin, shrinking
  coefficients.
\item
  Geometrically, the feasible region (ellipsoid from \(Ax\)) intersects
  with a ball constraint from \(\|x\|_2\). The solution is where these
  two shapes balance.
\end{itemize}

\subsubsection{Everyday Analogies}\label{everyday-analogies-84}

\begin{itemize}
\tightlist
\item
  Steering on ice: Without regularization, the car responds wildly to
  tiny wheel turns. With regularization, steering is damped, keeping
  control.
\item
  Balancing investments: Purely chasing returns (fit) leads to fragile
  portfolios. Regularization is like risk management, preferring stable
  outcomes.
\item
  Tuning a guitar: Without damping, strings resonate uncontrollably.
  Adding a damper produces cleaner, controlled sound.
\end{itemize}

\subsubsection{Extensions}\label{extensions-1}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Lasso (\(\ell_1\) regularization): Replaces \(\|x\|_2^2\) with
  \(\|x\|_1\), encouraging sparse solutions.
\item
  Elastic Net: Combines ridge and lasso penalties.
\item
  General Tikhonov: Uses \(\|Lx\|_2^2\) with some matrix \(L\),
  tailoring the penalty (e.g., smoothing in signal processing).
\item
  Bayesian View: Ridge regression corresponds to placing a Gaussian
  prior on coefficients.
\end{enumerate}

\subsubsection{Applications}\label{applications-47}

\begin{itemize}
\tightlist
\item
  Machine Learning: Prevents overfitting in regression and
  classification.
\item
  Signal Processing: Suppresses noise when reconstructing signals.
\item
  Image Reconstruction: Stabilizes inverse problems like deblurring.
\item
  Numerical PDEs: Adds smoothness constraints to solutions.
\item
  Econometrics and Finance: Controls instability from highly correlated
  variables.
\end{itemize}

\subsubsection{Why It Matters}\label{why-it-matters-85}

Regularization transforms fragile problems into reliable ones. It
acknowledges the reality of noise and finite precision, and instead of
chasing impossible exactness, it provides usable, stable answers. In
modern data-driven fields, almost every large-scale model relies on
regularization for robustness.

\subsubsection{Try It Yourself}\label{try-it-yourself-88}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Solve the system \(Ax = b\) where

  \[
  A = \begin{bmatrix}1 & 1 \\ 1 & 1.0001\end{bmatrix}, \quad b = \begin{bmatrix}2 \\ 2\end{bmatrix}.
  \]

  Compare the unregularized least-squares solution with
  ridge-regularized solutions for \(\lambda = 0.01, 1, 10\).
\item
  Using the SVD, show how coefficients for small singular values are
  shrunk.
\item
  In regression with many correlated features, compute coefficient paths
  as \(\lambda\) varies. Observe how they stabilize.
\item
  Explore image denoising: apply ridge regularization to a blurred/noisy
  image reconstruction problem.
\end{enumerate}

Regularization shows the wisdom of linear algebra in practice: sometimes
the best solution is not the exact one, but the stable one.

\subsection{90. Rank-Revealing QR and Practical Diagnostics (What Rank
Really
Is)}\label{rank-revealing-qr-and-practical-diagnostics-what-rank-really-is}

Rank-the number of independent directions in a matrix-is central to
linear algebra. It tells us about solvability of systems, redundancy of
features, and the dimensionality of data. But in practice, computing
rank is not as simple as counting pivots or checking determinants.
Real-world data is noisy, nearly dependent, or high-dimensional.
Rank-revealing QR (RRQR) factorization and related diagnostics provide
stable, practical tools for uncovering rank and structure.

\subsubsection{Why Rank Matters}\label{why-rank-matters}

\begin{itemize}
\tightlist
\item
  Linear systems: Rank determines if a system has a unique solution,
  infinitely many, or none.
\item
  Data science: Rank measures intrinsic dimensionality, guiding
  dimensionality reduction.
\item
  Numerics: Small singular values make effective rank ambiguous-exact
  vs.~numerical rank diverge.
\end{itemize}

Thus, we need reliable algorithms to decide ``how many directions
matter'' in a matrix.

\subsubsection{Exact Rank vs.~Numerical
Rank}\label{exact-rank-vs.-numerical-rank}

\begin{itemize}
\tightlist
\item
  Exact rank: Defined over exact arithmetic. A column is independent if
  it cannot be expressed as a linear combination of others.
\item
  Numerical rank: In floating-point computation, tiny singular values
  cannot be trusted. A threshold \(\epsilon\) determines when we treat
  them as zero.
\end{itemize}

For example, if the smallest singular value of \(A\) is \(10^{-12}\),
and computations are in double precision (\(\sim 10^{-16}\)), we might
consider the effective rank smaller than full.

\subsubsection{The QR Factorization
Recap}\label{the-qr-factorization-recap}

The basic QR factorization expresses a matrix
\(A \in \mathbb{R}^{m \times n}\) as:

\[
A = QR,
\]

where:

\begin{itemize}
\tightlist
\item
  \(Q\) is orthogonal (\(Q^T Q = I\)), preserving lengths.
\item
  \(R\) is upper triangular, holding the ``essence'' of \(A\).
\end{itemize}

QR is stable, fast, and forms the backbone of many algorithms.

\subsubsection{Rank-Revealing QR (RRQR)}\label{rank-revealing-qr-rrqr}

RRQR is an enhancement of QR with column pivoting:

\[
A P = Q R,
\]

where \(P\) is a permutation matrix that reorders columns.

\begin{itemize}
\tightlist
\item
  The pivoting ensures that the largest independent directions come
  first.
\item
  The diagonal entries of \(R\) indicate which columns are significant.
\item
  Small values on the diagonal signal dependent (or nearly dependent)
  directions.
\end{itemize}

In practice, RRQR allows us to approximate rank by examining the decay
of \(R\)'s diagonal.

\subsubsection{Comparing RRQR and SVD}\label{comparing-rrqr-and-svd}

\begin{itemize}
\tightlist
\item
  SVD: Gold standard for determining rank; singular values give exact
  scaling of each direction.
\item
  RRQR: Faster and cheaper; sufficient when approximate rank is enough.
\item
  Trade-off: SVD is more accurate, RRQR is more efficient.
\end{itemize}

Both are used depending on the balance of precision and cost.

\subsubsection{Example}\label{example-5}

Let

\[
A = \begin{bmatrix}1 & 1 & 1 \\ 1 & 1.0001 & 2 \\ 1 & 2 & 3\end{bmatrix}.
\]

\begin{itemize}
\tightlist
\item
  Exact arithmetic: rank = 3.
\item
  Numerically: second column is nearly dependent on the first. SVD shows
  a singular value near zero.
\item
  RRQR with pivoting identifies the near-dependence by revealing a tiny
  diagonal in \(R\).
\end{itemize}

Thus, RRQR ``reveals'' effective rank without fully computing SVD.

\subsubsection{Practical Diagnostics for
Rank}\label{practical-diagnostics-for-rank}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Condition Number: A high condition number suggests
  near-rank-deficiency.
\item
  Diagonal of R in RRQR: Monitors independence of columns.
\item
  Singular Values in SVD: Most reliable indicator, but expensive.
\item
  Determinants/Minors: Useful in theory, unstable in practice.
\end{enumerate}

\subsubsection{Everyday Analogies}\label{everyday-analogies-85}

\begin{itemize}
\tightlist
\item
  Team redundancy: If two players always move together, the team has
  fewer independent strategies. RRQR identifies the most independent
  players first.
\item
  Cooking recipes: If one recipe is just a slight variation of another,
  your cookbook effectively has fewer distinct dishes. Numerical rank
  captures this redundancy.
\item
  Music notes: Different instruments may play, but if they harmonize in
  sync, the number of independent melodies is smaller than the number of
  instruments.
\end{itemize}

\subsubsection{Applications}\label{applications-48}

\begin{itemize}
\tightlist
\item
  Data Compression: Identifying effective rank allows truncation.
\item
  Regression: Detecting multicollinearity by examining rank of the
  design matrix.
\item
  Control Systems: Rank tests stability and controllability.
\item
  Machine Learning: Dimensionality reduction pipelines (e.g., PCA) start
  with rank estimation.
\item
  Signal Processing: Identifying number of underlying sources from
  mixtures.
\end{itemize}

\subsubsection{Why It Matters}\label{why-it-matters-86}

Rank is simple in theory, but elusive in practice. RRQR and related
diagnostics bridge the gap between exact mathematics and noisy data.
They allow practitioners to say, with stability and confidence:
\emph{this is how many independent directions really matter.}

\subsubsection{Try It Yourself}\label{try-it-yourself-89}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Implement RRQR with column pivoting on a small \(5 \times 5\) nearly
  dependent matrix. Compare estimated rank with SVD.
\item
  Explore the relationship between diagonal entries of \(R\) and
  numerical rank.
\item
  Construct a dataset with 100 features, where 95 are random noise but 5
  are linear combinations. Use RRQR to detect redundancy.
\item
  Prove that column pivoting does not change the column space of \(A\),
  only its numerical stability.
\end{enumerate}

Rank-revealing QR shows that linear algebra is not only about exact
formulas but also about practical diagnostics-knowing when two
directions are truly different and when they are essentially the same.

\subsubsection{Closing}\label{closing-8}

\begin{verbatim}
Noise reduced to still,
singular values unfold space,
essence shines within.
\end{verbatim}

\section{Chapter 10. Applications and
computation}\label{chapter-10.-applications-and-computation-1}

\subsubsection{Opening}\label{opening-8}

\begin{verbatim}
Worlds in numbers bloom,
graphs and data interlace,
algebra takes flight.
\end{verbatim}

\subsection{91. 2D/3D Geometry Pipelines (Cameras, Rotations, and
Transforms)}\label{d3d-geometry-pipelines-cameras-rotations-and-transforms}

Linear algebra is the silent backbone of modern graphics, robotics, and
computer vision. Every time an image is rendered on a screen, a camera
captures a scene, or a robot arm moves in space, a series of matrix
multiplications are transforming points from one coordinate system to
another. These geometry pipelines map 3D reality into 2D
representations, ensuring that objects appear in the correct position,
orientation, and scale.

\subsubsection{The Geometry of
Coordinates}\label{the-geometry-of-coordinates}

A point in 3D space is represented as a column vector:

\[
p = \begin{bmatrix} x \\ y \\ z \end{bmatrix}.
\]

But computers often extend this to homogeneous coordinates, embedding
the point in 4D:

\[
p_h = \begin{bmatrix} x \\ y \\ z \\ 1 \end{bmatrix}.
\]

The extra coordinate allows translations to be represented as matrix
multiplications, keeping the entire pipeline consistent: every step is
just multiplying by a matrix.

\subsubsection{Transformations in 2D and
3D}\label{transformations-in-2d-and-3d}

\begin{itemize}
\item
  Translation Moves a point by \((t_x, t_y, t_z)\).

  \[
  T = \begin{bmatrix} 
  1 & 0 & 0 & t_x \\ 
  0 & 1 & 0 & t_y \\ 
  0 & 0 & 1 & t_z \\ 
  0 & 0 & 0 & 1 
  \end{bmatrix}.
  \]
\item
  Scaling Expands or shrinks space along each axis.

  \[
  S = \begin{bmatrix} 
  s_x & 0 & 0 & 0 \\ 
  0 & s_y & 0 & 0 \\ 
  0 & 0 & s_z & 0 \\ 
  0 & 0 & 0 & 1 
  \end{bmatrix}.
  \]
\item
  Rotation In 3D, rotation around the z-axis is:

  \[
  R_z(\theta) = \begin{bmatrix} 
  \cos\theta & -\sin\theta & 0 & 0 \\ 
  \sin\theta & \cos\theta & 0 & 0 \\ 
  0 & 0 & 1 & 0 \\ 
  0 & 0 & 0 & 1 
  \end{bmatrix}.
  \]

  Similar forms exist for rotations around the x- and y-axes.
\end{itemize}

Each transformation is linear (or affine), and chaining them is just
multiplying matrices.

\subsubsection{The Camera Pipeline}\label{the-camera-pipeline}

Rendering a 3D object to a 2D image follows a sequence of steps, each
one a matrix multiplication:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Model Transform Moves the object from its local coordinates into world
  coordinates.
\item
  View Transform Puts the camera at the origin and aligns its axes with
  the world, effectively changing the point of view.
\item
  Projection Transform Projects 3D points into 2D. Two types:

  \begin{itemize}
  \tightlist
  \item
    Orthographic: parallel projection, no perspective.
  \item
    Perspective: distant objects appear smaller, closer to human vision.
  \end{itemize}

  Example of perspective projection:

  \[
  P = \begin{bmatrix} 
  f & 0 & 0 & 0 \\ 
  0 & f & 0 & 0 \\ 
  0 & 0 & 1 & 0 
  \end{bmatrix},
  \]

  where \(f\) is focal length.
\item
  Viewport Transform Maps normalized 2D coordinates to screen pixels.
\end{enumerate}

This sequence-from object to image-is the geometry pipeline.

\subsubsection{Example: Rendering a
Cube}\label{example-rendering-a-cube}

\begin{itemize}
\tightlist
\item
  Start with cube vertices in local coordinates (\([-1,1]^3\)).
\item
  Apply a scaling matrix to stretch it.
\item
  Apply a rotation matrix to tilt it.
\item
  Apply a translation matrix to move it into the scene.
\item
  Apply a projection matrix to flatten it onto the screen.
\end{itemize}

Every step is linear algebra, and the final picture is the result of
multiplying many matrices in sequence.

\subsubsection{Robotics Connection}\label{robotics-connection}

Robotic arms use similar pipelines: each joint contributes a rotation or
translation, encoded as a matrix. By multiplying them, we get the
forward kinematics-the position and orientation of the hand given the
joint angles.

\subsubsection{Everyday Analogies}\label{everyday-analogies-86}

\begin{itemize}
\tightlist
\item
  Photography: The model transform arranges the objects, the view
  transform positions the camera, and the projection is the lens that
  flattens the scene.
\item
  Stage performance: Actors move on stage (translations), rotate to face
  the audience (rotations), and lighting projects their shadows onto a
  backdrop (projection).
\item
  Navigation: GPS coordinates are transformed into map coordinates, then
  into screen pixels-another geometry pipeline.
\end{itemize}

\subsubsection{Why It Matters}\label{why-it-matters-87}

Geometry pipelines unify graphics, robotics, and vision. They show how
linear algebra powers the everyday visuals of video games, animations,
simulations, and even self-driving cars. Without the consistency of
matrix multiplication, the complexity of managing transformations would
be unmanageable.

\subsubsection{Try It Yourself}\label{try-it-yourself-90}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Write down the sequence of matrices that rotate a square by 45°, scale
  it by 2, and translate it by \((3, 1)\). Multiply them to get the
  combined transformation.
\item
  Construct a cube in 3D and simulate a perspective projection by hand
  for one vertex.
\item
  For a simple 2-joint robotic arm, represent each joint with a rotation
  matrix and compute the final hand position.
\item
  Prove that composing affine transformations is closed under
  multiplication-why does this make pipelines possible?
\end{enumerate}

Geometry pipelines are the bridge between abstract linear algebra and
tangible visual and mechanical systems. They are how math becomes
movement, light, and image.

\subsection{92. Computer Graphics and Robotics (Homogeneous Tricks in
Action)}\label{computer-graphics-and-robotics-homogeneous-tricks-in-action}

Linear algebra doesn't just stay on the chalkboard-it drives the engines
of computer graphics and robotics. Both fields need to describe and
manipulate objects in space, often moving between multiple coordinate
systems. The homogeneous coordinate trick-adding one extra
dimension-makes this elegant: translations, scalings, and rotations all
fit into a single framework of matrix multiplication. This uniformity
allows efficient computation and consistent pipelines.

\subsubsection{Homogeneous Coordinates
Recap}\label{homogeneous-coordinates-recap}

In 2D, a point \((x, y)\) becomes \([x, y, 1]^T\). In 3D, a point
\((x, y, z)\) becomes \([x, y, z, 1]^T\).

Why add the extra 1? Because then translations-normally not
linear-become linear in the higher-dimensional embedding. Every affine
transformation (rotations, scalings, shears, reflections, and
translations) is just a single multiplication by a homogeneous matrix.

Example:

\[
T = \begin{bmatrix} 
1 & 0 & 0 & t_x \\ 
0 & 1 & 0 & t_y \\ 
0 & 0 & 1 & t_z \\ 
0 & 0 & 0 & 1 
\end{bmatrix}, \quad 
p_h' = T p_h.
\]

This trick makes pipelines modular: just multiply the matrices in order.

\subsubsection{Computer Graphics
Pipelines}\label{computer-graphics-pipelines}

Graphics engines (like OpenGL or DirectX) rely entirely on homogeneous
transformations:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Model Matrix: Puts the object in the scene.

  \begin{itemize}
  \tightlist
  \item
    Example: Rotate a car 90° and translate it 10 units forward.
  \end{itemize}
\item
  View Matrix: Positions the virtual camera.

  \begin{itemize}
  \tightlist
  \item
    Equivalent to moving the world so the camera sits at the origin.
  \end{itemize}
\item
  Projection Matrix: Projects 3D points to 2D.

  \begin{itemize}
  \tightlist
  \item
    Perspective projection shrinks faraway objects, orthographic
    doesn't.
  \end{itemize}
\item
  Viewport Matrix: Converts normalized 2D coordinates into screen
  pixels.
\end{enumerate}

Every pixel you see in a video game has passed through this stack of
matrices.

\subsubsection{Robotics Pipelines}\label{robotics-pipelines}

In robotics, the same principle applies:

\begin{itemize}
\tightlist
\item
  A robot arm with joints is modeled as a chain of rigid-body
  transformations.
\item
  Each joint contributes a rotation or translation matrix.
\item
  Multiplying them gives the final pose of the robot's end-effector
  (hand, tool, or gripper).
\end{itemize}

This is called forward kinematics.

Example: A 2D robotic arm with two joints:

\[
p = R(\theta_1) T(l_1) R(\theta_2) T(l_2) \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}.
\]

Here \(R(\theta_i)\) are rotation matrices and \(T(l_i)\) are
translations along the arm length. Multiplying them gives the position
of the hand.

\subsubsection{Shared Challenges in Graphics and
Robotics}\label{shared-challenges-in-graphics-and-robotics}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Precision: Numerical round-off errors can accumulate; stable
  algorithms are critical.
\item
  Speed: Both fields demand real-time computation-60 frames per second
  for graphics, millisecond reaction times for robots.
\item
  Hierarchy: Objects in graphics may be nested (a car's wheel rotates
  relative to the car), just like robot joints. Homogeneous transforms
  naturally handle these hierarchies.
\item
  Inverse Problems: Graphics uses inverse transforms for camera
  movement; robotics uses them for inverse kinematics (finding joint
  angles to reach a point).
\end{enumerate}

\subsubsection{Everyday Analogies}\label{everyday-analogies-87}

\begin{itemize}
\tightlist
\item
  Video games: Moving your character or rotating the camera is just
  matrix math running millions of times per second.
\item
  Animation rigs: A character's skeleton is a robotic arm in disguise,
  joints moving in sequence.
\item
  Augmented reality: Overlaying digital objects onto real-world camera
  images requires the same transforms as robotics calibration.
\end{itemize}

\subsubsection{Why Homogeneous Tricks Are
Powerful}\label{why-homogeneous-tricks-are-powerful}

\begin{itemize}
\tightlist
\item
  Uniformity: One system (matrix multiplication) handles all
  transformations.
\item
  Efficiency: Hardware (GPUs, controllers) can optimize matrix
  operations directly.
\item
  Scalability: Works the same in 2D, 3D, or higher.
\item
  Composability: Long pipelines are just products of matrices, avoiding
  special cases.
\end{itemize}

\subsubsection{Applications}\label{applications-49}

\begin{itemize}
\tightlist
\item
  Graphics: Rendering engines, VR/AR, CAD software, motion capture.
\item
  Robotics: Arm manipulators, drones, autonomous vehicles, humanoid
  robots.
\item
  Crossover: Simulation platforms use the same math to test robots and
  render virtual environments.
\end{itemize}

\subsubsection{Try It Yourself}\label{try-it-yourself-91}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Build a 2D transformation pipeline: rotate a triangle, translate it,
  and project it into screen space. Write down the final transformation
  matrix.
\item
  Model a simple 2-joint robotic arm. Derive the forward kinematics
  using homogeneous matrices.
\item
  Implement a camera transform: place a cube at \((0,0,5)\), move the
  camera to \((0,0,0)\), and compute its 2D screen projection.
\item
  Show that composing a rotation and translation directly is equivalent
  to embedding them into a homogeneous matrix and multiplying.
\end{enumerate}

Homogeneous coordinates are the hidden secret that lets graphics and
robots share the same mathematical DNA. They unify how we move pixels,
machines, and virtual worlds.

\subsection{93. Graphs, Adjacency, and Laplacians (Networks via
Matrices)}\label{graphs-adjacency-and-laplacians-networks-via-matrices}

Linear algebra provides a powerful language for studying graphs-networks
of nodes connected by edges. From social networks to electrical
circuits, from the internet's structure to biological pathways, graphs
appear everywhere. Matrices give graphs a numerical form, making it
possible to analyze their structure using algebraic techniques.

\subsubsection{Graph Basics Recap}\label{graph-basics-recap}

\begin{itemize}
\tightlist
\item
  A graph \(G = (V, E)\) has a set of vertices \(V\) (nodes) and edges
  \(E\) (connections).
\item
  Graphs may be undirected or directed, weighted or unweighted.
\item
  Many graph properties-connectivity, flow, clusters-can be studied
  through matrices.
\end{itemize}

\subsubsection{The Adjacency Matrix}\label{the-adjacency-matrix}

For a graph with \(n\) vertices, the adjacency matrix
\(A \in \mathbb{R}^{n \times n}\) encodes connections:

\[
A_{ij} = \begin{cases} 
w_{ij}, & \text{if there is an edge from node \(i\) to node \(j\)} \\ 
0, & \text{otherwise}
\end{cases}
\]

\begin{itemize}
\tightlist
\item
  Unweighted graphs: entries are 0 or 1.
\item
  Weighted graphs: entries are edge weights (distances, costs,
  capacities).
\item
  Undirected graphs: \(A\) is symmetric.
\item
  Directed graphs: \(A\) may be asymmetric.
\end{itemize}

The adjacency matrix is the algebraic fingerprint of the graph.

\subsubsection{Powers of the Adjacency
Matrix}\label{powers-of-the-adjacency-matrix}

The entry \((A^k)_{ij}\) counts the number of walks of length \(k\) from
node \(i\) to node \(j\).

\begin{itemize}
\tightlist
\item
  \(A^2\) tells how many two-step connections exist.
\item
  This property is used in algorithms for detecting paths, clustering,
  and network flow.
\end{itemize}

\subsubsection{The Degree Matrix}\label{the-degree-matrix}

The degree of a vertex is the number of edges connected to it (or the
sum of weights in weighted graphs).

The degree matrix \(D\) is diagonal:

\[
D_{ii} = \sum_j A_{ij}.
\]

This matrix measures how ``connected'' each node is.

\subsubsection{The Graph Laplacian}\label{the-graph-laplacian}

The combinatorial Laplacian is defined as:

\[
L = D - A.
\]

Key properties:

\begin{itemize}
\tightlist
\item
  \(L\) is symmetric (for undirected graphs).
\item
  Each row sums to zero.
\item
  The smallest eigenvalue is always 0, with eigenvector
  \([1, 1, \dots, 1]^T\).
\end{itemize}

The Laplacian encodes connectivity: if the graph splits into \(k\)
connected components, then \(L\) has exactly \(k\) zero eigenvalues.

\subsubsection{Normalized Laplacians}\label{normalized-laplacians}

Two common normalized versions are:

\[
L_{sym} = D^{-1/2} L D^{-1/2}, \quad L_{rw} = D^{-1} L.
\]

These rescale the Laplacian for applications like spectral clustering.

\subsubsection{Spectral Graph Theory}\label{spectral-graph-theory}

Eigenvalues and eigenvectors of \(A\) or \(L\) reveal structure:

\begin{itemize}
\tightlist
\item
  Algebraic connectivity: The second-smallest eigenvalue of \(L\)
  measures how well connected the graph is.
\item
  Spectral clustering: Eigenvectors of \(L\) partition graphs into
  communities.
\item
  Random walks: Transition probabilities relate to \(D^{-1}A\).
\end{itemize}

\subsubsection{Example: A Simple Graph}\label{example-a-simple-graph}

Take a triangle graph with 3 nodes, each connected to the other two.

\[
A = \begin{bmatrix} 
0 & 1 & 1 \\ 
1 & 0 & 1 \\ 
1 & 1 & 0 
\end{bmatrix}, \quad 
D = \begin{bmatrix} 
2 & 0 & 0 \\ 
0 & 2 & 0 \\ 
0 & 0 & 2 
\end{bmatrix}, \quad 
L = \begin{bmatrix} 
2 & -1 & -1 \\ 
-1 & 2 & -1 \\ 
-1 & -1 & 2 
\end{bmatrix}.
\]

\begin{itemize}
\tightlist
\item
  Eigenvalues of \(L\): \(0, 3, 3\).
\item
  The single zero eigenvalue confirms the graph is connected.
\end{itemize}

\subsubsection{Everyday Analogies}\label{everyday-analogies-88}

\begin{itemize}
\tightlist
\item
  Social networks: Adjacency matrices track friendships; Laplacians
  reveal communities.
\item
  Transportation: Nodes are cities, edges are roads; eigenvalues measure
  robustness of connectivity.
\item
  Electric circuits: Laplacians act like conductance matrices, governing
  current flow.
\item
  Internet links: Adjacency captures hyperlinks; spectral analysis finds
  central hubs.
\end{itemize}

\subsubsection{Applications}\label{applications-50}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Community Detection: Spectral clustering finds natural divisions in
  social or biological networks.
\item
  Graph Drawing: Eigenvectors of \(L\) provide coordinates for visually
  embedding graphs.
\item
  Random Walks \& PageRank: Transition matrices from adjacency define
  importance scores.
\item
  Physics: Laplacians appear in discrete versions of diffusion and
  vibration problems.
\item
  Machine Learning: Graph neural networks (GNNs) use Laplacians to
  propagate signals across graph structure.
\end{enumerate}

\subsubsection{Why It Matters}\label{why-it-matters-88}

Graphs and matrices are two sides of the same coin: one combinatorial,
one algebraic. By turning a network into a matrix, linear algebra gives
us access to the full toolbox of eigenvalues, norms, and factorizations,
enabling deep insights into connectivity, flow, and structure.

\subsubsection{Try It Yourself}\label{try-it-yourself-92}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute adjacency, degree, and Laplacian matrices for a square graph
  (4 nodes in a cycle). Find eigenvalues of \(L\).
\item
  Prove that the Laplacian always has at least one zero eigenvalue.
\item
  Show that if a graph has \(k\) components, then the multiplicity of
  zero as an eigenvalue is exactly \(k\).
\item
  For a random walk on a graph, derive the transition matrix
  \(P = D^{-1}A\). Interpret its eigenvectors.
\end{enumerate}

Graphs demonstrate how linear algebra stretches beyond geometry and data
tables-it becomes a universal language for networks, from molecules to
megacities.

\subsection{94. Data Preprocessing as Linear Operations (Centering,
Whitening,
Scaling)}\label{data-preprocessing-as-linear-operations-centering-whitening-scaling}

Before any sophisticated model can be trained, raw data must be
preprocessed. Surprisingly, many of the most common preprocessing
steps-centering, scaling, whitening-are nothing more than linear algebra
operations in disguise. Understanding them this way not only clarifies
why they work, but also shows how they connect to broader concepts like
covariance, eigenvalues, and singular value decomposition.

\subsubsection{The Nature of
Preprocessing}\label{the-nature-of-preprocessing}

Most datasets are stored as a matrix: rows correspond to samples
(observations) and columns correspond to features (variables). For
instance, in a dataset of 1,000 people with height, weight, and age
recorded, we'd have a \(1000 \times 3\) matrix. Linear algebra allows us
to systematically reshape, scale, and rotate this matrix to prepare it
for downstream analysis.

\subsubsection{Centering: Shifting the
Origin}\label{centering-shifting-the-origin}

Centering means subtracting the mean of each column (feature) from all
entries in that column.

\[
X_{centered} = X - \mathbf{1}\mu^T
\]

\begin{itemize}
\tightlist
\item
  Here \(X\) is the data matrix, \(\mu\) is the vector of column means,
  and \(\mathbf{1}\) is a column of ones.
\item
  Effect: moves the dataset so that each feature has mean zero.
\item
  Geometric view: translates the cloud of points so its ``center of
  mass'' sits at the origin.
\item
  Why important: covariance and correlation formulas assume data are
  mean-centered; otherwise, cross-terms are skewed.
\end{itemize}

Example: If people's heights average 170 cm, subtract 170 from every
height. After centering, ``height = 0'' corresponds to the average
person.

\subsubsection{Scaling: Normalizing
Variability}\label{scaling-normalizing-variability}

Raw features can have different units or magnitudes (e.g., weight in kg,
income in thousands of dollars). To compare them fairly, we scale:

\[
X_{scaled} = X D^{-1}
\]

where \(D\) is a diagonal matrix of feature standard deviations.

\begin{itemize}
\tightlist
\item
  Each feature now has variance 1.
\item
  Geometric view: rescales axes so all dimensions have equal ``spread.''
\item
  Common in machine learning: ensures gradient descent does not
  disproportionately focus on features with large raw values.
\end{itemize}

Example: If weight varies around 60 kg ± 15, dividing by 15 makes its
spread comparable to that of height (±10 cm).

\subsubsection{Whitening: Removing
Correlations}\label{whitening-removing-correlations}

Even after centering and scaling, features can remain correlated (e.g.,
height and weight). Whitening transforms the data so features become
uncorrelated with unit variance.

\begin{itemize}
\tightlist
\item
  Let \(\Sigma = \frac{1}{n} X^T X\) be the covariance matrix of
  centered data.
\item
  Perform eigendecomposition: \(\Sigma = Q \Lambda Q^T\).
\item
  Whitening transform:
\end{itemize}

\[
X_{white} = X Q \Lambda^{-1/2} Q^T
\]

Result:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The covariance matrix of \(X_{white}\) is the identity matrix.
\item
  Each new feature is a rotated combination of old features, with no
  redundancy.
\end{enumerate}

Geometric view: whitening ``spheres'' the data cloud, turning an ellipse
into a perfect circle.

\subsubsection{Covariance Matrix as the Key
Player}\label{covariance-matrix-as-the-key-player}

The covariance matrix itself arises naturally from preprocessing:

\[
\Sigma = \frac{1}{n} X^T X \quad \text{(if \(X\) is centered).}
\]

\begin{itemize}
\tightlist
\item
  Diagonal entries: variances of features.
\item
  Off-diagonal entries: covariances, measuring linear relationships.
\item
  Preprocessing operations (centering, scaling, whitening) are designed
  to reshape data so \(\Sigma\) becomes easier to interpret and more
  stable for learning algorithms.
\end{itemize}

\subsubsection{Connections to PCA}\label{connections-to-pca}

\begin{itemize}
\tightlist
\item
  Centering is required before PCA, otherwise the first component just
  points to the mean.
\item
  Scaling ensures PCA does not overweight large-variance features.
\item
  Whitening is closely related to PCA itself: PCA diagonalizes the
  covariance, and whitening goes one step further by rescaling
  eigenvalues to unity.
\end{itemize}

Thus, PCA can be seen as a preprocessing pipeline plus an analysis step.

\subsubsection{Practical Workflows}\label{practical-workflows}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Centering and Scaling (Standardization): The default for many
  algorithms like logistic regression or SVM.
\item
  Whitening: Often used in signal processing (e.g., removing
  correlations in audio or images).
\item
  Batch Normalization in Deep Learning: A variant of centering + scaling
  applied layer by layer during training.
\item
  Whitening in Image Processing: Ensures features like pixel intensities
  are decorrelated, improving compression and recognition.
\end{enumerate}

\subsubsection{Worked Example}\label{worked-example}

Suppose we have three features: height, weight, and age.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Raw data:

  \begin{itemize}
  \tightlist
  \item
    Mean height = 170 cm, mean weight = 65 kg, mean age = 35 years.
  \item
    Variance differs widely: age varies less, weight more.
  \end{itemize}
\item
  After centering:

  \begin{itemize}
  \tightlist
  \item
    Mean of each feature is zero.
  \item
    A person of average height now has value 0 in that feature.
  \end{itemize}
\item
  After scaling:

  \begin{itemize}
  \tightlist
  \item
    All features have unit variance.
  \item
    Algorithms can treat age and weight equally.
  \end{itemize}
\item
  After whitening:

  \begin{itemize}
  \tightlist
  \item
    Correlation between height and weight disappears.
  \item
    Features become orthogonal directions in feature space.
  \end{itemize}
\end{enumerate}

\subsubsection{Why It Matters}\label{why-it-matters-89}

Without preprocessing, models may be misled by scale, units, or
correlations. Preprocessing makes features comparable, balanced, and
independent-a crucial condition for algorithms that rely on geometry
(distances, angles, inner products).

In essence, preprocessing is the bridge from messy, real-world data to
the clean structures linear algebra expects.

\subsubsection{Try It Yourself}\label{try-it-yourself-93}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  For a small dataset, compute the covariance matrix before and after
  centering. What changes?
\item
  Scale the dataset so each feature has unit variance. Check the new
  covariance.
\item
  Perform whitening via eigendecomposition and verify the covariance
  matrix becomes the identity.
\item
  Plot the data points in 2D before and after whitening. Notice how an
  ellipse becomes a circle.
\end{enumerate}

Preprocessing through linear algebra shows that preparing data is not
just housekeeping-it's a fundamental reshaping of the problem's
geometry.

\subsection{95. Linear Regression and Classification (From Model to
Matrix)}\label{linear-regression-and-classification-from-model-to-matrix}

Linear algebra provides the foundation for two of the most widely used
tools in data science and applied statistics: linear regression
(predicting continuous outcomes) and linear classification (separating
categories). Both problems reduce to expressing data in matrix form and
then applying linear operations to estimate parameters.

\subsubsection{The Regression Setup}\label{the-regression-setup}

Suppose we want to predict an output \(y \in \mathbb{R}^n\) from
features collected in a data matrix \(X \in \mathbb{R}^{n \times p}\),
where:

\begin{itemize}
\tightlist
\item
  \(n\) = number of observations (samples).
\item
  \(p\) = number of features (variables).
\end{itemize}

We assume a linear model:

\[
y \approx X\beta,
\]

where \(\beta \in \mathbb{R}^p\) is the vector of coefficients
(weights). Each entry of \(\beta\) tells us how much its feature
contributes to the prediction.

\subsubsection{The Normal Equations}\label{the-normal-equations}

We want to minimize the squared error:

\[
\min_\beta \|y - X\beta\|^2.
\]

Differentiating leads to the normal equations:

\[
X^T X \beta = X^T y.
\]

\begin{itemize}
\tightlist
\item
  If \(X^T X\) is invertible:
\end{itemize}

\[
\hat{\beta} = (X^T X)^{-1} X^T y.
\]

\begin{itemize}
\tightlist
\item
  If not invertible (multicollinearity, too many features), we use the
  pseudoinverse via SVD:
\end{itemize}

\[
\hat{\beta} = X^+ y.
\]

\subsubsection{Geometric
Interpretation}\label{geometric-interpretation-18}

\begin{itemize}
\tightlist
\item
  \(X\beta\) is the projection of \(y\) onto the column space of \(X\).
\item
  The residual \(r = y - X\hat{\beta}\) is orthogonal to all columns of
  \(X\).
\item
  This ``closest fit'' property is why regression is a projection
  problem.
\end{itemize}

\subsubsection{Classification with Linear
Models}\label{classification-with-linear-models}

Instead of predicting continuous outputs, sometimes we want to separate
categories (e.g., spam vs.~not spam).

\begin{itemize}
\tightlist
\item
  Linear classifier: decides based on the sign of a linear function.
\end{itemize}

\[
\hat{y} = \text{sign}(w^T x + b).
\]

\begin{itemize}
\tightlist
\item
  Geometric view: \(w\) defines a hyperplane in feature space. Points on
  one side are labeled positive, on the other side negative.
\item
  Relation to regression: logistic regression replaces squared error
  with a log-likelihood loss, but still solves for weights via iterative
  linear-algebraic methods.
\end{itemize}

\subsubsection{Multiclass Extension}\label{multiclass-extension}

\begin{itemize}
\tightlist
\item
  For \(k\) classes, we use a weight matrix
  \(W \in \mathbb{R}^{p \times k}\).
\item
  Prediction:
\end{itemize}

\[
\hat{y} = \arg \max_j (XW)_{ij}.
\]

\begin{itemize}
\tightlist
\item
  Each class has a column of \(W\), and the classifier picks the column
  with the largest score.
\end{itemize}

\subsubsection{Example: Predicting House
Prices}\label{example-predicting-house-prices}

\begin{itemize}
\tightlist
\item
  Features: size, number of rooms, distance to city center.
\item
  Target: price.
\item
  \(X\) = matrix of features, \(y\) = price vector.
\item
  Regression solves for coefficients showing how strongly each factor
  influences price.
\end{itemize}

If we switch to classification (predicting ``expensive'' vs.~``cheap''),
we treat price as a label and solve for a hyperplane separating the two
categories.

\subsubsection{Computational Aspects}\label{computational-aspects}

\begin{itemize}
\tightlist
\item
  Directly solving normal equations: \(O(p^3)\) (matrix inversion).
\item
  QR factorization: numerically more stable.
\item
  SVD: best when \(X\) is ill-conditioned or rank-deficient.
\item
  Modern libraries: exploit sparsity or use gradient-based methods for
  large datasets.
\end{itemize}

\subsubsection{Connections to Other
Topics}\label{connections-to-other-topics}

\begin{itemize}
\tightlist
\item
  Least Squares (Chapter 8): Regression is the canonical least-squares
  problem.
\item
  SVD (Chapter 9): Pseudoinverse gives regression when columns are
  dependent.
\item
  Regularization (Chapter 9): Ridge regression adds a penalty
  \(\lambda \|\beta\|^2\) to improve stability.
\item
  Classification (Chapter 10): Forms the foundation of more complex
  models like support vector machines and neural networks.
\end{itemize}

\subsubsection{Why It Matters}\label{why-it-matters-90}

Linear regression and classification show the direct link between linear
algebra and real-world decisions. They combine geometry (projection,
hyperplanes), algebra (solving systems), and computation
(factorizations). Despite their simplicity, they remain indispensable:
they are interpretable, fast, and often competitive with more complex
models.

\subsubsection{Try It Yourself}\label{try-it-yourself-94}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Given three features and five samples, construct \(X\) and \(y\).
  Solve for \(\beta\) using the normal equations.
\item
  Show that residuals are orthogonal to all columns of \(X\).
\item
  Write down a linear classifier separating two clusters of points in
  2D. Sketch the separating hyperplane.
\item
  Explore what happens when two features are highly correlated
  (collinear). Use pseudoinverse to recover a stable solution.
\end{enumerate}

Linear regression and classification are proof that linear algebra is
not just abstract-it is the engine of practical prediction.

\subsection{96. PCA in Practice (Dimensionality Reduction
Workflow)}\label{pca-in-practice-dimensionality-reduction-workflow}

Principal Component Analysis (PCA) is one of the most widely used tools
in applied linear algebra. At its heart, PCA identifies the directions
(principal components) along which data varies the most, and then
re-expresses the data in terms of those directions. In practice, PCA is
not just a mathematical curiosity-it is a complete workflow for reducing
dimensionality, denoising data, and extracting patterns from
high-dimensional datasets.

\subsubsection{The Motivation}\label{the-motivation}

Modern datasets often have thousands or even millions of features:

\begin{itemize}
\tightlist
\item
  Images: each pixel is a feature.
\item
  Genomics: each gene expression level is a feature.
\item
  Text: each word in a vocabulary becomes a dimension.
\end{itemize}

Working in such high dimensions is expensive (computationally) and
fragile (noise accumulates). PCA provides a systematic way to reduce the
feature space to a smaller set of dimensions that still captures most of
the variability.

\subsubsection{Step 1: Organizing the
Data}\label{step-1-organizing-the-data}

We start with a data matrix \(X \in \mathbb{R}^{n \times p}\):

\begin{itemize}
\tightlist
\item
  \(n\): number of samples (observations).
\item
  \(p\): number of features (variables).
\end{itemize}

Each row is a sample; each column is a feature.

Centering is the first preprocessing step: subtract the mean of each
column so the dataset has mean zero. This ensures that PCA describes
variance rather than being biased by offsets.

\[
X_{centered} = X - \mathbf{1}\mu^T
\]

\subsubsection{Step 2: Covariance
Matrix}\label{step-2-covariance-matrix}

Next, compute the covariance matrix:

\[
\Sigma = \frac{1}{n} X_{centered}^T X_{centered}.
\]

\begin{itemize}
\tightlist
\item
  Diagonal entries: variance of each feature.
\item
  Off-diagonal entries: how features co-vary.
\end{itemize}

The structure of \(\Sigma\) determines the directions of maximal
variation in the data.

\subsubsection{Step 3: Eigen-Decomposition or
SVD}\label{step-3-eigen-decomposition-or-svd}

Two equivalent approaches:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Eigen-decomposition: Solve \(\Sigma v = \lambda v\).

  \begin{itemize}
  \tightlist
  \item
    Eigenvectors \(v\) are the principal components.
  \item
    Eigenvalues \(\lambda\) measure variance along those directions.
  \end{itemize}
\item
  Singular Value Decomposition (SVD): Directly decompose the centered
  data matrix:

  \[
  X_{centered} = U \Sigma V^T.
  \]

  \begin{itemize}
  \tightlist
  \item
    Columns of \(V\) = principal directions.
  \item
    Squared singular values correspond to variances.
  \end{itemize}
\end{enumerate}

SVD is preferred in practice for numerical stability and efficiency,
especially when \(p\) is very large.

\subsubsection{Step 4: Choosing the Number of
Components}\label{step-4-choosing-the-number-of-components}

We order eigenvalues
\(\lambda_1 \geq \lambda_2 \geq \dots \geq \lambda_p\).

\begin{itemize}
\item
  Explained variance ratio:

  \[
  \text{EVR}(k) = \frac{\sum_{i=1}^k \lambda_i}{\sum_{i=1}^p \lambda_i}.
  \]
\item
  We choose \(k\) such that EVR exceeds some threshold (e.g., 90--95\%).
\item
  This balances dimensionality reduction with information preservation.
\end{itemize}

Graphically, a scree plot shows eigenvalues, and we look for the
``elbow'' point where additional components add little variance.

\subsubsection{Step 5: Projecting Data}\label{step-5-projecting-data}

Once we select \(k\) components, we project onto them:

\[
X_{PCA} = X_{centered} V_k,
\]

where \(V_k\) contains the top \(k\) eigenvectors.

Result:

\begin{itemize}
\tightlist
\item
  \(X_{PCA} \in \mathbb{R}^{n \times k}\).
\item
  Each row is now a \(k\)-dimensional representation of the original
  sample.
\end{itemize}

\subsubsection{Worked Example: Face
Images}\label{worked-example-face-images}

Suppose we have a dataset of grayscale images, each \(100 \times 100\)
pixels (\(p = 10,000\)).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Center each pixel value.
\item
  Compute covariance across all images.
\item
  Find eigenvectors = eigenfaces. These are characteristic patterns like
  ``glasses,'' ``mouth shape,'' or ``lighting direction.''
\item
  Keep top 50 components. Each face is now represented as a
  50-dimensional vector instead of 10,000.
\end{enumerate}

This drastically reduces storage and speeds up recognition while keeping
key features.

\subsubsection{Practical Considerations}\label{practical-considerations}

\begin{itemize}
\tightlist
\item
  Standardization: If features have different scales (e.g., age in years
  vs.~income in thousands), we must scale them before PCA.
\item
  Computational shortcuts: For very large \(p\), it's often faster to
  compute PCA via truncated SVD on \(X\) directly.
\item
  Noise filtering: Small eigenvalues often correspond to noise;
  truncating them denoises the dataset.
\item
  Interpretability: Principal components are linear combinations of
  features. Sometimes these combinations are interpretable, sometimes
  not.
\end{itemize}

\subsubsection{Connections to Other
Concepts}\label{connections-to-other-concepts}

\begin{itemize}
\tightlist
\item
  Whitening (Chapter 94): PCA followed by scaling eigenvalues to 1 is
  whitening.
\item
  SVD (Chapter 9): PCA is essentially an application of SVD.
\item
  Regression (Chapter 95): PCA can be used before regression to reduce
  collinearity among predictors (PCA regression).
\item
  Machine learning pipelines: PCA is often used before clustering,
  classification, or neural networks.
\end{itemize}

\subsubsection{Why It Matters}\label{why-it-matters-91}

PCA turns raw, unwieldy data into a compact form without losing
essential structure. It enables visualization (2D/3D plots of
high-dimensional data), faster learning, and noise reduction. Many
breakthroughs-from face recognition to gene expression analysis-rely on
PCA as the first preprocessing step.

\subsubsection{Try It Yourself}\label{try-it-yourself-95}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Take a dataset with 3 features. Manually compute covariance,
  eigenvalues, and eigenvectors.
\item
  Project the data onto the first two principal components and plot.
  Compare to the original 3D scatter.
\item
  Download an image dataset and apply PCA to compress it. Reconstruct
  the images with 10, 50, 100 components. Observe the trade-off between
  compression and fidelity.
\item
  Compute explained variance ratios and decide how many components to
  keep.
\end{enumerate}

PCA is the bridge between raw data and meaningful representation: it
reduces complexity while sharpening patterns. It shows how linear
algebra can reveal hidden order in high-dimensional chaos.

\subsection{97. Recommender Systems and Low-Rank Models (Fill the
Missing
Entries)}\label{recommender-systems-and-low-rank-models-fill-the-missing-entries}

Recommender systems-such as those used by Netflix, Amazon, or
Spotify-are built on the principle that preferences can be captured by
low-dimensional structures hidden inside large, sparse data. Linear
algebra gives us the machinery to expose and exploit these structures,
especially through low-rank models.

\subsubsection{The Matrix of
Preferences}\label{the-matrix-of-preferences}

We begin with a user--item matrix \(R \in \mathbb{R}^{m \times n}\):

\begin{itemize}
\tightlist
\item
  Rows represent users.
\item
  Columns represent items (movies, books, songs).
\item
  Entries \(R_{ij}\) store the rating (say 1--5 stars) or interaction
  (clicks, purchases).
\end{itemize}

In practice, most entries are missing-users rate only a small subset of
items. The central challenge: predict the missing entries.

\subsubsection{Why Low-Rank Structure?}\label{why-low-rank-structure}

Despite its size, \(R\) often lies close to a low-rank approximation:

\[
R \approx U V^T
\]

\begin{itemize}
\tightlist
\item
  \(U \in \mathbb{R}^{m \times k}\): user factors.
\item
  \(V \in \mathbb{R}^{n \times k}\): item factors.
\item
  \(k \ll \min(m, n)\).
\end{itemize}

Here, each user and each item is represented in a shared latent feature
space.

\begin{itemize}
\tightlist
\item
  Example: For movies, latent dimensions might capture ``action
  vs.~romance,'' ``old vs.~new,'' or ``mainstream vs.~indie.''
\item
  A user's preference vector in this space interacts with an item's
  feature vector to generate a predicted rating.
\end{itemize}

This factorization explains correlations: if you liked Movie A and B,
and Movie C shares similar latent features, the system predicts you'll
like C too.

\subsubsection{Singular Value Decomposition (SVD)
Approach}\label{singular-value-decomposition-svd-approach}

If \(R\) were complete (no missing entries), we could compute the SVD:

\[
R = U \Sigma V^T.
\]

\begin{itemize}
\tightlist
\item
  Keep the top \(k\) singular values to form a rank-\(k\) approximation.
\item
  This captures the dominant patterns in user preferences.
\item
  Geometric view: project the massive data cloud onto a smaller
  \(k\)-dimensional subspace where structure is clearer.
\end{itemize}

But real data is incomplete. That leads to matrix completion problems.

\subsubsection{Matrix Completion}\label{matrix-completion}

Matrix completion tries to infer missing entries of \(R\) by assuming
low rank. The optimization problem is:

\[
\min_{X} \ \text{rank}(X) \quad \text{s.t. } X_{ij} = R_{ij} \text{ for observed entries}.
\]

Since minimizing rank is NP-hard, practical algorithms instead minimize
the nuclear norm (sum of singular values) or use alternating
minimization:

\begin{itemize}
\tightlist
\item
  Initialize \(U, V\) randomly.
\item
  Iteratively solve for one while fixing the other.
\item
  Converge to a low-rank factorization that fits the observed ratings.
\end{itemize}

\subsubsection{Alternating Least Squares
(ALS)}\label{alternating-least-squares-als}

ALS is a standard approach:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Fix \(V\), solve least squares for \(U\).
\item
  Fix \(U\), solve least squares for \(V\).
\item
  Repeat until convergence.
\end{enumerate}

Each subproblem is straightforward linear regression, solvable with
normal equations or QR decomposition.

\subsubsection{Stochastic Gradient Descent
(SGD)}\label{stochastic-gradient-descent-sgd}

Another approach: treat each observed rating as a training sample.
Update latent vectors by minimizing squared error:

\[
\ell = (R_{ij} - u_i^T v_j)^2.
\]

Iteratively adjust user vector \(u_i\) and item vector \(v_j\) along
gradients. This scales well to huge datasets, making it common in
practice.

\subsubsection{Regularization}\label{regularization}

To prevent overfitting:

\[
\ell = (R_{ij} - u_i^T v_j)^2 + \lambda (\|u_i\|^2 + \|v_j\|^2).
\]

\begin{itemize}
\tightlist
\item
  Regularization shrinks factors, discouraging extreme values.
\item
  Geometrically, it keeps latent vectors within a reasonable ball in
  feature space.
\end{itemize}

\subsubsection{Cold Start Problem}\label{cold-start-problem}

\begin{itemize}
\tightlist
\item
  New users: Without ratings, \(u_i\) is unknown. Solutions: use
  demographic features or ask for a few initial ratings.
\item
  New items: Similarly, items need side information (metadata, tags) to
  generate initial latent vectors.
\end{itemize}

This is where hybrid models combine matrix factorization with
content-based features.

\subsubsection{Example: Movie Ratings}\label{example-movie-ratings}

Imagine 1,000 users and 5,000 movies.

\begin{itemize}
\tightlist
\item
  The raw \(R\) matrix has 5 million entries, but each user has rated
  only \textasciitilde50 movies.
\item
  Matrix completion with rank \(k = 20\) recovers a dense approximation.
\item
  Each user is represented by 20 latent ``taste'' factors; each movie by
  20 latent ``theme'' factors.
\item
  Prediction: the dot product of user and movie vectors.
\end{itemize}

\subsubsection{Beyond Ratings: Implicit
Feedback}\label{beyond-ratings-implicit-feedback}

In practice, systems often lack explicit ratings. Instead, they use:

\begin{itemize}
\tightlist
\item
  Views, clicks, purchases, skips.
\item
  These signals are indirect but abundant.
\item
  Factorization can handle them by treating interactions as weighted
  observations.
\end{itemize}

\subsubsection{Connections to Other Linear Algebra
Tools}\label{connections-to-other-linear-algebra-tools}

\begin{itemize}
\tightlist
\item
  SVD (Chapter 9): The backbone of factorization methods.
\item
  Pseudoinverse (Chapter 9): Useful when solving small regression
  subproblems in ALS.
\item
  Conditioning (Chapter 9): Factorization stability depends on
  well-scaled latent factors.
\item
  PCA (Chapter 96): PCA is essentially a low-rank approximation, so PCA
  and recommenders share the same mathematics.
\end{itemize}

\subsubsection{Why It Matters}\label{why-it-matters-92}

Recommender systems personalize the modern internet. Every playlist
suggestion, book recommendation, or ad placement is powered by linear
algebra hidden in a massive sparse matrix. Low-rank modeling shows how
even incomplete, noisy data can be harnessed to reveal patterns of
preference and behavior.

\subsubsection{Try It Yourself}\label{try-it-yourself-96}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Take a small user--item matrix with missing entries. Apply rank-2
  approximation via SVD to fill in gaps.
\item
  Implement one step of ALS: fix movie factors and update user factors
  with least squares.
\item
  Compare predictions with and without regularization. Notice how
  regularization stabilizes results.
\item
  Explore the cold-start problem: simulate a new user and try predicting
  preferences from minimal data.
\end{enumerate}

Low-rank models reveal a powerful truth: behind the enormous variety of
human choices lies a surprisingly small set of underlying patterns-and
linear algebra is the key to uncovering them.

\subsection{98. PageRank and Random Walks (Ranking with
Eigenvectors)}\label{pagerank-and-random-walks-ranking-with-eigenvectors}

PageRank, the algorithm that once powered Google's search engine
dominance, is a striking example of how linear algebra and eigenvectors
can measure importance in a network. At its core, it models the web as a
graph and asks a simple question: if you randomly surf the web forever,
which pages will you visit most often?

\subsubsection{The Web as a Graph}\label{the-web-as-a-graph}

\begin{itemize}
\tightlist
\item
  Each web page is a node.
\item
  Each hyperlink is a directed edge.
\item
  The adjacency matrix \(A\) encodes which pages link to which:
\end{itemize}

\[
A_{ij} = 1 \quad \text{if page \(j\) links to page \(i\)}.
\]

Why columns instead of rows? Because links flow from source to
destination, and PageRank naturally arises when analyzing
column-stochastic transition matrices.

\subsubsection{Transition Matrix}\label{transition-matrix}

To model random surfing, we define a column-stochastic matrix \(P\):

\[
P_{ij} = \frac{1}{\text{outdeg}(j)} \quad \text{if \(j \to i\)}.
\]

\begin{itemize}
\tightlist
\item
  Each column sums to 1.
\item
  \(P_{ij}\) is the probability of moving from page \(j\) to page \(i\).
\item
  This defines a Markov chain: a random process where the next state
  depends only on the current one.
\end{itemize}

If a user is on page \(j\), they pick one outgoing link uniformly at
random.

\subsubsection{Random Walk
Interpretation}\label{random-walk-interpretation}

Imagine a web surfer moving page by page according to \(P\). After many
steps, the fraction of time spent on each page converges to a
steady-state distribution vector \(\pi\):

\[
\pi = P \pi.
\]

This is an eigenvector equation: \(\pi\) is the stationary eigenvector
of \(P\) with eigenvalue 1.

\begin{itemize}
\tightlist
\item
  \(\pi_i\) is the long-run probability of being on page \(i\).
\item
  A higher \(\pi_i\) means greater importance.
\end{itemize}

\subsubsection{The PageRank Adjustment:
Teleportation}\label{the-pagerank-adjustment-teleportation}

The pure random walk has problems:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Dead ends: Pages with no outgoing links trap the surfer.
\item
  Spider traps: Groups of pages linking only to each other hoard
  probability mass.
\end{enumerate}

Solution: add a teleportation mechanism:

\begin{itemize}
\tightlist
\item
  With probability \(\alpha\) (say 0.85), follow a link.
\item
  With probability \(1-\alpha\), jump to a random page.
\end{itemize}

This defines the PageRank matrix:

\[
M = \alpha P + (1-\alpha)\frac{1}{n} ee^T,
\]

where \(e\) is the all-ones vector.

\begin{itemize}
\tightlist
\item
  \(M\) is stochastic, irreducible, and aperiodic.
\item
  By the Perron--Frobenius theorem, it has a unique stationary
  distribution \(\pi\).
\end{itemize}

\subsubsection{Solving the Eigenproblem}\label{solving-the-eigenproblem}

The PageRank vector \(\pi\) satisfies:

\[
M \pi = \pi.
\]

\begin{itemize}
\tightlist
\item
  Computing \(\pi\) directly via eigen-decomposition is infeasible for
  billions of pages.
\item
  Instead, use power iteration: repeatedly multiply a vector by \(M\)
  until convergence.
\end{itemize}

This works because the largest eigenvalue is 1, and the method converges
to its eigenvector.

\subsubsection{Worked Example: A Tiny
Web}\label{worked-example-a-tiny-web}

Suppose 3 pages with links:

\begin{itemize}
\tightlist
\item
  Page 1 → Page 2
\item
  Page 2 → Page 3
\item
  Page 3 → Page 1 and Page 2
\end{itemize}

Adjacency matrix (columns = source):

\[
A = \begin{bmatrix} 
0 & 0 & 1 \\ 
1 & 0 & 1 \\ 
0 & 1 & 0 
\end{bmatrix}.
\]

Transition matrix:

\[
P = \begin{bmatrix} 
0 & 0 & 1/2 \\ 
1 & 0 & 1/2 \\ 
0 & 1 & 0 
\end{bmatrix}.
\]

With teleportation (\(\alpha=0.85\)), we form \(M\). Power iteration
quickly converges to \(\pi = [0.37, 0.34, 0.29]^T\). Page 1 is ranked
highest.

\subsubsection{Beyond the Web}\label{beyond-the-web}

Although born in search engines, PageRank's mathematics applies broadly:

\begin{itemize}
\tightlist
\item
  Social networks: Rank influential users by their connections.
\item
  Citation networks: Rank scientific papers by how they are referenced.
\item
  Biology: Identify key proteins in protein--protein interaction
  networks.
\item
  Recommendation systems: Rank products or movies via link structures.
\end{itemize}

In each case, importance is defined not by how many connections a node
has, but by the importance of the nodes that point to it.

\subsubsection{Computational Challenges}\label{computational-challenges}

\begin{itemize}
\tightlist
\item
  Scale: Billions of pages mean \(M\) cannot be stored fully; sparse
  matrix techniques are essential.
\item
  Convergence: Power iteration may take hundreds of steps;
  preconditioning and parallelization speed it up.
\item
  Personalization: Instead of uniform teleportation, adjust
  probabilities to bias toward user interests.
\end{itemize}

\subsubsection{Why It Matters}\label{why-it-matters-93}

PageRank illustrates a deep principle: importance emerges from
connectivity. Linear algebra captures this by identifying the dominant
eigenvector of a transition matrix. This idea-ranking nodes in a network
by stationary distributions-has transformed search engines, social
media, and science itself.

\subsubsection{Try It Yourself}\label{try-it-yourself-97}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Construct a 4-page web graph and compute its PageRank manually with
  \(\alpha = 0.85\).
\item
  Implement power iteration in Python or MATLAB for a small adjacency
  matrix.
\item
  Compare PageRank to simple degree counts. Notice how PageRank rewards
  links from important nodes more heavily.
\item
  Modify teleportation to bias toward a subset of pages (personalized
  PageRank). Observe how rankings change.
\end{enumerate}

PageRank is not only a milestone in computer science history-it is a
living example of how eigenvectors can capture global importance from
local structure.

\subsection{99. Numerical Linear Algebra Essentials (Floating Point,
BLAS/LAPACK)}\label{numerical-linear-algebra-essentials-floating-point-blaslapack}

Linear algebra in theory is exact: numbers behave like real numbers,
operations are deterministic, and results are precise. In practice,
computations are carried out on computers, where numbers are represented
in finite precision and algorithms must balance speed, accuracy, and
stability. This intersection-numerical linear algebra-is what makes
linear algebra usable at modern scales.

\subsubsection{Floating-Point
Representation}\label{floating-point-representation}

Real numbers cannot be stored exactly on a digital machine. Instead,
they are approximated using the IEEE 754 floating-point standard.

\begin{itemize}
\item
  A floating-point number is stored as:

  \[
  x = \pm (1.m_1 m_2 m_3 \dots) \times 2^e
  \]

  where \(m\) is the mantissa and \(e\) is the exponent.
\item
  Single precision (float32): 32 bits → \textasciitilde7 decimal digits
  of precision.
\item
  Double precision (float64): 64 bits → \textasciitilde16 decimal
  digits.
\item
  Machine epsilon (\(\epsilon\)): The smallest gap between 1 and the
  next representable number. For double precision,
  \(\epsilon \approx 2.22 \times 10^{-16}\).
\end{itemize}

Implication: operations like subtraction of nearly equal numbers cause
catastrophic cancellation, where significant digits vanish.

\subsubsection{Conditioning of Problems}\label{conditioning-of-problems}

A linear algebra problem may be well-posed mathematically but still
numerically difficult.

\begin{itemize}
\item
  The condition number of a matrix \(A\):

  \[
  \kappa(A) = \|A\| \cdot \|A^{-1}\|.
  \]
\item
  If \(\kappa(A)\) is large, small input errors cause large output
  errors.
\item
  Example: solving \(Ax = b\). With ill-conditioned \(A\), the computed
  solution may be unstable even with perfect algorithms.
\end{itemize}

Geometric intuition: ill-conditioned matrices stretch vectors unevenly,
so small perturbations in direction blow up under inversion.

\subsubsection{Stability of Algorithms}\label{stability-of-algorithms}

\begin{itemize}
\tightlist
\item
  An algorithm is numerically stable if it controls the growth of errors
  from finite precision.
\item
  Gaussian elimination with partial pivoting is stable; without
  pivoting, it may fail catastrophically.
\item
  Orthogonal factorizations (QR, SVD) are usually more stable than
  elimination methods.
\end{itemize}

Numerical analysis focuses on designing algorithms that guarantee
accuracy within a few multiples of machine epsilon.

\subsubsection{Direct vs.~Iterative
Methods}\label{direct-vs.-iterative-methods}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Direct methods: Solve in a finite number of steps (e.g., Gaussian
  elimination, LU decomposition, Cholesky for positive definite
  systems).

  \begin{itemize}
  \tightlist
  \item
    Reliable for small/medium problems.
  \item
    Complexity \textasciitilde{} \(O(n^3)\).
  \end{itemize}
\item
  Iterative methods: Generate successive approximations (e.g., Jacobi,
  Gauss--Seidel, Conjugate Gradient).

  \begin{itemize}
  \tightlist
  \item
    Useful for very large, sparse systems.
  \item
    Complexity per iteration \textasciitilde{} \(O(n^2)\) or less, often
    leveraging sparsity.
  \end{itemize}
\end{enumerate}

\subsubsection{Matrix Factorizations in
Computation}\label{matrix-factorizations-in-computation}

Many algorithms rely on factorizing a matrix once, then reusing it:

\begin{itemize}
\tightlist
\item
  LU decomposition: Efficient for solving multiple right-hand sides.
\item
  QR factorization: Stable approach for least squares.
\item
  SVD: Gold standard for ill-conditioned problems, though expensive.
\end{itemize}

These factorizations reduce repeated operations into structured,
cache-friendly steps.

\subsubsection{Sparse vs.~Dense
Computations}\label{sparse-vs.-dense-computations}

\begin{itemize}
\tightlist
\item
  Dense matrices: Most entries are nonzero. Use dense linear algebra
  packages like BLAS and LAPACK.
\item
  Sparse matrices: Most entries are zero. Store only nonzeros, use
  specialized algorithms to avoid wasted computation.
\end{itemize}

Large-scale problems (e.g., finite element simulations, web graphs) are
feasible only because of sparse methods.

\subsubsection{BLAS and LAPACK: Standard
Libraries}\label{blas-and-lapack-standard-libraries}

\begin{itemize}
\tightlist
\item
  BLAS (Basic Linear Algebra Subprograms): Defines kernels for vector
  and matrix operations (dot products, matrix--vector, matrix--matrix
  multiplication). Optimized BLAS implementations exploit cache, SIMD,
  and multi-core parallelism.
\item
  LAPACK (Linear Algebra PACKage): Builds on BLAS to provide algorithms
  for solving systems, eigenvalue problems, SVD, etc. LAPACK is the
  backbone of many scientific computing environments (MATLAB, NumPy,
  Julia).
\item
  MKL, OpenBLAS, cuBLAS: Vendor-specific implementations optimized for
  Intel CPUs, open-source systems, or NVIDIA GPUs.
\end{itemize}

These libraries make the difference between code that runs in minutes
and code that runs in milliseconds.

\subsubsection{Floating-Point Pitfalls}\label{floating-point-pitfalls}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Accumulated round-off: Summing numbers of vastly different magnitudes
  may discard small contributions.
\item
  Loss of orthogonality: Repeated Gram--Schmidt orthogonalization
  without reorthogonalization may drift numerically.
\item
  Overflow/underflow: Extremely large/small numbers exceed representable
  range.
\item
  NaNs and Infs: Divide-by-zero or invalid operations propagate errors.
\end{enumerate}

Mitigation: use numerically stable algorithms, scale inputs, and check
condition numbers.

\subsubsection{Parallel and GPU
Computing}\label{parallel-and-gpu-computing}

Modern numerical linear algebra thrives on parallelism:

\begin{itemize}
\tightlist
\item
  GPUs accelerate dense linear algebra with thousands of cores (cuBLAS,
  cuSOLVER).
\item
  Distributed libraries (ScaLAPACK, PETSc, Trilinos) allow solving
  problems with billions of unknowns across clusters.
\item
  Mixed precision methods: compute in float32 or even float16, then
  refine in float64, balancing speed and accuracy.
\end{itemize}

\subsubsection{Applications in the Real
World}\label{applications-in-the-real-world}

\begin{itemize}
\tightlist
\item
  Engineering simulations: Structural mechanics, fluid dynamics rely on
  sparse solvers.
\item
  Machine learning: Training deep networks depends on optimized BLAS for
  matrix multiplications.
\item
  Finance: Risk models solve huge regression problems with factorized
  covariance matrices.
\item
  Big data: Dimensionality reduction (PCA, SVD) requires large-scale,
  stable algorithms.
\end{itemize}

\subsubsection{Why It Matters}\label{why-it-matters-94}

Linear algebra in practice is about more than theorems: it's about
turning abstract models into computations that run reliably on imperfect
hardware. Numerical linear algebra provides the essential
toolkit-floating-point understanding, conditioning analysis, stable
algorithms, and optimized libraries-that ensures results are both fast
and trustworthy.

\subsubsection{Try It Yourself}\label{try-it-yourself-98}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute the condition number of a nearly singular matrix (e.g.,
  \(\begin{bmatrix} 1 & 1 \\ 1 & 1.0001 \end{bmatrix}\)) and solve
  \(Ax=b\). Compare results in single vs.~double precision.
\item
  Implement Gaussian elimination with and without pivoting. Compare
  errors for ill-conditioned matrices.
\item
  Use NumPy with OpenBLAS to time large matrix multiplications; compare
  against a naive Python implementation.
\item
  Explore iterative solvers: implement Conjugate Gradient for a sparse
  symmetric positive definite system.
\end{enumerate}

Numerical linear algebra is the bridge between mathematical elegance and
computational reality. It teaches us that solving equations on a
computer is not just about the equations-it's about the algorithms,
representations, and hardware that bring them to life.

\subsection{100. Capstone Problem Sets and Next Steps (A Roadmap to
Mastery)}\label{capstone-problem-sets-and-next-steps-a-roadmap-to-mastery}

You've now walked through the major landmarks of linear algebra:
vectors, matrices, systems, transformations, determinants, eigenvalues,
orthogonality, SVD, and applications to data and networks. The journey
doesn't end here. This last section is designed as a capstone, a way to
tie things together and show you how to keep practicing, exploring, and
deepening your understanding. Think of it as your ``next steps'' map.

\subsubsection{Practicing the Basics Until They Feel
Natural}\label{practicing-the-basics-until-they-feel-natural}

Linear algebra may seem heavy at first, but the simplest drills build
lasting confidence. Try solving a few systems of equations by hand using
elimination, and notice how pivoting reveals where solutions exist-or
don't. Write down a small matrix and practice multiplying it by a
vector. This might feel mechanical, but it's how your intuition
sharpens: every time you push numbers through the rules, you're learning
how the algebra reshapes space.

Even a single concept, like the dot product, can teach a lot. Take two
short vectors in the plane, compute their dot product, and then compare
it to the cosine of the angle between them. Seeing algebra match
geometry is what makes linear algebra come alive.

\subsubsection{Moving Beyond Computation: Understanding
Structures}\label{moving-beyond-computation-understanding-structures}

Once you're comfortable with the mechanics, try reflecting on the bigger
structures. What does it mean for a set of vectors to be a subspace? Can
you tell whether a line through the origin is one? What about a line
shifted off the origin? This is where the rules and axioms you've seen
start to guide your reasoning.

Experiment with bases and coordinates: pick two different bases for the
plane and see how a single point looks different depending on the
``ruler'' you're using. Write out the change-of-basis matrix and check
that it transforms coordinates the way you expect. These exercises show
that linear algebra isn't just about numbers-it's about perspective.

\subsubsection{Bringing Ideas Together in Larger
Problems}\label{bringing-ideas-together-in-larger-problems}

The real joy comes when different ideas collide. Suppose you have noisy
data, like a scatter of points that should lie along a line. Try fitting
a line using least squares. What you're really doing is projecting the
data onto a subspace. Or take a small Markov chain, like a random walk
around three or four nodes, and compute its long-term distribution. That
steady state is an eigenvector in disguise. These integrative problems
demonstrate how the topics you've studied connect.

Projects make this even more vivid. For example:

\begin{itemize}
\tightlist
\item
  In computer graphics, write simple code that rotates or reflects a
  shape using a matrix.
\item
  In networks, use the Laplacian to identify clusters in a social graph
  of friends.
\item
  In recommendation systems, factorize a small user--item table to
  predict missing ratings.
\end{itemize}

These aren't abstract puzzles-they show how linear algebra works in the
real world.

\subsubsection{Looking Ahead: Where Linear Algebra Leads
You}\label{looking-ahead-where-linear-algebra-leads-you}

By now you know that linear algebra is not an isolated subject; it's a
foundation. The next steps depend on your interests.

If you enjoy computation, numerical linear algebra is the natural
extension. It digs into how floating-point numbers behave on real
machines, how to control round-off errors, and why some algorithms are
more stable than others. You'll learn why Gaussian elimination with
pivoting is safe while without pivoting it can fail, and why QR and SVD
are trusted in sensitive applications.

If abstraction intrigues you, then abstract linear algebra opens the
door. Here you'll move beyond \(\mathbb{R}^n\) into general vector
spaces: polynomials as vectors, functions as vectors, dual spaces, and
eventually tensor products. These ideas power much of modern mathematics
and physics.

If data excites you, statistics and machine learning are a natural path.
Covariance matrices, principal component analysis, regression, and
neural networks all rest on linear algebra. Understanding them deeply
requires both the computation you've practiced and the geometric
insights you've built.

And if your curiosity points toward the sciences, linear algebra is
everywhere: in quantum mechanics, where states are vectors and operators
are matrices; in engineering, where vibrations and control systems rely
on eigenvalues; in computer graphics, where every rotation and
projection is a linear transformation.

\subsubsection{Why This Capstone
Matters}\label{why-this-capstone-matters}

This final step is less about new theorems and more about perspective.
The problems you solve now-whether small drills or large projects-train
you to see structure, not just numbers. The roadmap is open-ended,
because linear algebra itself is open-ended: once you learn to see the
world through its lens, you notice it everywhere, from the patterns in
networks to the behavior of algorithms to the geometry of space.

\subsubsection{Try It Yourself}\label{try-it-yourself-99}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Take a dataset you care about-maybe sports scores, songs you listen
  to, or spending records. Organize it as a matrix. Compute simple
  things: averages (centering), a regression line, maybe even principal
  components. See what structure you uncover.
\item
  Write a short program that solves systems of equations using
  elimination. Test it on well-behaved and nearly singular matrices.
  Notice how stability changes.
\item
  Draw a 2D scatterplot and fit a line with least squares. Plot the
  residuals. What does it mean geometrically that the residuals are
  orthogonal to the line?
\item
  Try explaining eigenvalues to a friend without formulas-just pictures
  and stories. Teaching it will make it real.
\end{enumerate}

Linear algebra is both a tool and a way of thinking. You now have enough
to stand on your own, but the road continues forward-into deeper math,
into practical computation, and into the sciences that rely on these
ideas every day. This capstone is an invitation: keep practicing, keep
exploring, and let the structures of linear algebra sharpen the way you
see the world.

\subsubsection{Closing}\label{closing-9}

\begin{verbatim}
From lines to the stars,
each problem bends, transforms, grows—
paths extend ahead.
\end{verbatim}

\newpage

\subsection{Finale}\label{finale}

\emph{A quiet closing, where lessons settle and the music of algebra
carries on beyond the final page.}

\textbf{1. Quiet Reflection}

\begin{verbatim}
Lessons intertwining,
the book rests, but vectors stretch—
silence holds their song.
\end{verbatim}

\textbf{2. Infinite Journey}

\begin{verbatim}
One map now complete,
yet beyond each line and plane
new horizons call.
\end{verbatim}

\textbf{3. Structure and Growth}

\begin{verbatim}
Roots beneath the ground,
branches weaving endless skies,
algebra takes flight.
\end{verbatim}

\textbf{4. Light After Study}

\begin{verbatim}
Numbers fade to light,
patterns linger in the mind,
paths remain open.
\end{verbatim}

\textbf{5. Eternal Motion}

\begin{verbatim}
Stillness finds its place,
transformations carry on,
movement without end.
\end{verbatim}

\textbf{6. Gratitude and Closure}

\begin{verbatim}
Steps of thought complete,
spaces carved with gentle care,
thank you, wandering mind.
\end{verbatim}

\textbf{7. Future Echo}

\begin{verbatim}
From shadows to form,
each question births new echoes—
the journey goes on.
\end{verbatim}

\textbf{8. Horizon Beyond}

\begin{verbatim}
The book closes here,
yet the lines refuse to end,
they stretch toward the stars.
\end{verbatim}




\end{document}
